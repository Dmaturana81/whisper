 Okay, we're recording. So I know there was some questions on the forum, Matt, I think it was you, right? A couple of questions. Yep, that's correct. I can read them out for you if you like. Oh, yeah, or you can express them in your own terms, whatever. Yeah, tell me. So I guess there weren't so much, first of all, there weren't a couple of, there weren't questions, they were just sort of differences when working on paper space, you're working on your local GPU. And I found Simlinking the Kaggle folder into storage with my API keys, something that I did to make it a little bit easier to restart. Just wanting to verify that that's sort of a good thing to do. And but the one thing that I would be really keen to know about is the, when I did the pip install of Tim, it was it was fine, it was installed, but I had to restart the kernel and I'm wondering that might be a bit of a pain going forward. I'd prefer to have it persistently just there, ready to go. Yep. But it's not a con, not a conda package, so I wasn't sure how to. Yeah. So it's actually, it's actually pip packages are the only ones we've actually got the persistence working for. So let's do that one first. So the key thing when you install Tim, let's see, do I already have it installed? What Tim? Okay, great. I don't. So let's do it. So the key to think, remember, is when you install Tim is to do it with dash, dash user. Now, in order to make that easier, I think what I would be inclined to do would be to edit our slash storage slash dot bash dot local and add to it alias. Let's do PI for pip install equals pip install. Let's do minus you upgrade. That should work even if it's not installed already. Minus minus user. Okay. Now, if I, so I could close and reopen my terminal or I could just type source and then the name of the script, which of course, in this case is exclamation mark dollar. Oopsie dozy exclamation mark theme will rerun this. This whole thing needs to be in quotes because it's a single thing. It's my alias. Okay, I'll pair it twice. Okay, so now I can just type PI. And by the way, if you want to know what something is, if you type which PI, it won't tell you anything useful because it's an alias, not a binary. But if you type type PI, it will tell you exactly what it is in this case. Oh, it's something that's an alias. So I can type PI Tim. And the key thing about minus minus user is that's going to put it in my dash dash local directory. Sorry, my dot local directory. So. There it is, Tim. So then all you need to make sure is that. Your local directory is sim linked. Two dot slash storage config local. Oh, no, that's interesting. Our, this is here is telling us we've got a broken sim link. So that's what that means. Yeah, dot get config is sim linked to slash storage. But there is no dot get config there. So I might have maybe forgot to move that or something. So, okay, next time we try to commit it'll tell us and we'll know to fix that then. To create a file that's empty, you just use touch. So I'm just going to go ahead and create an empty file. So at least it exists. And then things won't get horribly confused. Did I not touch it correctly? Oh, there's a slash at the end. Oh, that's why that's why it's confused. So that would be a directory, but this is not a directory. So my guess is that there's a bug in our pre run script for dot to config. Yes, I've got a slash at the end. So that's why that didn't work. So if I. Source that. Now it's happy. Great. So now, yeah, so now since it's been installed into something that's similar back to slash storage, Tim will be available. And if I run I Python, we can further did install. That should be all good. Does that answer that part of the question? Yes, thank you. So then the second one, yeah, it was not a question, but a comment, which is about Kaggle. So yeah, when I get back to using Kaggle on this machine, we will do that for sure. Which will probably be next time. And you also had a question about jumping around to. You know, the end of a string, for example. Which. Let's grab. Last day, I was repo, for example. Oh, and you also had a question about loading and serving models. Great. So, I mean, one thing obviously is it'd be nice to have tags file. At some point we could even talk about how to set up Vim to automatically create that forest from time to time. But let's have a look at, I don't know, layers, for example. So a few things to mention, the first is something which sounds very obscure but actually isn't, is, is, f in Vim. f in Vim is like slash. Now slash searches. So we've seen it before, slash in it. Well, search for the next thing called in it. Okay. Oh, maybe something we haven't discussed is to go back to where we were, regardless of whether it was a tag or a search or anything. It's control O. And right next to control O is the letter I, which goes forward again. Okay. So control O and control I go kind of like pressing the back button and the forward button on your browser. There's something a lot like slash, but with just finds a single letter, which is f. If I type f, it's got a, and it's under your search on the current line. It'll search on this line for the next thing I type. So if I type f double quote, actually, maybe more interesting would be f full stop. So if I type f full stop, it's going to jump to the full stop f dot. So you see it jumps to the full stop. Right. And so your question was, well, what about jumping to the end of a string? Now, in this case, the end of a string is the last character of the line. So there's a better answer, which is to start inserting at the end of the line. It's shift A. Just one moment. My daughter's got kicked off her zoom call. Always technical problems. Okay. So I can undo that. Control O to go back to where I was. But yeah, so let's say there was some stuff at the end hash some comment. Right. And we wanted to go to the next double quote. I can just type f double quote. And it takes me there. And then shift f does the opposite, such as backwards. And the reason it's interesting mainly is that that's emotion, and therefore I can combine things with it. So for example, if I wanted to delete everything up to the next quote, I can press D, f double quote. Right. And then I could press slash double quote to search the next one and press dot. And it'll do the same thing again. Right. Or maybe delete everything up to the next comment would be df hash. So, yeah, those are a couple of useful things. Another really useful one is percent. Percent jumps to between the start and the end of a set of paired parentheses or braces or brackets. So if I press percent here, it goes to the start. It goes to the start of the end of the next parentheses and then press it again. You can see it jumps between the two. Right. And so if I do it from here, you can see it jumps to the end of this one. Right. Or if I do it at the very end, you can jump to this one. So if I want to delete from here to the end of the parenthesis, parenthetical expression, let's say to delete this bit, I could press df. Sorry, df percent. Sorry, not df percent, just d percent. There you go, d percent. You see? Although there's actually something even better for that, which is i. And i refers to an area, the whole area that is surrounded by some kind of parentheses. So even when I'm in the middle of these parentheses, the enclosing parentheses would go from here to here. And so i stands for inside. So if I want to delete everything inside those parentheses, I can type di, open round parenthesis, and it deletes the contents, which is really nice. So let's say I wanted to replace all my parameters with something else, like a comma b. Then I would use c for change inside parentheses. So type my change, like a comma b, right? And then I can come down here and type dot, and it'll do the same thing. So yeah, it's like, maybe you'll work, you can kind of really crush with these tricks. Great. Fantastic. Yeah, it's cool. There's a lot of them, and you don't have to know them all. You know, it's like you can learn one thing each day or something. Yeah, I'm not using any plugins or anything. Okay, so we're going to save a model in a moment. Any other questions or comments before I go back to our book. I want to make one comment about the Tim installation. I don't know if maybe you discussed this yesterday because I came a little late. But with the Tim installation. Sometimes it might be better to install from master because there are some changes that Ross has made that you might not receive. Yeah, I did mention that yesterday. Especially I think the conclusion we came to was to install the latest prerelease because that's like something that's. Or stable than installing from master, but. But you know, better than his sometimes like here he went six months without updating. So yeah, I agree. In fact, so let's do that. So this is 0.6.2 dev. So I think we decided that would go and let's use on UPI thing. Tim is greater than or equal to 0.6.2 dev. Great. Yeah, thanks for the reminder. All right, great. It's kind of this thing in. Python modules and quite a lot of other things if there's like. An extra dot dev at the end. That means it's a prerelease, basically. And so PIP has this. Convention that if you say I want to install something that is at least as recent as 0.6.2 dev, then that's a way of signaling to pip that you're happy to include prerelease options. Is there any reason that when you do the installation of team and then you try to use the learner. It doesn't it says that team doesn't exist when you try to load the model. Right. That's because you have to restart the kernel after installing it. And so now that it's installed in local every time I start a machine, it's going to be there anyway, so you wouldn't have to worry about that again. Okay. So this was our. Notebook from yesterday. And I wanted to try to improve the model. And one of the reasons I wanted to try to improve the model is because we are, you know, our our result was, you know, worse than the top 50%. There you go. Top 50% I didn't know that was a tip. That's handy. And so we should, you know, I want to aim to at least be as good as this helpful. Fast AI out of the box person. So they got 0.97385. How far are we? You know, which is better than ours. Right. That was me. That was my number. Fantastic. I like it. It's a good notebook. So we're going to try to beat you. I hope you don't mind. But then you'll know how to beat us because we at least you know how to match us. So my own. So yeah, I saw that what you did here was you trained for longer, which makes sense. And you also used some data augmentation, which makes sense. So let's talk about. About this. So if we're going to train. For, so what's your name, Gerardo? Is it. Is it your auto or Gerardo? Either way, that's fine. Which is right. I want to be accurate. Well, my name is Gerardo. I see. So both of them. There is. Thank you. Gerardo. Okay. So if we're going to train as long as far out of it, then. You know, if you train more than about five epochs, you're in danger of overfitting and certainly tan. I feel like you're in subsequent danger of overfitting because your model is going to have seen every image, you know, 10 times. So in order to avoid overfitting to the specific images it's seeing, we should make it so that it sees a slightly different image each time. And this is discussed in the book. In some detail. But basically. If you pass in batch transforms, these are things that are going to be applied to each mini batch. So to each bunch of however many 32 or 64 or whatever images. And there, this is basically a bunch of functions that are going to be applied. So what does this function do or transform? So this is transforms for data augmentation. So we know that the best way to find out what something's going to do is to check its help. So let's start there. Not help doc. Okay, so it's going to do things like flip our images, rotate them, zoom them, change their brightness, their warp. See, show and docs. Okay, and here's some examples of a very cute puppy that's all they're found. I think it's all they're founded. So this is all the same puppy. It's all the same picture. And as you can see each time the model sees it, it sees a somewhat skewed or rotated or brightened or darkened or whatever version of that picture. And so this is called data augmentation. So. Let's try then running that. And so all transforms actually returns a list. Right. It returns a list of transformations. So here's the flip transformation with the probability of 0.5 it all flip. It's got a brightness transformation with a probability of one, it will change the lighting by up to 0.2. And then random resized crop is perhaps the most interesting one, which is it will zoom in such that it has at least 75% of the height and width. And it will, yeah, it will basically pick a smaller zoomed in section randomly chosen each time. So what we can do is when we say show batch, if you say unique equals true, it'll show the same picture each time. And so here you can see four versions of the same picture. You can see sometimes it's flipped. Sometimes it's moved a little bit up and down. Sometimes it's a little bit darker or less dark and it's also a little bit rotated. So that's what data augmentation is. And that really helps us if we want to train a few more epochs. Then the second thing I figured we should do is, you know, resnets actually great, but there are things which are greater. And as we talked about, Tim has a bunch of them and in particular, com of next, pretty good. And the other thing we could do is think about learning rates. The default learning rate used by FastAI is one where I would say I picked it on the conservative side, which means it's a little bit lower than you probably need, because I wanted things to always be able to train. But there's actually a downside to using a couple of downsides to using a lower learning rate than you need. The first is that given fixed resources, fixed amount of time, you're going to have less epochs, not less epochs, sorry, less distance that the weights can move. The second is, it turns out, a high learning rate helps the optimizer to explore the space of options by jumping further to see if there's better places to go. So the learning rate finder is suggesting things around about.002, which is indeed the default. But you can see that all the way up to like 10 to the negative two, it still looks like a pretty nice slope. And the other thing to remember is, as we saw after answering next question yesterday, we're using one cycle training schedule, which means we're gradually increasing the learning rate. And my claim was that by doing that, we can reach higher learning rates. So I would also say that even these recommendations are going to be a bit more conservative side. So what I did just before I started this call was I tried training at a learning rate of.01, which is five times higher than the default. And so that's up here. And I did find actually that that did give us a better result with a 2% error. So let's see, I mean, obviously you've got different training sets, but this is hopeful. Right, that we're going to get a better result than our target. It's nice to have a target aim for. Okay, so that's that was the next thing. So then it's since this took, you know, six minutes to train, it's probably a good idea to save it. So there's a couple of different things we can save with. One is.save and the other is.export. So learner.export. Saves the contents. That's not very well written self. So the learner self means that this learner. And it saves it to self.path slash F names or learner.path slash F using pickle. So basically, what that means is if you call this, learn.export, it's going to save it into learner.path. So let's find out. Learn.path is what? Train images. And so this is actually whatever we passed in here. So if we want to save things somewhere else, there's, we've got a couple of options. One is to change learner.path by setting it equal to some other path. Or we can just use an absolute path. So an absolute path is something that starts with slash. And so if I want to save it somewhere in storage, for example, then I can type slash storage slash whatever. Or maybe I want to put it in slash notebooks somewhere. So these are some ways you can change where it's going to save. I might even just put it into the current directory. I think that seems fine to me. Well, actually, where are we? Current directory. Yeah, put it in git patty. That sounds fine. Or maybe to be a bit more sure just in case the directory ever changes. It must be specific. So then the other option is learn.save. So learn.save doesn't save the whole learner. It just saves the model and the optimizer state. The difference is that remember a learner doesn't just contain the model, but it also contains the information about the data loaders, and specifically what transformations are applied. So I don't really often, if ever, use dot save. The only reason I would use dot save is if I was writing something to like, we already have stuff in FastA8. We'll take an example. In FastA8, we have something that's a callback that can save the model at the end of each epoch. Each time it gets a better result than its previous best, whatever. In those cases, we might use dot save. So then you recreate a learner and you can dot load into the learner. But yeah, for exporting something, I want to be able to just load that exact thing with all the same details next time dot export the way to go. So I'm going to call dot export. I'm going to use, it's a conf next. It's small, and I did 12 epochs. Oh, and this needs to be an actual path. Normally we actually try to make these things do that for you, but this is less friendly than I would like. Sorry about that. There we go. Okay. So we should now be able to see it. There it is. Okay. And it looks like we need to give it a dot pickle or whatever. By default, it all with with all transforms, which uses random resource, but it'll randomly pick a subset of the crop of the image of this up to this of this size or bigger. And the validation set, it will pick out the center. It'll, you know, is all the width that can or all the height it can without changing the aspect ratio. If you say squish instead, it will grab the whole thing and change the aspect ratio to squish it into a square. You don't have to raise your hand. Just talk to me, mate. What's up. Can you hear me? I can't hear you. Does that mean you can't hear me? I can hear you. But you can't hear anybody. They do need to raise your hand. Why can't you hear? But you guys can hear me. Okay. Yes. Yes, we can hear you, Jim. We can hear you. I see why. Okay. Say something. Can you hear me now? Yeah, yeah, I can. All right. Okay. All right. Did you guys, were you guys saying anything I was meant to be hearing? Did I miss anything? Yeah. Why did you choose 12 feet parks? Oh, no particular reason. I just saw that this one was using 14 and I thought. Oh, I'm for something around there, but maybe just do a little bit less. I guess I often do around well, wish epochs. Like. It seems to like for. I don't know. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. My assumptions were that the number for 60 is because of the size of this. The images were for 60. And then another assumption was to 24 because when you show the team. With the different, the, the convex and. The image size was to 24. That's the reason that I selected that. Is that okay? Is that a, is that a correct? It's a look it up in the book. It's under the section called pre sizing and I think this is around what we always pre sized to. So actually maybe for 80 would have been better because then it wouldn't have had to change one of the dimensions because there was 640 by 480. And then your size you picked actually changed it. So I don't picked. 230, but actually most of these models are the trained on image net. I generally trained on 224. So I wanted them to be the same size as what they trained on. So that's why I picked 224. Yeah, so then squish I've talked about. Oh, and then the other thing is the model I picked is one with a suffix in 22 K. I N here refers to image net and the 22 K refers to the version of image net with 22,000 categories as opposed to the version that's normally used which only has 1000 categories. So this is a conf next, which is small, but is trained on image net with a 22,000 category version. The 22,000 category version, it just has a lot more images covering a lot more different things. So there's a much higher chance that it's going to have seen something like. Rice patty illness than the one with 1000 images and it's just seen a lot more different picks, you know. So yeah, I would recommend always using the in 22 K pre trained models. So those are, I think, the key differences at the training stage. Yeah, I think when you had put the, the export and then the error came, that's when it cut off. So I don't think you explained what you did to we didn't catch the part where you explained the fix. The fix. Well, because because the export had an error, right. And then I guess you've not added. I don't think it had an error, but I just, oh, I see. Yes. Yes. Yes. Okay. Yeah. The export had an error because this was a string and it actually has to be a path. Which I'd say is an oversight on my part. I try to make it so that everything can accept a path or a string. So I would consider that a bug that ought to be fixed. So hopefully by the time people watch this video, that might have been fixed. But yes, at the moment, I had to change this to a path. Thank you. All right. So. There's a few things we could do here, right, but one key issue is that the. Is that particularly if you don't have methodical squish, when we do validation, it's only selecting the center of the image. And that's a problem. Right. We would like it to see all the image. And then another thing is that we've been training it with various different augmentations, but the validation set. We don't use any of those augmentations. So there's a trick you can use, which you should particularly use if you don't use squish and it's effectively cropping into the center, which is something called test time augmentation. And in test time augmentation, we basically get multiple versions of each image. We actually by default get four different randomly augmented versions of each image. And plus the un augmented version, we get the prediction on every one, and then we take their average. And that's called test time augmentation. And as I said, it's going to work particularly well without the squish, but it ought to work well, even with the squish. So to get those predictions. Let's first of all make sure we can replicate this error rate manually. Right. So if we go. And then we pass in the validation set. And then we should find that if we ask now for the error rate. Shift tab. So the inputs of the probabilities and the targets of the targets. There we go. Okay. So that's our 2.02% error rate. So we've we've replicated that. Okay. So now we've got that 2.02. I would then try out TTA. And of course, before we use a new function, we would always read its documentation. Here we are.TTA. So return the predictions on some data set or some data loader. We get the predictions n times by default for using the training set transformations. Great. Oh, and instead of getting the average of predictions, we could also get the max of predictions. And, you know, the other thing which I definitely encourage you to do is, you know, it's always good to look at the source code. Because my claim is that fast AI functions are generally not very big. And like also quite a bit of its stuff, you can kind of skip over it. This kind of like, oh, what if it's none? What if it's none? Like, it's just setting default. So you can kind of skip it. Try finallys you can skip because it's just error handling. With this, you can pretty much split progress bars. You can pretty much skip. So the actual work starts happening here. We're going to call self.getPreads, passes in the data loader, and then it catenates that all together. And then it takes either the maximum or the mean, depending on whether you asked for the max or not. And it also grabs it for the validation set data loader. Yeah, so you kind of get the idea. So let's run it. See if we can beat 2.02%. So you can see here it's running at four times, each of the four augmented versions. And then it will run at one time for the non augmented version. Okay, and it beat it, but just by a little bit. And then, you know, another thing is, well, what if we did the non maximum? Use max equals false. Use max equals true. Use the maximum instead of the average. Yeah, I kind of wish I didn't have the squish in now, but I don't want you guys to have to wait 10 minutes for it to retrain because then it's much more clearly see the benefit of using TTA. That's interesting. That one's worse. So I generally find that when not using squish, that using TTA and use max equals true is best. Okay, so now we've done all that. We can try and submit this one to Kaggle. So we can just repeat basically what we had yesterday, but instead of get spreads. We'll do TTA. Now, there's no with decoded. I don't think for TTA. So we're going to have to do a bit of extra work here. So this is going to give us the probabilities and the targets. And so the probabilities each row is going to contain a probability for each element of the vocab. So we can take a look. And so it's a. So for each of the 3,469 things in the test set, there are 10 probabilities, which presumably means the length of the vocab is 10, which it is. So to find, so what we want to do is find out, well, what's it actually predicting? And the thing it's predicting is whatever thing has the highest probability. So I'm going to go through each row and find the index of the thing with the highest probability. So in in math and pie torch NumPy, that's called argmax. So argmax is the index of the thing with the highest value. So, mobs. And so what do we want to take the maximum over which dimension. So we want to do it over rows, which I think we say dimension equals one. There we go. So that's the correct shape. So now we should be able to do the same thing we did yesterday, which is to convert that into a series. And now we should be able to run. This mapping. Now I realize actually this thing we did yesterday where we went K colon V for K comma V in enumerate is actually a really long way of just saying create a dictionary from those from those tuples. So when you create a dictionary, you can do it like this. Right. Or you could do this. Here's a here's a tuple of tuples. Okay, sorry, here's a tuple of tuples. And ideally what we'd like is to call dict and pass in each pair of these as an argument to it. And so Python actually has syntax to do exactly that for any function, not just dict, which is the function star star and star star means take a mapping and pass it in as as pairs. So that's what this does. Right. And that's going to be a mapping which enumerate already is. So that's what's star. Just pop this here. This is not working. I expect this to work. How annoying. Well, so much for that discussion. Annoying. All right, I'm going to have to try to think of a better way to make this work. So far. Similar problem to what we had yesterday. I think you don't need the star star in that case. Wow, that's nice. Isn't it even better. Thanks for the trick. Okay. I didn't quite get to show you about how call star star is. Never mind. Okay. So what I'm going to do is I'm going to make a copy of the last time we did ahead of the submission. And one reason I like to do that for my new submission is to confirm that our new one looks somewhat similar. So previously we went, Hispa normal downy blast blast. Now we go Hispa normal blast blast blast. And so this makes me feel comfortable that, okay, we haven't totally broken things. It's still giving largely the same results as before with a few changes. And so that's just something I like to do. Okay. And then another thing I like to do is kind of kick track of stuff I've done before. I try not to delete things I've used before. So just pop it into a different notebook or comment. So down here, I'm just going to have non TTA version. Just in case I want that again later. All right, so we should be able to submit that now. Okay. So I use controller and then to start a typing competitions. Okay, so this is now a squish. Mind tune. What on earth did it do to my window? How do I get back? Oh, it... Oh, I see. I've got to... How does it happen? I've got to... I guess Tom's going. I didn't notice that. All right, let's go and check out Kaggle. Nice submissions. Oh, look at that. How about us still beating us, I think, but at least we've beaten our previous one. That's amazing. That's great. Jumped to our leadable position, we're going to have a good battle on 34. No, I think you beat me up. Wait, I thought yours was better than that. I think I'm a little bit lower. Code. Oh, you were 9.7. Oh, I'm getting a good shot. Okay, 9.7.9. That's not bad. It's just a fun competition. Nobody's trying too hard, but still, it's nice to feel like you're in the mix. How far are we? This person's still way ahead. They've got an error of 1.3%, and we've got an error of 2.1%. You know, something else that would be fun would be, you know, you could, like, you can kind of super easily create an ensemble. So maybe I'll show you how I would go about creating an ensemble. To create an ensemble, I would be inclined to maybe, we could create an ensemble with an unsquished version, for instance. So what I would do is I'd kind of like copy all the stuff that we used to get our predictions. Right, and then I would kind of paste them down here. Go through and remove the stuff that isn't quite needed. Like so. This one's going to be no squish. And go to max is max. It calls true. And so then to merge cells, it's shift M, M for merge. And to make the error rate anymore. And so this is going to be a second set of probabilities and a second set of targets. Yeah, so we could just run that and take the average of these two models out, remove squish here. Okay, so that might be our third model. And then another model I would be inclined to try is one that doesn't use square. So we've got 640 by 480 images, right? So the aspect ratio is four to three. So I would be inclined to say, take that and multiply that by the smaller side we want. Okay, that gives us 298.66. Nice to find something that works a bit more evenly, wouldn't it? What if we did it the other way around? So we could create 168 by 224 images, for instance, or 256, maybe 336 by 252 images. Yeah, let's do that. So 336 by 252 images. And so the reason I'm doing rectangular images is that all of our input images are the same aspect ratio. So there's no particular reason to make them square. And some of your images are wider than tall and some are taller than wide, then it makes perfect sense to use square as your thing that everything gets changed to. But when everything's wider than they are tall, especially when they're all the same aspect ratio, it makes more sense to keep them at that same aspect ratio. And another thing I guess we should consider doing for 640 by 380 is to, you know, you can change their resolution more gracefully without weird interpolating fuzziness by doing it by, you know, a factor of two. So we could do 320 instead of 640. And by 240. So that would be another one I'd be inclined to try. Yeah, in fact, let's just do that. Let's make that the aspect ratio. There we go. And so obviously we should check it and we know how to check it, which is to go show that. Okay, so you can see I've got it the wrong way around. There we go. That's better. Cool. And like given that we're going to have such nice clear images, I would probably do the, the affine transforms are the ones where we're zooming and rotating and stuff. So to say, don't do those so often, we can change the probability of affine transforms from 0.75 to 0.5. So in theory, I feel like this one feels the most correct, given that the data that we have is a fixed input size of that type. So I would be inclined to, well, you know, we'll take a look afterwards. But I just do copy, we'll save a different set. And so we can easily then check the accuracy of each of them. And this one's going to be rectangular. There we go. Now that we're saving a few, I guess I'm a little worried that paper space might disappear. And so I'm actually inclined to save these into my notebooks directory, just to be a bit paranoid. And I'm going to copy paste. And so let's move. Oh, that's right. I'm not using paper space. So I don't have to. I forgot. All right, I'm going to not have you guys watch that run for 20 minutes. So I'm going to go. Any questions or comments before we wrap up. You're like focusing a lot on like the data, transformations and augmentations, when would you focus on that versus, you know, playing around with different models and things like that instead. Given that this is a image classification task for natural link for natural photos. It will almost certainly have exactly the same characteristics as. Image net in terms of accuracy, or at least by any fine tuning on image net. So I would I'm just working on the assumption, which I could read off. We can test later, but I'm pretty sure it's going to be true that the things that are in that. That notebook showing which which Tim models are better than others will apply to this data set. So I would once everything else is working really well. You know, I would then try it on a couple of models or at least run it on a bigger one like base or large or whatever I can get away with. If it was like a. Segmentation problem or an object detection problem or a medical imaging data set, which has the kind of pictures that aren't in image net, you know, for all of these things I would try more different architectures, but then for those cases, I would. Plant says a segmentation problem, which is about recognizing what each pixel is, it always is is a pixel of even there. I would not try to replicate the research of others. Instead, I would go and look at something like papers with code.com to find out which techniques have the best results on segmentation and better still, I would go and find two or three previous Kaggle competitions that have a similar problem type and see who won and see what they did. Now, when you look at who won, they always say, Oh, we made an ensemble, which is fine. The but the important thing isn't that they didn't ensemble. It'll be there'll always say pretty much the best model in our ensemble was X. And so I would just use X and I would use as kind of like smallest version of X I can get away with. And yeah, generally fiddling with architectures tends not to be very useful nowadays for any kind of problem that like people have fairly regularly studied, which almost any computer vision problem is of that type. I guess the only interesting question for this one would be, there is something saying what kind of rice is in this patty, which is like a category. But I'm fairly sure that using that information is not going to be helpful in this case, because the model can perfectly well see what kind of rice it is. So I very much doubt we have to tell it because it's got pictures. Jeremy, yeah, it's going to take me a while to work through all of the videos. Yeah, are they going to be virtually available. Yes. Cool. And don't feel like you can only join if you've watched all the previous videos and don't feel like you can only ask a question if you've watched all the previous videos, like, it's totally fine to ask a question about a video we did a week ago, or about something that we just covered yesterday or whatever. If the answer to your question is, oh, we covered this in this video. Here's where you go. I will tell you that. And that's totally fine. And if it's like, okay, you said this thing in this other video, but I don't get it. Say it again. That's totally fine too. Like, we're moving at quite a fast pace because people can go back and rewatch the videos and because people can come back later and ask questions about things that aren't clear. So yeah, it definitely does rely on people turning up and saying, I'm not clear on this or whatever. Yeah, well, I sort of started from around zero in this whole environment, but it is starting to make sense now on starting to be a little bit more comfy with it. Nice. And I just want to take the time to work through my way through and absorb that's all what you've been talking about. So Daniel, I will say like, there's a couple more lesson lessons to come. Like, what is it next week or the week after I suspect during those two weeks I'll probably stop the walkthroughs. So there'll be a couple of weeks there to catch up. But yeah, like feel free to share join in any time or not join in any time and ask questions about any video or even about things it's not covered in a video but you feel like would be something useful to know in order to understand. I'm really looking forward to the tabular data actually. Oh, cool. Okay, thank you. Thanks all. See you next time. Okay.
