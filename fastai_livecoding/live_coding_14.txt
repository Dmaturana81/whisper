 Peter, there we go. Yeah. Another question. Yeah. So Joe, the training sort of process in fast AI, like is there a concept or capability to do like early stopping or best kind of thing? Or if there isn't, why is there a reason why you chose not to do that? I never remember because I don't use it myself. So what I would check and just checking now is the callbacks, which is under trainings. I'm just going to the docs training callbacks. And if anybody else knows, please shout out. There's a guy back. Early stopping callback. Yeah, I found it. Okay. It's under tracking callbacks. So if you go to the docs training callbacks tracker, there's an early stopping callback. So perhaps the more interesting part then is like, why do I not use it? So I don't even know whether it exists. There's a few reasons. One is that it doesn't play nicely with one cycle training or fine tuning. If you stop early, then the learning rate hasn't got a chance to go down. And for that reason, it's almost never the case that earlier APOCs have better accuracy because the set learning rate hasn't settled down yet. If I was doing one cycle training and I saw that an earlier APOC had a much better accuracy, then I would know that I'm overfitting, in which case I would be adding more data augmentation rather than doing early stopping because it's good to train for the amount of time that you have. So yeah, I can't think offhand of a situation where I haven't come across a situation where I've personally wanted to use early stopping. So like in some of the training examples, like where you had the error rate, but some of the prior runs may have had a better lower error rate. Oh, I mean, in the ones I've shown like a tiny bit better, yeah, but like not enough to be like meaningful, you know, and yeah, so that there's no reason to believe that those are actually better models and there's plenty of very prior reason to believe that they're actually not, which is that the learning rate still hasn't settled down at that point. So we haven't let it fine tune into the best spot yet. So yeah, if it's kind of going down, down and down, it's kind of bottoming out and just bumps a little bit at the bottom, that's not a reason to use early stopping. It's also, I think, important to realize that the validation sets relatively small as well. So it's only a representation of, you know, of the distribution that the data is coming from. So reading too much into those small fluctuations can be very counterproductive. I know that I've wasted a lot of time in the past, you know, doing that, but a lot of time. But yeah, we're looking for changes that dramatically improve things, you know, like changing from ResNet 26 to ComfNEXT, we improved by what 400 or 500%. And he's like, okay, that's an improvement. Over the weekend, I went on my own server that I have here behind me, that I have a 10, 10 ADI, and I run all like 35 models for the patty thing. And I was just, I didn't do the example, but I was thinking about this that when I was taking algebra back in in high school or college, you have some of these expressions that you have the function of x is equal to x squared for the x greater than something. And the absolute value of x when you have x equal to something. So it just got me my idea that the idea is that maybe some of the data set is going to fail the target value for every single one of the models that we tried. But if we try different models, it's going to be successful. So can we do that? I mean, of course we can, but I mean, what would be the easiest approach to say for this validation when x is equal to this or greater than that, this is the model to use. But then if this is the other model, this is the way you have to use. Yeah, I mean, you could do that, right? And like a really simple way to do that, which I've seen used for some success on Kaggle is to train lots of models. And then to trade a gradient boosting machine, whose inputs are those model predictions that whose output is the targets. And so that'll do exactly what you just described. It's very easy to overfit when you do that. And you're only going to get it. If you've trained them well, you're only going to get a tidy increase, right? Because because the neural nets are flexible, it shouldn't have that situation where this part of the space, it has bad predictions. And this part of the space, it has good predictions. Like it's, that's not really how neural nets work. If you had a variety of types of like different, totally different types of model, like a random forest, energy, BAM, and a neural net, I could see that maybe. But most of the time, one of those will be dramatically better than the other ones. And so like I don't that often find myself wanting to ensemble across totally different types of model. So, you know, I'd say it's another one of these things like early stopping, which like a lot of people waste huge amounts of time on, you know, and it's not really where the big benefits are going to be seen. But yeah, if you're like in gold metal zone on a Kaggle competition, and you need another 0.002% or something, then these are all things you can certainly try at that point. And I think you can remind me of auto and now, when you kind of write me of auto and now, like the regime of tools. I don't know how you feel about how you feel about those things. Yeah, last night, actually, so you'll have to catch up to see what I said. If you haven't seen the lesson, yeah. I'll mention also reading Kaggle winners descriptions of their approaches is great. But you've got to be very careful because remember, like Kaggle winners, the people who did get that last 0.002%, you know, because like everybody found all the low hanging fruit and the people who won grab the really high hanging fruit. And so every time you win a Kaggle winner's description, they almost always have complex, ensembleing methods. And that's why, you know, in like something like a big image recognition competition, it's very hard to win or probably impossible to win with a single model, unless you invent some amazing new architecture or something. And so you're kind of, you might get the impression then that ensembleing is the big thing that gets you all the low hanging fruit, but it's not. Ensembling is the thing which, or particularly complex, ensembleing is a thing that gets you that last fraction of a fraction of a percent. Yeah, of course. Yeah, of course. Yeah, TTA will concept, right? So I'm trying to... I mean, TTA? TTA, sorry. Yeah, TTA. Yeah, no time to do. Yeah, so if I understand, like I'm trying to understand conceptually why TTL improves the score, because technically when you're training, it is using those augmented sort of pictures and providing them, providing a percentage number. But when you're when you run that TTA function, why is it able to predict better? Sure. So like, you know how sometimes you're like, are looking at some like, I don't know, a sprue head or a pla or a socket or something that's really small, that you can't quite see like what, how many pins are in it or what type is it or whatever. And you're kind of like looking at it from different angles, and you're kind of like put it up to the light and you try to like, and at some point you're like, okay, I see it, right? And there's like some angle and some lighting that you can see it. That's what you're doing for the computer. You're giving it different angles and you're giving it different lighting in the hope that in one of those it's going to be really clear. And for the ones where it's easy, it's not going to make any difference, right? But for the ones where it's like, oh, I don't know if it's this disease or that disease, but oh, you know, when it's a bit brighter and you kind of zoom into that section like, oh, now I can see. And so when you then average them out, you know, or the other ones are all like, oh, I don't know which kind of is, which kinds of it's like 0.5, 0.5, and then this one is like 0.6. And so that's the one that in the average it's going to end up picking. That's basically what happens. It also has another benefit, which is when we train our models, I don't know if you've noticed, but our training loss generally gets much lower than our validation loss. And sometimes our validate sometimes our, well, so basically like what's happening there is that on the training set, the model is getting very confident, right? So even though we're using data augmentation, it's seeing slightly different versions of the same image dozens of times. And it's like, oh, I know how to recognize these. And so what it does is that the probabilities that associates with them is like 0.9, 0.99, you know, like it's like, I'm very confident of these. And it actually gets overconfident, which actually doesn't necessarily impact our accuracy, you know, to be overconfident. But at some point it can. And so we are systematically going to have like overconfident predictions of probability. Even when it doesn't really know, just because it's really seen that kind of image before. So then on the validation set, it's going to be, you know, over picking probabilities as well. And so one nice benefit is that when you average out a few augmented versions, you know, it's like, oh, 0.9, 0.9 probability is this one. And then on the next, it's like, next, or bent version with the same image, it's like, oh, no, 0.1 probability is that one. And they'll kind of average out to much more reasonable probabilities, which can, you know, allow it sometimes to, yeah, combine these ideas into an average that that makes more sense. And so that can improve accuracy, but in particular, it improves the actual probabilities to get rid of that overconfidence. It's a fair to say that when you train without, when you train, it's not able to separate the the replicated sort of images or the distorted slightly the variant of the original image. But when you use the TTA, it is able to group all the four images and it. Well, wait, that's what TTA is. We present them all together and average out that group. Yes. But in training, we don't indicate in any way that they're the same image, or that they're the same underlying object. One other question, Jeremy, we, I'm going to be touching and stumbling and how to pick the best and stumbling I was going to ask about that. But another question is, we have a fairly unbalanced data set, I guess, with the normal versus the disease states. Yeah. You're doing augmentation. Is there any benefit to sort of over representing the minority classes? So let's pull away augmentation. So it's actually got nothing to do with augmentation. So more generally, when you're training, does it make sense to over represent the minority class? And the answer is maybe. Yeah, it can. Right. And so, okay, so just for those who aren't following the issue, Matt's talking about is that there was, you know, a couple of diseases, which appear lots and lots in the data, and a couple which hardly appear at all. And so, you know, do we want to try to balance this out more? And one thing that people often do to balance it out more is that they'll throw away some of the images in their more represent, highly represented classes. And I can certainly tell you straight away, you should never ever do that. You never want to throw away data. But Matt's question was, well, could we, you know, over sample the less common diseases? And the answer is, yeah, absolutely you could. And in first AI, if you go into the docs, yeah, where is it? There is a weighted data loader somewhere. Weighted. Search for that. Here we go. Of course, it's a callback. So if you go to the callbacks data section, you'll find a weighted DL callback or a weighted data loader's method. Are you sharing your screen? I'm not. No, I'm just telling you where to look. Thanks for checking. So, yeah, I mean, let's look at that today, right? Because I kind of want to look at like things we can do to improve things today. It doesn't necessarily help. Because it does mean, you know, given that you're, you know, let's say you do 10 epochs of 1000 images, it's going to get to look at 10,000 images, right? And if you over sample a class, then that also means that it's going to get it's going to see less of some images and going to get more repetition of other images, which could be a problem, you know, and really it just depends on depends on a few things. If it's like really unbalanced, like 99% all of one type, then you're going to have a whole lot of batches that it never sees anything of the underrepresented class. And that's so basically there's nothing for it to learn from. So at some point, you probably certainly need weighted sampling. It also depends on the evaluation. You know, if people like say in the evaluation, okay, we're going to kind of average out for each disease, how accurate you were. So every disease will then be like equally weighted. Then you would definitely need to use weighted sampling. But in this case, you know, presuming, presuming that the test set has a similar distribution as a training set, weighted sampling might not help. Because they're going to care the most about how well we do on the highly represented diseases. I'll note my experience with like oversampling and things like that. I think one time I had done with I think diabetic retinopathy, there was a competition for that. And I had used weighted sampling or oversampling and it did seem to help. And then also a while back, I did an experiment where I think this was back with Fassai version one, where I took like the minced data set and then I I like artificially added some sort of imbalance. And then I trained with and without weighted sampling. And I saw like there was an improvement with the weighted sampling on accuracy on like just a regular means the validation set. So I so from that, from those couple experiments, I'd say like, I've at least seen some help and improvement with weighted sampling. Cool. And was that cases where the data set was like highly unbalanced or was it more like the data set that we're looking at at the moment? It wasn't highly unbalanced. It was maybe like, I don't know, like maybe like, yeah, just 75% versus 25% or something like that. It's not like 99 versus 1%, nothing like that. It was more well, which isn't that bad. Let's go. Let's try it today. Yeah. I see we've got a new face today as well. Hello, Zach. Thanks for joining. Hey, hey, glad I could finally make these. Yeah. Are you joining from Florida? No, I'm in Maryland now. Maryland now. Oh, I have my own place. That's quite a change. Yes. Much more up north. Okay, great. So let's try something. Okay, so let's connect. Right. To my little computer upstairs. There are a way to shrink my zoom out of the way. It takes up so much space. Hide floating meeting controls. I guess that's what I want. Control Alt Shift H. Well, press Escape to show floating meeting controls. That doesn't work very well with VIM. Oh, well, control Alt Shift H. Okay. All right, we're not doing tabular today, so let's get rid of that. So I think what I might do is, you know, because we're iterating, well, I guess we could start with the multi task one. This is our kind of like thing is to try to improve version. So close that. I'll leave that open just in case we want it. Okay. By the way, if you've got multiple GPUs, this is how you just use one of them. You can just set an environment variable. Okay, so this is where we, this is where we did the multi target model. Okay. Okay, just moved everything slightly. Comp. Comp. Not comp pass. Right, that's where we were. Okay. So, oh, now what? Let's break it in. Data block. Get image files. Well, this is working the other day. So I guess we better try to do some debugging. So the obvious thing to do would be to call this thing here, get image files on the thing that we passed in here, which is train pass. Okay, so that's working. Then the other thing to do would be to check our data by doing show batch. Okay, that's working. Then I guess, all right, and it's showing our two different things. That's good. Oh, is it? We've got the two category blocks, so we can't use this one. We have to use this one. So fit one cycle. Yeah, okay. So to remind you, we have, this is the one where we had two categories and one input. And to get the two categories, we used the parent label and this function, which looked up the variety from this dictionary. Okay, and then when we fine tuned it, and let's just check yeah, c equals 42. So that's our standard set. We should be able to then compare that to small models, train for 12 epochs. And then that was this one. Part two. And let's see. They're not quite the same because this was 480 squish. Or else this was rectangular pad. Let's do five epochs. Let's do it the same as this one. Yeah, let's do this one because we want to be able to do quick iterations. Let's see, resize 192 squish. There we go. And then we trained it for.01 with FP 16 with five epochs. All right. So this would be our base case. This is our base case 0.045. This will be our next case. Okay, so while that's running, the next thing I wanted to talk about is.progressive resizing. So this is training at a size of 128, which is not very big. And we wouldn't expect it to do very well. So, but it's certainly better than nothing. And as you can see, it's not error. Disease error. It's down to 7.5% error already, and it's not even done. So that's not bad. And in the past, what we've done is we've said, okay, well, that's working pretty well. Let's throw that away and try bigger. But there's actually something more interesting we can do, which is we don't have to throw it away. What we could do is to continue training it on larger images. So we're basically saying, okay, this is a model which is fine tuned to recognize 128 by 128 pixel images of rice. That's fine tuned to recognize 192 by 192 pixel images of rice. And we could even like, there's a few benefits to that. One is it's very fast to do the smaller images. And it can recognize the key features of it. So this lets us do a lot of epochs quickly. And then the difference between small images of rice disease and large images of rice disease isn't very big difference. So you would expect to probably fine tune to bigger images of rice disease quite easily. So we might get most of the benefit of training on big images, but without most of the time. The second benefit is it's a kind of data augmentation, which is we're actually giving it different sized images. So that should help. So here's how we would do that. Let's grab this data block. Let's make it into a function. Get DL. Okay, and the key thing I guess we're going to do. Well, let's just do the item transforms and the batch transforms as usual. Oops. So the things we're going to change are the item transforms. And the batch transforms. And then we're going to return the data loader for that, which is here. Okay. Okay, so let's try going up a bit. DL equals get DL. I guess it should be get DLs really, because it returns data loaders get DLs. Okay, so let's see what we did last time. Let's be scaled up a bit. So this is going to be data augmentation as well. We're going to change how we scale. So we'll scale with zero padding. And that's got to 160. Okay. Okay. So our, where's our squish one here? So the squish here got.45. Our multitask got.48. So it's actually a little bit worse. This might not be a great test actually, because I feel like one of the reasons that doing a multitask model might be useful, because it might be able to train for more epochs. Because we're kind of giving it more signal. So we should probably revisit this with like 20 epochs. Any questions or comments about progressive resizing while we wait for this to train? Sorry, I can't see how you progress with the change. I messed it up. Whoops. Thank you. I have to do that again. I actually did it. Oh, and we need to get our deals back as well. Okay. Let's start again. Okay. And let's, in case I mess this up again, let's export this. I'll call this like stage one. See. Yeah, the problem was we created a new learner. So what we should have done is gone learn.thels equals deals. That's actually, so that would actually change the data loaders inside the learner without recreating it. Was that where you were heading with your comment? Give me the first unfreeze method, right? Like, I'm going to be using that. Sorry. There was an unfreeze method like in the same thing in the book actually mentioned, it's using the unfreeze method. So, there is an unfreeze method. Yes. What were you saying about the unfreeze method? Isn't an unfreeze required for progress you're resizing? Am I wrong? No, because fine tune has already unfrozen. Although I actually want to fine tune again. So if anything, I kind of actually want to, actually want to rephrase it. Because we've changed the resolution, I think fine tuning the head might be a good idea to do again. Which line of code is doing the regressive resizing part? Just to be clear. It's not online of code. It's basically this. It's basically saying our current learner is getting new data loaders. And the new data loaders have a size of 160, or else the old data loaders had a size of 128. And the old data loaders did a pre sizing of 192's quish, and our newly data loaders are doing a pre sizing of rectangular padding. Does that make sense? Why are you calling it progressive in this case? You're going to keep changing the size or something like that. Yeah, it's changing the size of the images without resetting the learner. Just looked it up because I was curious. Fine tune calls a freeze first. I had a feeling it did. For that first set. Thanks for checking, Zach. So this time, let's see, it'll be interesting to see how it does. So after the initial epoch, it's got 0.09. Realize previously it had 0.27. So obviously it's better than last time, but it's actually worse than the final point. This time it got all the way to 0.418. Whereas this time it has got worse. So it's got some work to do to learn to recognize what 160 pixel images look like. Can I just clarify Jeremy? You're doing one more step in the progressive resizing here. It's not an automated resizing. Correct. Correct. Yeah. There isn't anything in Fast.io to do this for you. And in fact, this technique is something that we invented so it doesn't exist in other bibaries at all. So yeah, it's the name of a technique. It's not the name of like a method in Fast.io. And yeah, the technique is basically to replace the data loaders with one's analog size. And we invented it as part of a competition called Dawnbench, which is where we work very well on a competition for ImageNet training. And Googled and talked the idea and studied it a lot further as part of a paper called EfficientNet V2 and found ways to make it work even better. Oh my gosh, look at this. So we've gone from 0.418 to 0.0336. Have we done training at 160 before? I don't think we have. Oh, I should be checking this one. 128, 128. 128, 171 by 128. No, we haven't. This is a 256 by 192. So eventually I guess we're going to get to that point. So let's keep going. So we're down a 2.9% error. How did you come up with that idea for this? Is this something that you just wanted to try or did it happen? Did you stumble upon it while looking at something else? Oh, I mean, it just seemed very obviously to me like something which obviously we should do because like we were spending, okay, so on Dawnbench, we were training on ImageNet. It was taking 12 hours, I guess, to train a single model. And the vast majority of that time, it's just recognizing very, very basic things about images. It's not learning the finer details of different cat breeds or whatever, but it's just trying to understand about the concepts of like fur or sky or metal. I thought, well, there's absolutely no reason to need 224 by 224 pixel images to be able to do that. It just seemed obviously stupid that we would do it. And partly it was like also like I was just generally interested in changing things during training. So one of the particular learning rates, right? So the idea of changing learning rates during training goes back a lot longer than Dawnbench. That people had been generally training them by having a learning rate that kind of dropped by a lot from then stayed flat and dropped by a lot of stayed flat. And Leslie Smith in particular came up with this idea of gradually increasing it over a curve and then gradually decreasing it following another curve. And so I was definitely in the mindset of like, oh, there's kind of interesting things we can change during training. So I was looking at like, oh, what if we change data augmentation during training, for example, like maybe towards the end of training, we should turn off data augmentation so it could learn what unalgmented images look like because that's what we really care about, for example. So yeah, that was the kind of stuff that I was kind of interested in at the time. And so yeah, definitely this thing of like, you know, why are we looking over 224 by 224 pixel images the entire time? Like that just seemed obviously stupid. And so it wasn't something where I was like, wow, here's a crazy idea. I bet it won't work. As soon as I thought of it, I just thought, okay, that this is definitely going to work. You know, that did. Interesting. Thanks. Yeah, I'm worries. One question that I have for you, Jeremy. There was a paper that came out like in 2019 called Fixing the Test Train Resolution Discrepancy. Yeah, I've very clear. Yeah, were they like trained on 224 and then did inference finally on like 320 by 320? Yeah. Have you seen that still sort of work? Have you done that at all in your workflows? I mean, honestly, I don't remember. I need to revisit that paper because you're right. It's important tonight. I, I'm going to be You know, I would generally try to fine tune on the final size. I was going to be predicting on anyway. So yeah, I guess we'll kind of see how we go with this, right? I mean, you can definitely take a model that was trained on 224 by 224 images and use it to predict 360 by 360 images. And it will generally go pretty well. But I think it'll go better if you first fine tune it on 360 by 360 images. Yeah, I don't think they tried pre training and then also training them like 320 versus just 320 and the 224. Yeah. So that would definitely be an interesting experiment. Yeah, it would be an interesting experiment. And it's definitely something that any of us here could do, you know, I think it'd be cool. Right. So let's try scaling this up. So we can change these two lines to one. And so this is one something I often do is I do things like, yeah, I think we don't have yours. Okay. So. I was just saying previously I had like two cells to do this. And so now I'm just going to combine it into one cell. So this is what I tend to do as I fiddle around as I title like gradually make things a little bit more concise, you know. Okay. Does it make sense to go smaller than the by the original pre training like, Convinate. Cock Comve next. Yeah, I mean, you can fine tune to any size you like. Absolutely. I'm just going to get rid of the zero padding because again, I want to like try to change it a little bit each time just to kind of. You know, it's a kind of augmentation, right? Okay. So, okay. So it's got to 192. You know, one thing I find encouraging is that, you know, my training loss isn't getting way underneath the validation loss. It's not like we're. Yeah, it feels like we could do this for ages before our error rates start going up. Interestingly, when I reran this, my error rate was much better point 418. You've got a good memory to remember these these old papers, it's very helpful to be able to do that. Usually what I wind up doing is my dad and I will email back and forth papers to each other. So I can just go through my scent, look at archive and usually if I don't remember the name of it, I remember the subject of it in some degree. Yeah, I can just go through it all. I mean, it's a very, very good idea to use a paper manager of some sort to save papers, you know, whether it be Mendelay or the Nodo or archive sanity or whatever or bookmarks or something. Yeah, because otherwise these things disappear. Personally, I just tend to like. Tweet or favorite tweets about papers I'm interested in. And then I've set up pin board.in. I don't know if you guys have seen that, but it's really nice little thing, which basically any time you're on a website. You can click a button and the extension and it adds it to pin board, but it also automatically adds all of your tweets and favorites and it's got a full text search of the thing that the URL is linked to, which is very helpful. See you favorite it's something that just says, Oh, shit. No, I actually wrote something that just said, Oh, shit. That was me. I think it was this. I mean, totally off topic, but this absolutely disaster. I hope it's wrong. But it's absolutely disastrous sounding. Paper that came out yesterday. That basically, where was this key thing? People who've had one covered infection have a list of one sequel, 8.4% to infections 23% three infections 36%. It's like my worst nightmare is like the more people get infected with covert, the more likely it is that they'll get long term. Simptance, which is horrifying. That was my post sheet. That's very horrifying. It's really awful. Okay, so keeps going down right which is cool. Let's keep bringing along as pose. I guess, you know what we could do is just grab this whole damn thing here. We kind of have a bit of a comparison. So we're basically going to run. Exactly the same thing we did earlier. But this time. With some pre sizing first. All right. So that'll be an interesting experiment. So while that's running. This is where I hit the old duplicate button. And this is why it's nice if you can to have the second card, because while something's running you can try something else. And visible devices. There we go. So we can keep working. Okay. So, wait a day to later. So this is something I added. To fast a while ago and haven't used much myself since. But if I just search for weighted, here it is. Here it is. So you can see in the docs. It shows you exactly how to use weighted data loaders. And so. We pass in a batch size. We passed in some weights. This is the weights is going to be 1234578. It's actually zero. And then some item transforms. So like these are kind of really interesting in the docs. In some ways. It's extremely advanced. And otherwise it's extremely simple. Which is to say if you look at this example in the docs. Everything is totally manual. So our labels are some random integers. Kind of the. I've even added a comment here. It's going to be in the training set. So our data block. It's going to contain one category block. Because we just got the one thing. And rather than doing get X and get Y. You can also just say getters. Because get X and get Y basically become getters. Which is a list of. Transformations to do. And so this is going to be a single getter or a single get X. If you like. Which is going to return the I. Label. And a splitter. Which is going to decide whether something's valid or not based on this function. So you can see this whole thing is like totally. Manual you know. So we can create a data set by passing in a list of the numbers from not nine. And a single item transform that's going to convert that to a tensor. And then our weights will be the numbers from not to seven. And so then we can take our data set. Or data sets. And turn them into data loaders. And then you can those are weights. So. With a batch size of one, we say show batch. We get back a single number. Okay. And it's not doing random shuffling. So we get the number zero because that was. The first thing in our data set. Let's see. What do we do next? Now we got to do an equals 160. So now we've got all of the numbers from not to 159. Or getters. Yes, forgetters. Yep. You mentioned this is for X or Y. This is a list. That's whatever. Right. This is so there is just one thing. I don't know if you call that X or you call it Y. It's just one thing. So if you have a get X and a get Y, that's the same as having a getters with a list of two things. Okay. So yeah. I think I could just write getters. This has been a just since I've got this, but I think I could just write get X here and put this not in a list. Would probably be the same thing. Okay. I'll probably handle a little bit of mystery that might be happening as well. Yeah. So the block has an input parameter, correct, which is how it determines what of the getters is X versus Y. Correct, which we actually looked at last time. Yeah. When we created our multi image block. Before you joined sec. But yes, useful reminder. Okay. So. So here we. See in a histogram of how often so our. We created like a little synthetic learner that doesn't really do anything, but we can pass callbacks to it. And there's a call back called collect data call back, which just collects the data that it's part that is called in the learner. And so this is how we can then find out what data was passed to the learner, and we can see that the number 160. Was received a lot more often when we trained this learner, which is what you would expect. This is the source of the weighted data loaded class. Here. And as you can see, other than the boilerplate. It's one, two, three, four, five lines of code. And then the weighted data loaders method is one, two lines of code. So there's actually a lot more lines of examples than there is of actual code. So often it's easier just to. Read the source code. Because, you know, thanks to the very layered approach to fast, we can do so much stuff. With so little code. And so in this case, if we look through the code, we're passing in some weights. And basically the key thing here is that we set if the, if you pass in no weights at all, then we're just going to set it equal to the number one, repeated n times. So everything's going to get one, a weight of one. And then we divide the weights by the sum of the weights so that the sum of the weights ends up summing up to one, which is what we want. And then, if you're not shuffling, then there's no weighted anything to do. So we just pass back the indexes. And if we are shuffling, we will grab a random choice of, based on the, based on the weights. Cool. All right, so there's going to be one weight per row. All right, let's come back to that because I want to see how our things gone. It looks like it's finished. Notice that the fav icon in Jupyter will change depending on whether something is running or not. So that's how you can quickly tell if something's finished. Point two, one, six. Point two, two, one. Okay. I mean, it's not a huge difference, but maybe it's a tiny bit better. I don't know, like it's. Two. You know, the key thing though is this lets us use our resources better, right? So we often will end up with a better answer. But you can train for a lot less time. In fact, you can see that the error was at point two, one, six back here. So, you know, we could probably have trained for a lot less epochs. So that's progressive resizing. Sir, is there a way to look at that and go. Actually, I'd like to take the outputs from the top nine. Because we had a better answer. That was the question we got earlier about that's called early stopping. And the answer is no, you probably wouldn't want to do early stopping. But you can't go back to a previous. Like epoch, there's no history. You can. You have to use the early stopping call back to do that. Cool. Okay. Okay. I'll look at that. Or there's other things you can use. As I say, I don't think you should. But you can. If I go training callbacks. Tracker. So, I think the other part of that is, yeah, is it counterproductive for. Yeah, it's kind of true if it works, but not if it doesn't. It won't. It's probably not a good idea. It probably will make it worse. Yeah. Okay. So the other thing you can do is a safe model call back, which saves, which is kind of like early stopping, but it doesn't stop it. It saves the. Parameters of the best model during training, which is probably what you want instead of early stopping. So, I think that's a good idea. I'm going to do that either for the same reason we discussed earlier. Why shouldn't you do this? It seems like you could just ignore it if you didn't want it or. Like it might not hurt you. Well, so this, this actually automatically loads. The best set of parameters at the end. And. You know, you're just going to end up. You know, with this kind of like model that just so happened to. Look a tiny bit better on the validation set at an earlier epoch, but at that earlier epoch. The landing rate hadn't yet stabilized and it's very unlikely it really is better. So you've probably actually just picked something that's slightly worse. And, you know, made your process slightly more complicated for no good reason. So, you know, it's a big big big big big better on money. It doesn't necessarily say anything about the phone or hidden. Yeah. Yeah. Yeah. We have a strong prior belief that it will improve each epoch. Unless you're overfitting. And if you're overfitting, then you shouldn't be doing early stopping. You should be doing more augmentation. And I also have a great opportunity for somebody to document the arguments. Because I'm like curious what add in does. Yes, that would be a great opportunity for somebody to document the arguments. And if somebody is interested in doing that. We have a. Really cool thing called documents, which I only invented after we. Oh, this is like, it's not. I should delete this because this is the old version. Yeah. That a fast core. And documents. You document each parameter by putting a comment after it. And you document the return by putting a comment after it. And. Exactly. So, I'm going to go ahead and add documents to add documents comments to everything in fast AI, which of course is not finished because fast AI is pretty big. And so here's an example of something that doesn't yet have documents comments. So if somebody wants to go and add a comment to each of these. Things and put that into a PR. I'm. I know what we. Once you do that to do it. I was just going to say that kid. Something we should do Zach is to actually include an example in the documents. Documentation of what it ends up looking like in N.B. Dev. Because I can see that's missing. That might be a good idea. I can see if I can get on that tomorrow. Yeah. Sorry. How about what you're saying? I just wanted to encourage everybody that, like. Writing the documentation is like an excellent way to learn deeply what. How everything works. And like you get on. You know what ends up happening is you get you write this documentation and, you know, somebody like Jeremy will review it carefully and let you know what you don't understand. And that's. Yeah, that's how I learned about some other, you know, some other fast AI library. So I highly recommend it going, you know, going doing that. And here's what it ends up looking like. Right. So here's optimizer and you can see it's got a little table underneath. And if we look at the source of optimizer. You'll see that. Each parameter has a comment next to it. So there's parameters automatically turned into. Into this table. All right. So I'm going to make a super cool. Yeah, super cool. This sounds like a good place to wrap up. Anybody got any. Questions or comments or anything before we wrap up. I have a question regarding to. Decising. Yes. We didn't do actually. I'm fine after you just that don't you think is something helpful. The L I find, did you say? Yeah. Yeah. Yeah, I. To be honest, I don't use our find much anymore. Nowadays, because. You know, at least for object recognition in computer vision. The optimal learning rates pretty much always the same. It's always around point. Yeah, there's a reason to believe that we have any need to change it just because we changed the. Resolution. So yeah, I would, I wouldn't bother. Just leave it where it was. German, if you're training and validation loss is still decreasing after 12 people. Can you pick up and train for a little longer without restarting? You can. The first thing I say is you shouldn't be looking at the validation loss to see if you're overfitting. You should be looking at the error rate. So the validation loss can get worse whilst the error rate gets better and that doesn't count as overfitting because the thing you want is to improve as the error rate. That can happen if it gets over confident, but it's still improving. Yeah, you can keep training for longer. Because we're using if you're using fit one cycle or fine tune and fine tune users fit one cycle behind the scenes. Continuing to train further, your learning rates going to go up and then down and then up and then down each time, which is not necessarily a bad thing. But you know, if you. Yeah, if you basically want to keep training at that. At that. You know, at that point, you would probably want to like decrease the learning rate by maybe 4x or so. And in fact, you know, I think after this, I'm going to rerun this whole notebook. But half the learning rate each time. So I think that would be. Potentially a good idea. I have a question. I don't know if it's too late, but I think it might be useful to discuss when you do the progressive resizing. What part of the model gets dropped? Like, what, you know, is there some part of the model that needs to be. Reinitialized for the new nothing needs to be re initialized. No, I found this on the web. You're talking to me, but you're talking to Siri. I'm offended. Siri, teach me deep learning. Yeah, conf next is what we call a resolution independent architecture, which means it doesn't. It works for any input resolution. And. Time permitting in the next lesson. We will see how convolutional neural networks actually work, but I guess a lot of you probably already knows so. For those of you to do. If you think about it, it's basically going patch by patch and doing this kind of. Many matrix multiply for each patch. So if you change the input resolution, it just has more patches to cover. But it doesn't change the parameters at all. So there's nothing to. Reinitialized. Does that make sense, Hamill? Yeah, that makes sense. I was just asking for the. For the record. Fair enough. Yeah. Just a question. Yeah. I was just going to quit night to say his, his resonant resolution independent. Yep. Good. Yeah. Everything we use is normally, but in the, like. Have a look at that, like best fine tuning. Models notebook and you'll see that two of the best ones are called. V it and swing. And also swing V two. None of those are resolution independent. Although there is a trick you can use to kind of make them as a independent, which we should try out. In a future walkthrough. Is that fiddling with the head. Oh, there's a Tim. There's a thing you can pass to Tim. I don't know if we can use it to support progressive resizing or not. It'll be interesting to experiment with. It's basically changing the positional encodings. I have a question. Interest. Yeah. After you've done your experiments, progressive resizing. In fine tuning. How do you infest AI train with the whole train? I never got around to do that. Do you. I almost never do. Like instead I do what we saw in the last walkthrough, which is, I just train on a few different. Randomly selected valid training sets. Because that way. You know, you get the benefit on sampling. You're going to end up seeing all the images at least one anyway. And you can also kind of see if something's messed up because you've still got a validation set each time. So yeah, I used to like do this thing where I would create a validation set with a single item in. To like get that last bit of juice, but I don't, I don't even do that anymore. Okay, thanks. No worries. All right gang. Enjoy the rest of your day slash evening. Nice to see you all. Bye. Goodbye. Thanks. Thank you. Bye.
