 Okay. All right. So this is the repo. Fasti, paper space set up. I've started a machine. I'll cd to my home directory. I'll get clone the repo. I'll cd into the thing I get cloned. I'll run.slash setup.sh. Okay. It says install complete. Please start a new instance. So then I'll stop the machine. And then I'll start a machine. And that's going to install a prerun.sh script, which is going to set up all these things and all these things. And it's going to install a.bash.local script, which will set up our path. And it's going to also install things to set up things for installing software. PIP I for PIP install and Mamba I for Mamba install. So we now have a machine running. And so we should now put a credit terminal. I just pressed press terminal. Something's happening. Great. Okay. Check creating a terminal here then. Okay. Much better. All right. So in theory, if we look at our home directory, oh, look at that. Well, this stuff is now sim linked to slash storage. So I should be able to PIP I fast AI and get the latest version. I wonder if I can add a minus U to say upgrade. Yes, I can. So that's how I get the latest version. And so that should have installed it locally. There it is. And okay. So now if I create a notebook. Import fast AI. Fast AI version. Oh, look, that's a good start. Okay. Next question. Can we install binaries? For example, universal syntax. I remember install universal syntax. There we go. So you see the nice thing about this is even all this persistent stuff we're installing into, you know, all works on the free paper space as well. So we should now be able to check C tags. Ta da. It works. And which one is it? And that is actually in our storage. Oh, so I think we've done it. What do you guys think? Simple enough? It's good. All right. It's good. Okay. So next step is I thought we might try to fix a, I don't know if you call it fixing a bug or maybe it's probably we could generously call it adding an enhancement to fast AI, which is to add normalization to Tim models. So all right. So it's grab fast AI. Now this is where, so when I get blown this, so let's go to notebooks. So slash notebooks is persistent on a particular machine. And I think this will not work because I'm using SSH. Oh, it's already there. That's interesting. Oh, you know, so there's a bug in our script, which is I didn't pop D. So let's fix that. Prerun.sh. I did a push D at the start. No pop D at the end. Okay. Fixed. All right. No worries. That means. Okay. Yes. No worries. All right. So let's restart this. And then I'll tell you about the bug we're fixing while we wait for it. Okay. So, so, so normalization is where we subtract the means and divide by the standard deviation of each channel for vision. And that goes, that's a transform called normalize. And we need to use the same standard deviation and mean that was used in the, when the model was pre trained. Because you know, there is, you know, so some people will normalize. So it's everything's between zero and one, some will normalize. So it's got a mean of zero and a standard deviation of one. So we need to make sure we use the same. You don't divide by the same thing to track the same thing. If you look at vision learner. Vision learner has a normalized parameter. And if it's true, then it will attempt to add the correct normalization here. So if it's not a pre trained model, it doesn't do anything because it doesn't know what to normalize by. Otherwise, it's going to try and get the correct statistics from the models metadata. So the models metadata is here, model underscore meta. And it's just a list of models with, with metadata and the metadata here, stats, image net stats. So the image net stats is the mean and standard deviation of image net, which I can't quite remember where that comes from, but that's something we import from somewhere. So none of these are 10 models. And so that means currently 10 models aren't normalized. Now. Tim has its own stats. Not this, not this. There's a lot of stuff in TMI still I haven't looked into. I actually haven't used this transforms factory. Maybe in fast AI three, we should consider using more of this functionality from Tim. It's like a configuration for them. I guess we can just try and find it. I actually forgot to edit this. My bad. Stop the machine. Here we go. Okay. So we can just do this locally now. All right. So this happens in vision learner. And Tim is optional. You don't have to use it. But if you do, then we have a create Tim model, which you don't normally call yourself. Normally you just call vision learner and you pass in an architecture as a string. And if it's a string, it will create a Tim model for you. So there's a best models, for example. Let's say conv next or something like that. I don't know what conv it is. Never tried that one. Let's do a tiny. So we can create a model using create model. We pass in a string. And I have a feeling that's, yeah, that's got a config. Yeah, see, and it's got a mean and a standard deviation. So models equals Tim dot list models. Maybe just do pre trained ones. I wonder if they all have this for M in models. So let's create a model and have a look at M dot default config. I mean. And to the deviation. Yeah. Yeah, so you can see a lot of them use.5. And then some of them use image stats stats. And guessing they're the only two options. So okay, so hopefully you get the idea. Jim, you just read out usually putting the image in the mean to the user and standard duration should be one by. I mean, not necessarily. Sometimes people make the minimum zero in the maximum one. But what we need to do is use the same stats that it was pre trained with. Because we want our range to be the same as the range that was pre trained with otherwise our data has a different meaning. So let's go to add norm. Okay, so here's add norm. And it's being passed a meta. Stats. So this only works for non team. So how about we put this here and we'll create an else. Or I guess really an L if. And then here we'll have for Tim if normalized. We could have a Tim normalized. We can refactor out some duplicate code later. And basically for Tim, we're going to be passing in the architecture. We don't need to pass in the architecture, we can just pass in the model. And to protect against future like ability to pass in other types of the strings that aren't Tim, do you think there's any benefit in having like default normalization function that if you pass through, you can actually do your own normalization? No, because my answer to all of those questions is always you won't get a native. So I very intentionally don't do like, you know, dealing with things that might not happen in the future. It'd be simpler just to create your own vision liner because that looks like there's not much going on there that you can duplicate if you wanted to have support for a different model. Yeah, exactly. I mean, this is just a small little wrapper really. You can call create Tim model or create vision model. You can call learner. You can call create head. Yep. Okay. So we'll call that. So the normalize takes a mean and a standard deviation. So it should be just those two things, I guess. So, all right. Okay. Tim normalize using the model and pretread. Okay. I see I already had an else there. So do that. There we go. And okay. So let's test this out. Tls. So what happens when you add a transform? Add a transform to each data loader in it. Okay. So what does that do? Yeah. What did I do wrong? Okay. Oh, it's part of I see it's part of. Okay. That's a bit confusing. Right. Okay. So let's find sometimes it's just easiest to look at the code. Add trifons. I see. So it's just calling add. I see for this particular event. And we're adding it. I see we're adding it to the after batch event. So we should find there's a after batch event. Here we are. I see. And there's our transforms. So if we call vision learner, that should change our data loader. Yep. Now I've got normalize using image net stats. And if we now try it for a string version. Oh, no, that's interesting. Okay. What happened differently? Oh, I see. We need to recreate the data loaders for this test. So that doesn't have normalize anymore. And that gives us. Okay. That gives us an error. This is as we're passing a sequential object. Okay. That makes sense. Because create Tim model actually, yeah, modifies things. That's why. And it creates a sequential model because it's got the head and the body in it. So we need to change how we do this. All right. This is Tim body here is the model. And oh, look here we use default config to get stuff here. So Tim body is called from here. I guess like it would be nice to know how Tim does this exactly. Where does that default config come from? So when we call Tim.create model, I wonder if we should. Take a look. default config. It's probably a lot. Here's data config.py. Where does it get set? It seems like this button needs some restructuring. It's not surprising. It was originally built not to expect between stuff with Tim. Create vision model. Create body. Maybe we should change how these work. So let's do something we think about doing some redesign maybe. So the idea of the redesign I guess would be that this doesn't instantiate the model. But it assumes it's already instantiated. So we remove that. We're creating body with model. So we may as well just do that directly. Okay. Make this a function. It's new on each time. Okay. So in this refactoring, we now are passing around models, not architectures. The model model doesn't change. Okay. So this changes. So now we say model equals arch pre change. So we're going to do the same thing for Tim. So we're going to pass in a model. Okay. So it's going to be the same here. Let's see a vision minus two works. Okay, it does. So maybe we should move keep moving this back further and further. So we're going to make Tim work. Do that. Maybe we'll just call that the Tim model. Okay. So the problem with that is the keyword arguments. So there's a lot of this is crazy. There's a lot of keyword arguments. So I think actually what we'll do is we'll do it up here. Okay. And so Tim body. Doesn't need quags anymore. So what we might do is we'll say this is the result. And we'll return the things. Or even return. Those two things. Now we've got the config. And so we can pass the config. Like so. So how much we broke. Okay. So create Tim model. Yes, we do pass in an architecture after all. So we'll just change that back. Oh, that looks hopeful. So we should find that if we create a. And check its default config. Yep, that looks good. Now, comfnext tiny on the other hand uses image nets stats. That looks very hopeful. So somebody feels like an interesting and valuable problem to solve. Making create unit model work with Tim would be super helpful. All right. Now create unit model. Needs to do the same thing. So we can do this. It has create vision model, which is to actually instantiate the model. Is anybody potentially interested in having a go at doing unit models with Tim? If so, did you want to talk about it? I'd be interested. Okay. So. Are you somewhat familiar with using units in general and in the dynamic unit? A little bit. I'm training one at the moment. That's my maximum experience. And then I've been through some notebooks to walk through. Last day I wanted everything. Great. So, okay. So. The interesting. Okay. So you know, the basic idea of a unit is. That it has. Not just the usual kind of. Downward sampling path where the image is getting kind of effectively smaller and smaller as it goes through convolutions with strides. And we end up with. You know, a kind of a very small set of patches. And then rather than averaging those to get a vector and using those as our features to our head. Instead, we go through reverse convolutions, which are things which make it bigger and bigger. And as we do that, we also don't just take the input from the previous layer of the up sampling, but also the input from the equivalently sized down sampling size down sampling there. Before fast AI, all units had to be only handled a fixed size. But Karen did was he created this thing called the dynamic unit, which would look to see how big each size was on the downward path and automatically create an appropriate size thing on the upward path. And that's what the dynamic unit does. Fast AI has been very aggressive in like using pre trained models everywhere. So something we added to this idea is this idea that the downward sampling path can be can have a pre trained model, which is not rocket science. Obviously, it's like this this one line of code. The So to understand, like at the moment I'm using say like a resident 34 does that mean the down path is a resident 34 backbone and then there's a reverse resident 34 being automatically generated. And not a reverse, it's not a reverse resident 34. It's, it is a resident 34 backbone. So here's our dynamic unit unit. The upward Sam, the up sampling path is has a fixed architecture, which is a bar indeed res blocks. But they're not like, if you use as a downward sampling part, you know, down sampling of the I T the upward sampling is not going to be a reverse V it, you know, it's not a mirror. No, exactly. It's just a bit. Would there be an advantage in doing that or is it just not really helpful. I don't see why there would be. I'd also don't see why there wouldn't be. But he's tried it as far as I know I don't even know if there's such a thing as an up sampling transformer block. I, there may well be without digressing. Yeah, there's no need to worry about that. The key thing is that in the downward sampling path. What we do is we, we have the downward sampling bit we call the encoder. Okay. And what we do is we do a dummy eval. Now a dummy eval is basically to take a, I can't remember like I either a zero length batch or one length batch like a very small batch and pass it through. At some image size. And we use, I believe we use hooks if I remember correctly. Seven to my screen. My screen's gone crazy. Okay. Yeah, so we've got these hawks. Yes. Okay, so we use fast AI's walk outputs function, which says, I want to use pytorch walks to grab the outputs of these layers. And so. What is the CCCH indexes. So this is yeah okay so that's a great question so this is the indices of this is the key thing this is the indices of the layers where the size changes. Right. So that's where you want to. That's where you want the cross connection. Right. Either just before that or just after that, you know. So get, get, get the indices with the size changes. So the sizes. Here, model sizes. So we hook outputs. We do a dummy of owl and we find the shape. Of each thing and yeah so he can see dummy of owl is using just a single image. And so yeah this just returns the shape of the output of every layer. That's going to be in sizes and so then this is just a very simple function, which just goes through and finds where the size changes. Okay. And so this is the indices of those things. So now that we know where the size changes we know where we want our cross connections to be. Now for each of the cross connections we need to store the output of the model at that point because that's, that's going to be an input in the up sampling block. So these sfs. For each unit block we create. So for each change in the index for each of sampling block, you have to pass in that that. Those outputs. Sampling side. So this is the index where it happened. And so this will be the actual. So if we go to the unit block. So it looks like it's so it's the size of that list minus one is that having a unit blocks get created on the other side. So it's going to be past the hook, right, which is, and so that's just the hook that was used. That's the hook that was used on the down sampling side. And from that we can get the stored activations. And so those stored activations then. So this is the shape of those stored activations. And this is a minor tweak. So let's just ignore this if block for a moment. Basically, all we then do is we take those activations, stick them through a batch norm. And then we can concatenate them with the previous layers up sampling. And check that through a rally. And then we do. And the comm comms aren't just comms. They're fast AI comms, which can include all kinds of things like batch norm activation, whatever. So it's, it's a, some combination of batch norm, you know, activation, convolution. You can, you can also do up sampling. So it's transpose. That's not can go first or last, whatever. So that's quite a, you know, a very rich convolutional layer. Okay, so then this if part here is that it's possible that things didn't quite round off nicely, so that the cross connection doesn't quite have the right size. And if that happens, then we'll interpolate the cross connection to be the same shape as the up sampling connection. And again, I don't know if anybody else does this, but this is to kind of make it so that the dynamic unit always just works. That's the basic idea. Yeah, so to make this work for Tim. You know, this encoder. It needs to know about the spots right. Oh, no, it wouldn't do checks with spots. So, so, so honestly, this, this might almost just work. Like, I don't like, I don't think it does. I think somebody tried it at night. Right. But yeah, it, it would. You know, to, to, to figure out what doesn't work. You know, you would need to change this line to say, Oh, if it's a string create trim model otherwise do this, you know. And then you'd like create body would need to be creative. Tim body if it's a string. So like at minimum, do the same stuff that create vision model does. And then, yeah, and then see if this works. Right. Well, now I will say, if you do get it working. Tim does have an API to actually tell you where the feature size has changed. So like you could actually optimize out that dummy of all stuff, but I don't even know if I bother because it makes the code more complex for no particular benefit. Yeah, sure. So, look, I think if you know, this you commit, this is a PR will definitely be looking at it. I was actually going to try conf next in my unit. So I had no idea. Actually, so that would have been, I would have noticed that already, but I just had no time. So I'd love to, because I, you know, tried resident 32. I've got particular results and I'd like to see. We can push it with a different model. Yeah. No, I mean, I think there'd be a lot of benefit to that. So, all right. So now we should run the tests. Just to know, would that all likely be in the same, a notebook that you're editing the vision letter is that when most of the source code is the unit learners, or is it a different. I don't know. I was just using this. To whatever automatically and VIMS so I was using VMC tags to jump around. So I don't, I have no idea where I was. So, yeah, so there's a models unit, so where the dynamic unit lives. Okay. So, is there anything unique about the fact that the Tim model doesn't, that's sort of an option that it cut the tail and head off. Does that need to be done with the unit architecture. Oh, got an error here. So, yeah, you absolutely have to cut the head off, because it comes with a default classifier head. So you only, you know, so you know, once you get it working, you'll probably find you can factor out some duplicate code between the unit and the vision letter. But yeah, basically have to cut off the classifier head in the same way that create Tim body does. And I don't think you'll need to change any input processing as far as I know. The vision, create vision model, you know, handles like, you know, if you've only got one or two or four channel inputs in the models of three channel input it handles that automatically. But Tim actually, I think Ross and I independently invented this as I know we both kind of automatically handle like copying weights if necessary or deleting weights if necessary or whatever but yeah so the same stuff them vision minor should should work there as well. So, interestingly, layers, the layers notebook. It doesn't work because it is actually creating a model, which is curious. And that will be easily fixed. That's interesting. Okay. Okay. So, the big question then is, do we still predict rice disease. That's compare. I don't know if it's going to make much difference or not. Because we're pretty careful about fine tuning the batch norm layers. It's actually interesting to see whether normalization matters as much as it used to. It used to be absolutely critical. Is it possible to create like a layer that learns the normalization sort of thing. Yeah, I mean that's basically what batch norm does, you know. It's a those weights in the batch norm layer are basically learning the aggregate of that batch that optimally give the best activations for the next layer. Yeah, exactly. Yeah. Yeah, it's just, it's just, you know, multiply by something and add something. So it's finding what's the best thing to multiply by and add by. So, let's take a look. So I mean, all right, so this got. It's got 44% error. Yeah, so I mean, it's a bit disappointing after all that work. It doesn't actually, I mean, this is fascinating. Like, yeah, when you fine tune the way we do. Basically doesn't really matter, you know. And let's just double check it actually is. It actually is working. It would be fair to say that the one advantage would be if you wanted to use pre train models without fine tuning, you definitely want the statistics in there. Yes, absolutely. I mean, I don't know if that's an actual thing that people do, but yes, if you did. All right, so we did deals.train.afterbatch. Yep, there it is. Groovy. Yeah, it's funny these things that, you know, we've been doing for years and I guess never question. I have a question relating to that because one of the things I wanted to do is get this unit into a mobile app. So I use the latest torch script and it works with the demo app. I took similar analogs is broken from PyTorch. But of course, in there you need to provide the averaging statistics for the app. So it's like inference mode. So I wonder, I know that at the moment, the fast AI is kind of idea is that you dump everything as like a pickle, but conceivably it would be helpful if you could maybe extract those new fine tunes. Statistics or something for your deployment in particular environments. Is that how would I go about doing that? I mean, they're just parameters in batch, normally as you know, they're just parameters. So they'll be in the parameters. That tribute of the model. But like they're not really parameters that make sense independently of all the other parameters at all. So I don't think you would treat them any differently. If you use say image nets statistics when you're fine tuning and that's the result of your model, right? You're going to use that down the track as well. Well, yes and no, like that's what you'd normalize with, but you've got batch norm layers, which then obviously dividing and subtracting themselves. So yeah, I mean, you're those normalizations to that site going to change, but there isn't really any reason to, you know, it would only be if you trained a new model from scratch. I just want to have a look at this come the next one. So this is 27 to 18 24. Yeah, this is actually kind of what I thought might happen is on a slightly better model. We may be getting slightly better errors initially. And then as it trains a bit. Thanks. No difference. Cool. All right, so, yeah, I'd love people to try out fast AI from master because, tell me if any of your models look substantially better or even more important substantially worse. So, I'm going to want to normalize. Tim models. Okay. All right. Anybody have any questions before we wrap it up. Just with normalize. It's just the initial, it will be a bit more less than early on approach. Yeah, so like that, that, that, you know, well. At first you have random head. So at first it doesn't actually matter right it randoms random whether you normalize or not. So, maybe, you know, the after 10 batches. It's better or something. But, yeah, I don't know. It might be interesting to see if anybody notices a difference. I mean, it's just, this used to matter a lot, right, for a couple of reasons. One is that. Most people didn't fine tune models. Most people train most models and scratch until. Until fast AI came along pretty much. And then secondly, well, we didn't have batch nom. Right. So. It was totally critical. And then even when batch nom came along, we didn't know how to fine tune models with batch nom. So we just fine tuned the head. At that point, we didn't realize that you had to fine tune the batch nom as well. So I remember. Emailing Francois, the creator of Keras. And I was saying to him, like, I'm trying to fine tune your Keras model. And it's like. Bizarrely bad. Like, why, why is that? Well, probably doing the wrong thing. His documentation, whatever. And like, no, I'm pretty sure I'm doing the right thing. And I, yeah, I spent like three months trying to answer this question. Eventually, I realized it's like, holy shit, it's the batch normalize. I sent him an email and said, Oh, we can't fine tune. Keras models like this. You have to fine tune batch nom. Which I don't think they changed for years. Actually. Anyway, so those there, so those changes is why I guess. This whole normalization layer thing is. Much less interesting than I guess we thought, which is why we hadn't really noticed it wasn't working before. Because I'm also training fine. Anybody else have any questions before we wrap up. Thank you. See you. Good luck with you. Yeah. Thank you.
