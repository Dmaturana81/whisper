 Mr. May, I am recording now, so but please keep talking. Don't be shy, Sir, it's okay. Oh, getting a bit noisy out here with street plating something. I think your head said it's very good for 30 about. Oh, good. I didn't hear anything. So, I mean, I say street plating, it's more like footpath plating. We have a walking path along the front of their house. Sure. Oh, come on. I start the pressing the recording, but not everybody stops talking. Well, you know, I don't want to just hear my voice all the time on these recordings, guys. There are things I wanted to cover in today's session, but then the responsible part of me says I probably ought to create a lesson before Tuesday's class. So maybe we'll do that. I've got a question, Jeremy. I had to leave before you finished that code change yesterday. Did you? Was that actually, did you want to recap on where we got to with waiting with data? Probably not because you can just watch the video. And so, like otherwise, I guess we're just doing it. So is it working now? Yeah, yeah, it's all good. You know, I mean, it's the concept is working correctly in terms of the code. We didn't like get a better score, but I didn't particularly expect to either. You know, maybe after next Tuesday's lesson, we will revisit it because I actually think the main thing it might be useful for is what's called curriculum learning, which is basically focusing on the hard bits. Looks like Nick's internet still isn't working, but Nick was saying the other day that he looked at which ones we're having the errors on, which is like what we've, which what we look at in the book, like looking at the classification interpretation and looking at like plot, plot, blosses and stuff. And he said like, yeah, all the ones that we're making that we're getting wrong are basically from the same one or two classes. So I haven't done much with curriculum learning in practice. Like I like all it means in theory is that we use our weighted data loader to wait the ones that we're getting wrong higher. And whether that will actually give us a better result or not, I'm not sure. But that I think that's more likely to be a useful path than simply reweighting things to be more balanced. Because we don't want things to be more balanced because the ones that we observe the most often in the test set are actually the ones we want to be the best at. I will say I didn't check whether the distribution of the test set is the same as the training set. If it's randomly selected, then it will be. And if it's not, then that would be a reason to use a weighted data loader as well. So, yeah. Okay. So, what's the difference? I guess like what's the, is it is a curriculum learning kind of related to boosting in conceptually? Not really. I mean, maybe. So boosting is where you calculate the difference between the actuals and your predictions to get residuals. And then you create a model that tries to predict the residuals. And then you can add those two predictions together. Which is, if not done carefully as a recipe for overfitting. But if done carefully, can be very effective. Yeah. We're talking about something which is conceptually very different, which is saying like, oh, we're like really bad at recognizing this category. So let's show that category more often during training. That's a good question. I guess that really kind of focusing on examples, you're getting it wrong, like more and then conceptually doing something similar. I was just going to ask, are the labels ever wrong? Like by accident or intentionally in Kaggle? Of course, absolutely. So, so are both intentionally as well? No, not intentionally. I mean, I mean, not normally. Like sometimes there might be a competition where they say like, Oh, this is a synthetically generated data set and some of the data is wrong because we're trying to do something like what happens in practice, but we can't share the real data. So is there any other advantage in trying something like some uncertainty values from something like MC dropout, try to find like a threshold of things that are too difficult and then potentially they're wrongly labeled. I'm not sure you would need that. Like the thing we use in the book and the course is simply to find the things that we are confident of. But we know we're about that turned out to be wrong. And then just look at the pictures. So the more soft things value is enough, you think to basically know whether or not I do. Yeah, I mean, that seems to work pretty well. I mean, the only thing is you would need to be able to recognize these things in photos. But I'm sure if you spent an hour reading on the internet about what these different diseases are and how they look, you would be able to pick it up fast, soon enough. And then, you know, just like we did in chapter two, if you're recognizing the things that aren't black and brown, daddy bears. Okay, but so plausibly, even just knocking out some of the extremely difficult examples might get you higher on the leaderboard by virtue of them misleading the model. Not by knocking out the hard ones, but by knocking out the wrong ones. Yes. Unless the test set is mislabeled consistently with the training set, in which case you would not want to knock them out because you would want to be able to correctly predict the things which people are incorrectly recognizing as the one of the things. Something to try though. Yeah. Yeah. So I would do exactly what we did in chapter two, you know, you can use exactly the same widget. But as I say, you'd have to probably spend an hour learning about rice disease, which probably be a reasonably interesting thing to do anyway. I just a main discussion in the peti. Some people identified there some mislabeling at least over 20 already. Oh, okay. Yeah, so definitely happens. It says we have manually annotated every image with the help of agricultural experts, but there could be errors. Wow, this person knows more about rice than I do. I think the images in the tungro have chance of issues. This symptoms can easily be confused with potassium deficiency. Fair enough. Is that an example of what you're talking about? Where if a layman or sorry for semi expert gets confused, then the labeling in the test sets probably the same? Yeah, exactly. So you're probably fixing these would probably screw up your model because assuming that the test set was labeled used by the same people in the same way. I mean, sometimes test sets, the test set is more of a gold standard. They'll make more effort to talk to like a larger number of high quality experts and have them vote or something. Honestly, this competition seems like it doesn't even have any prize money attached. So I'd like, I think it's really low, low investment probably. That's why doubt they did that. But that can happen. Yeah, that the test set could have. I mean, it makes sense to invest in getting really good labels for the test set actually. I was looking at one of the other competitions on uni FESP, the x rays. I think there there was one somebody who had identified that a wrist was wrongly labeled as a carot bl teams. Yeah, sorry. I don't know. I was just looking around and it looked interesting. So I'm I'm number 15 at the moment. But it is a slightly weird one because well, it's it's interesting because some of the x rays have multiple labels, but the labels are just concatenated. So there's interesting discussion on how you'd analyze that. Would you treat a combination as a distinct classification, whether it was like a neck and a chest or something? Or do you look at each of them individually and then try and label a multiple one from the different things? So, so. Okay, so I'm just having a look at this competition. So when does it close? This is a month to go. But I don't know exactly when that is. Not really. The light is the first. Okay, where do you see that? When you go to the bottom of like on the overview and it says there's a whole timeline. So then you just hover over the oh my god, I see it says close in a month, but you actually have to get a tall tip by hovering. Okay, thanks to NISK. That's change UX. Okay, so it actually got more than a month. So maybe next week we could have a look at this one because it would be a good opportunity to play around with medical image stuff because they're using DICOM, I think. Yeah, somebody is also which I used, supplied a library of PNGs, which made it easier to use, but I don't know what you'd lose in using that rather than the DICOM images. Well, it rather depends. So DICOM is a very generic file format that can contain lots of different things. One of the things DICOM contain is higher bit depth images than a PNG allows. So if they've, yes, they might have gotten rid of that. But last AI has a nice medical imaging, it's pretty small, but like has some useful stuff, medical imaging library, which I think is fastai.vision.medical, which can handle DICOM directly. And I say there's a fastai entry as well. That'd be fun. We should try this next week. I see and there's the PNGs. I think the DICOMs come to about 27 gigabytes. Oh my god. Okay. So the PNG was quite attractive from that point. So one thing that you can do with DICOM is to compress them, particularly using JPEG 2000, which is a really good compression. But yeah, people often don't, for some reason. So probably the first thing I'd look at in that competition is to see, look at our DICOM and see is it storing 16 bit data or not. And if it is, I would try to find a way to re save that without losing the extra information, which I think we've got examples of in our medical imaging tutorial. All right. I'm going to share my screen. Even though I don't know what I'm doing. Yeah. I'm going to have to drop in a few minutes, but I'll catch the rest on the recorder. Thanks for this. Nice to see you. By the way, I was looking at this conf next paper. And gosh, everybody congratulates transformers on everything. Vision transformers bring new ideas like the Adam W optimizer. I guess I actually wrote the first thing saying we should always use the other W optimizer. That would be silver. In fast AI, I think that was years before vision transformers. Adam W here we go. Mid 2018. I read that paper last night and I was just thinking like, they kind of talk about how all of these things were already there, right? That they just rediscovered them like slightly larger kernel size and things like that. Which begs the question, why is it no one just done like experiments to just tweak these things together? I mean, it's been pretty. I mean, but nobody takes any notice because they're not written in PDFs. Is it? I mean, these benchmarks, though, like, the thing is that like a lot of researchers aren't good practitioners. So they just they're not very good at training accurate neural networks and they don't know these tricks, you know, and they don't hang out on cackle and learn about what actually works. But then the thing is like, it's not always easy to publish. Like, even if you did stick it into a PDF and submit it to NeurIPS, there's no particular high likelihood that they're going to accept it because the field research wise is very focused on theory results and, you know, things with lots of Greek letters in them. Does that mean that the part of the problem is that the data sets, the benchmarks are just too inaccessible to the average person? So, No, I wouldn't say that for ImageNet 1K. No, I wouldn't say that. The issue is I think the culture of research is not particularly interested in experimental results, you know. Okay, my limited experience, I will say it's very hard to find the viewer as well, especially you have a very strong domain, not just one thing or the sample data set you can find in open source. When you cause domain and then a lot of peer reviewers, they're just not taking up to the viewer. Even if we pay for the viewer, we're using, so people can get it for free and we take a few months just to find the viewer. Jeremy, is that on the topic of papers, one do you know when a paper is worth reading in, given the situation? I mean, it don't like until, I mean, I'm very fond of like papers that describe things which did very well in an actual competition. You know, that then we know this is something that actually predicts things accurately. You know, you can get similar results if they've got a good you know, just table of results. So generally speaking, I like things that actually have good results, particularly if they show like how long it took to train and how much data they trained on. And yeah, so are they getting good results using less data and less time than you might expect from the same thing. And yeah, I certainly wouldn't focus only on those that get good results on really big data sets. That's not necessarily more interesting. I'm very interested in things that show good results using transfer learning. So I look for things that are like, practically useful. I don't train that much from random. So I'm very interested in things that do well on transfer learning. Also, like look for people who you've liked their work before, you know, and in particular, that doesn't mean like always reading the latest papers. You know, if you come across a paper from somebody that you find useful, go back and look at their Google scholar and look at read their older papers. See who they collaborate with and read their papers. So for example, I really like Quok Lee in Google Brain. His, him and his team do a lot of good work. It tends to be, you know, very practical and high quality results. And so we know when his team releases a paper, and I also know like he seems to have similar interests to mine, like he tends to do stuff involving transfer learning and getting good results in less epochs and stuff like that. So if I say he's got a new paper out, I'm pretty likely to read it. I have a question. And I mean, for the, the, the, the, character competitions and like, like in a lab type of environment is, I mean, I mean, when to, the question that I have is when to stop iterating on a model, on a model that you have is, is, I have the, someone asked me when is enough enough to do the training on the data that you have, when is enough. So that question. I mean, I mean, there's some reason you're doing this work, right? So like you hopefully know when it does what you want it to do. I mean, the thing that happens, all that, that happens is especially to me all the time is that he trained the model and it works perfectly fine on the lab. We're doing it. And then as soon as we throw a couple of images that they are not part of the set, I mean, that goes nuts. And okay, because it's like live or more light or the temperature is different or stuff like that. So that's a different problem, right? So that, that, that means your problem is that you're, you're not using the, you know, the right data to train on. So like you need to, you need to be thinking about how you're going to deploy this thing when you train it. And if you train it with data that's different to, you know, how you're going to deploy it, it's not going to work. Yeah, so that's, that's what that means. And it might be difficult to get data, enough data of the kind you're going to deploy it on. But like, at some point you're going to be deploying this thing, which means by definition, you've got some way of getting that data you're going to deploy it with. So like do the exact thing you're going to use to deploy it, but don't deploy it, just capture that data until you've got some, some actual data from the actual environment you want to deploy the model in. You can also take advantage of semi supervised learning techniques to then, you know, transfer learning to maximize the amount of juice you get from that data that you've collected. And finally, I'd say, like let's say for medical imaging, like, okay, you want to deploy a model to like a new hospital, they've got a different brand of MRI machine you haven't seen before. I would take advantage of fine tuning, you know, each time I deployed it to some different environment where things a bit different, I would expect to have to go through a fine tuning process to train it to recognize that particular MRI machines images. But you know, each time you do that fine tuning, it shouldn't take very much data or very much time, because it's your models already learnt the key features, and you're just asking it to learn to recognize slightly different ways of seeing those features. Yeah, I don't think you'll solve this by trading for longer, you know, you'll solve it by figuring out your data pipeline, your data labeling, and your rollout strategy. Usually the issues that we're having is that we don't have enough data of a certain category, but I mean, the thing that you did yesterday, it resolves a little bit of that problem, I think we're going to start using this. Yeah, well, also, like, if you don't have enough data of some category, don't use the model for that category, you know, so like, you know, rather than using softbacks, use binary sigmoid, you know, as your last layer, and so then you've kind of got like a probability that X appears in this image, and so then you can you can recognize when none of the things that you can predict will appear in the image, and so then have a, you know, you always want to human in the loop anyway, so when you didn't find any of the categories of things that you've got enough data to be able to find, then triage those to human review. One thing that we did is, I mean, we have like 50 something categories. Oh, just one moment, hang on. Sorry about that. We had like 50 categories, and some of them are like, they have a lot, like 10 of them have a lot of items. So we end up doing like in a three step kind of process, like the ones with a lot, the ones with medium number, and then with a smaller number, and it looks like it resolved the problem a little bit. But this was to classify metadata coming from other systems, classify for legal purposes, for legal retention. I see. I had a question actually, actually, you tried the beta data lot also, right? So I think you have it submitted that to Kaggle not book. So did you do any validation locally first, before submitting to Kaggle, something like that? So you have a set of, right? No, I mean, you saw what I did, right? And when I did it, so I just, yeah, I just, I was intentionally using a very mechanistic approach. Because it was kind of like just, yeah, showing like his, the basic steps of pretty much any computer vision model, which is entirely mechanical and doesn't require any domain expertise. So yeah, my question more was, like, shouldn't we always treat the publicly readable, like a good or like should we take a hold out local data sets first to validate? Yeah, so I mean, I always have a validation set, yeah, yeah, which we saw in this, in this, I just used a random splitter because as far as I know, test set in the Kaggle competition is a random split validation set. Yeah, so like, whether it be for Kaggle or anything, I'd think creating a validation set that as closely as possible represents the data you expect to get in deployment or in your test set is really important. And yeah, I actually didn't spend the time doing that on this Paddy competition. Normally on Kaggle, if somebody does and notices there's a difference between the private leader board and the public leader board, like the test set and the trading set, normally it'll appear, you know, in discussions or on a Kaggle kernel or something, which is partly why I didn't look into it. But yeah, I mean, you should probably check, doesn't have the same distribution of disease types, you know, from the predictions that you create. Do the images look similar? Do they have the same sizes? And for me, if I said, as I see any difference between the test set and the trading set, that puts my alarm bells on, right? Because now I know that it's not randomly selected. And if you know it's not randomly selected, then you immediately have to think, okay, they're trying to trick us. So I would then look everything I could for differences. Because it takes effort to not randomly select a trade a test set. So then they must be doing it very intentionally for some reason. Quite often for wrong reasons, but I think so. Like I don't think a Kaggle competition should ever silently give you a systematically different test set. I think there's great reasons to create a systematically different test set, but there's never a reason not to tell people. So if it's like medical imaging as a different hospital, you should say this is a different hospital or if it's fishing, you should say these are different boats or, you know, because like you want people to do well on your data. So if you tell them, then they can use that information to give you better models. So, Kuryan, like going back to what you asked about, there's this validation in training, then there's this whether your local validation maps to what's happening on the leaderboard, the score on the hidden test set. But there's one other scenario that I encountered recently, and maybe it will be interesting to someone. When you're working on a competition, sometimes you might miss something in your code or the prediction. You know, your model is doing something useful, but you're failing to output a correctly formatted submission file. And not in a sense that the submission phase on Kaggle, but some predictions are not aligned where there should be or, you know, therefore a different customer ID or stuff like that. So once you have one good submission file, relatively good, you can just sort it locally and then see, you know, run a check the correlation between your new submission and the one that you know that this is okay. And you know, the correlation should be upwards of 0.9. And then, you know, yeah, okay, so I didn't mess up anything with the technical aspect of the prediction. I mean, it's not a great straight, but, you know, I was like, you put my hair out, why is it not working? It's a better model. So this was like a sanity check step, maybe at some point. Thanks. Cool. Thanks. Alright, so let me share my screen. Let's find zoom. Zoom. Share screen. Yeah. Oh, that's not the right button. Control, shift, H. Okay. Where did we get to in the last lesson? We finished random forests, right? And, oh, that's right. I haven't posted that video yet. That's okay. We can check the live. First year, live. Okay, so we could get some more models. And do we get to the end of this? Okay. So that basically finished the second one of our Kaggle things. So next week, see what's in part three. Right. Gradient accumulation. I think that's worth covering. So one thing that somebody pointed out on Kaggle is I'm using gradient accumulation wrong. I was passing in two here to make create two batches before you accumulate. But actually what I'm meant to be putting in here is the target batch size I want. So that would be actually, I should be putting 64 here. So I feel a bit stupid. So what I've been doing is I've been actually not using gradient accumulation at all. I guess it's been doing a batch and saying that's over, saying my maximum batch size would be two. Okay. So this has actually been not working at all. That's interesting. Whoops. So I've been using a batch size of 32 and not accumulating. Okay. So that's one thing to note. So when I get Kaggle GPU time again, we'll need to rerun this. Actually, it only took 4,000 seconds. So I guess we should, we could just get it running right now, couldn't we? So So that should be 64. How many of past defines how large the effective batch size you want is over batches. Oh, we can just remove this sentence entirely. Oh, no, that's right. Okay. We divide the batch size by some number based on how small we need it to be for our GPUs. Okay. So and on Kaggle, I think these were all smaller. I don't know why, but the Kaggle GPUs use less memory than my GPU for some reason. Okay. So we're now, let's try running it. So Jeremy, if you would increase that I've come number until the longer get could I have a memory? Yeah. And you could be able to pretty much guess it's by looking at like, I mean, you can just, once you found a batch size, it fits, you know, so the default batch size, I believe, is 32. So once you find a batch size that fits, sorry, 64 is a default batch size, it fits, you just like, it's like, okay, what fits in 32, then I just need to set it to two because 64 divided by two is enough. And the key thing I do here is, you know, so I've got this report GPU function. So what I did at home was I just, you know, changed this until it got less than 16 gig. And as you can see, I'm just doing like a single a park on small images. So this ran in, I don't know, 15 seconds or something. Yeah, batch size 64 by default. Yeah, so then I just went through checking the memory use of Conf next large or different image sizes again just keeping on using just one epoch. And that's how I figured out what I could do to just set a queue to get to work. All right, so that should be right to save and run. And then, turn off this one. So when you're running something like he clicks a version and you click run, you'll then see it down here. And that runs in the background, you don't have to leave this open. And so you can go back to it later. So if I just copy that can close it. And if I go to my notebook in Kaggle. And it shows me version three of four because version four hasn't finished running yet. So if I click here, I can go to version four and it says, oh, that's still running. And I can see here you go it's been running for about a minute. And it shows me anything that you print out will appear. Including warnings. Yeah, that's what happens in Kaggle. So if we also do the multi objective loss function thing. That would be cool. So I thought like next time in our next lesson, broadly speaking. Actually, this is taking a long time. I kind of want to cover like what the inputs to a model look like and what the outputs to a model look like. So like in terms of inputs, really the key thing is embeddings. That's the key thing we haven't seen yet in terms of what model inputs look like. So model outputs, I think we need to look at soft max. Outputs, soft max cross entropy loss. Entropy loss. And then you know our multi target loss. Which we could do first kind of a segue. So maybe in terms of the ordering, the segue would be like doing multi target loss first. And then we could talk about soft max and cross entropy, which would then lead us potentially to like looking at the bear classifier. What if there's no bears. So we can just use the binary sigmoid. So then for embeddings, I guess that's where we'd cover the collaborative filtering, collaborative filtering because that's like a really nice version of embeddings. So I guess the question is for those who have done the course before. Are there any other topics, I guess like time and bidding, it would be nice to look at like the conf net what a conf net is just kind of so that's like that we've got like the outputs, the inputs, and then the middle. What about more and old piece of, you know, people like what. Well, I've heard that. I can face is getting integrated was past day I may be looking at that works. Well, it's not done yet so we can't do that yet but definitely in part two. I got a question I don't know if it's helpful but there's a lot of emphasis on outputs and inputs. But like in the middle just understanding like the outputs of a layer, whether they're going to write on how do you debug that how do you understand, you know, when to kind of look at the yeah very helpful. Last time we did a part two we did a very deep dive into that and I think we should do that again and apart to because like most people won't have to debug that because if you're using an off the shelf. Model, you know, like it's, you know, with off the shelf initializations that shouldn't happen. So it's probably more of an advanced debugging technique, I would say. But yeah, if you're interested in looking at it now definitely check out our previous part two, because we did a very deep dive into that and developed the, the so forth colorful dimension plot, which is absolutely. Great for that. Yeah. Yeah. So that would exactly so collaborative filtering would lead us exactly into that. Thank you. Yeah sorry, Sarah. Do you like to spend finally talking about the importance of the ethical side, at least you point to the resources. I think it's a very crucial prepare before so I think people, because it's so easy to build a model but how to apply is getting more scary now. Yes. Yes, I mentioned in lesson one, the data ethics course but you're right it would be nice to kind of like. Isn't it. Yeah, I mean that I mean that actually would be a great thing just to talk about, you know, that that lecture is not at all out of date. So, So maybe touch on it in this one and also talk link to, you know, for varying levels of interest, the two hour version would be Rachel's talk in the 2020 lecture, and then deeper interest deal would be the yes, the four ethics course it's a great point. Thank you. So then, for, for actually pretty much all of these things. We have Excel spreadsheets, which is fun. So there's, let's have a look, collaborative filtering. Oh, looks like I've already downloaded that. I will encourage you to continue teaching in Excel yesterday I on the panel in a data science conference and when I mentioned I start with Excel actually inspired a lot of people, they want to help go with data science and learning it. Oh, that's good feedback. Because there's a certainly some people who don't find it useful at all, and they tend to be quite loud about that so certainly nice to hear that that feedback. So I thought you didn't let those people get to you. Oh, I only pretend that anybody doesn't get to me. So I don't necessarily say that's that was really great to see running the city done once before. Great. Okay. Thank you. I will. So I think these are actually from the 2019 course. First day, I won courses, do one. So I'm just going to grab them all. So one thing I don't think we're going to cover this year, this part one that we will cover in part two is like different optimizers, like momentum and Adam and stuff. But I think that's okay because I feel like nowadays just use the default Adam W and it works. So I don't, I think it's fine. Not to know too much more than that. It's a little bit of a technicality nowadays. Yeah. It used to be something we did in one of the first lessons, you know, but that was when you kind of had to know it right because you was fiddled around with momentum and blah, blah, blah. So we always like the biggest thing when starting on something is to how to, you know, once I figure out how to read in the data, then things. But I'm really grateful that there's such an emphasis in this edition of the course on the reading, you know, theta and, you know, with similar data, that is something that we also study the lookout for just understanding better reading the data. Great. I don't think we did this one anymore, because we kind of have better versions in in Jupiter with IPay widgets so we've got this fun convolutions example, which I think is still valuable. Softmax, and cross entropy examples. And we've got collaborative filtering. And something interesting. What are what that is. And then, also we've got word embeddings. All right. And then, also we've got word embeddings. All right. And there are such a cool and important subject. And the something that we haven't discussed that much in this course. No, we haven't touched them at all. All right. It feels like a lot to cover. Hmm. That we will. We will do our best. Okay. I think we're up to our hour. So thanks everybody. Nice chat today. And I will get to work on putting this together. Have a nice weekend. Thank you so much. We might everyone. There's a waste and bias. Video today. I think six or four. This one time. So with anyone interest. The guy mentioned he told us to mention you're going to have another US session as well. We can join. Yes, I think there's details on the forum. Thanks. See you.
