WEBVTT

00:00.000 --> 00:01.920
 I've got a question. Yeah.

00:03.920 --> 00:09.440
 To do with, is there a way the machine learning can actually find the

00:10.560 --> 00:18.320
 sort of conditional probabilistic segments that are, say, in sort of heterogeneous data?

00:20.800 --> 00:24.880
 I am having trouble parsing that question. Can you give like an example or something?

00:24.880 --> 00:33.360
 Yeah. Okay. All right. Well, I'm modeling with road surface friction with road risk, rather.

00:33.360 --> 00:41.360
 And, and fundamentally, there's, you know, there's this set of stereotypes in road analysis.

00:41.360 --> 00:48.320
 And, you know, we all know them as highways, three ways, urban materials. And they actually

00:48.320 --> 00:56.160
 go through a series of stages, sort of, they're almost like states. And each of the states is

00:56.160 --> 01:04.640
 going to sort of conditional probabilistic relationship between a set of predictors and

01:04.640 --> 01:09.600
 the actual response variable, we don't know the crash response variable. Is there anything that

01:09.600 --> 01:19.200
 you want that in, in deep learning? So how, how is that different to a normal predictive model?

01:19.200 --> 01:26.800
 Like, I mean, all predictive models are conditional probabilities, right? What's there? Yeah,

01:26.800 --> 01:32.960
 that's here. Well, I mean, take something like X, G, B, for example, and you want to predict

01:32.960 --> 01:40.800
 a risk of a given road. So it'll give you value. The thing you got no idea as to what's happened

01:40.800 --> 01:50.080
 inside of the model. And, you know, I'm really, we're really interested in that because once you

01:50.080 --> 01:56.320
 find the distributions, you can then start to do some quality testing on, on when they actually

01:56.320 --> 02:03.040
 follow the domain or, you know, whether you segmentation process that actually determines

02:03.040 --> 02:12.320
 predictions is good or not. And so, you know, why, rather than say predicting some sort of

02:14.960 --> 02:24.320
 crash rate or risk or whatever, I'm really looking for those probabilistic distributions and

02:24.320 --> 02:33.840
 learning within the service. So all deep learning models will return a set of probabilities. That's

02:33.840 --> 02:39.120
 what they're, that's what their final layer returns. And then we decode them by taking the

02:39.120 --> 02:44.880
 argmax across them. But there's nothing to stop you using those probabilities directly.

02:46.800 --> 02:53.120
 But I'm probably misunderstanding your, your question. It's, it's a little abstract for me to

02:53.120 --> 03:01.280
 understand. Like, I mean, I know there's, there's lots of things you can do with, you know,

03:01.920 --> 03:08.560
 confidence intervals and whatnot. But really depends a great deal on the specific details of

03:08.560 --> 03:12.320
 the application, what you're trying to do and how you're trying to do it.

03:13.760 --> 03:20.560
 Good question, Daniel. I'm just, are you, are you talking about probability of an incident or risk

03:20.560 --> 03:27.920
 related, related to the right surface? So you're going to need some sort of tabular data that has

03:27.920 --> 03:33.520
 the, the, the occurrences with each right surface that you're trying to.

03:34.320 --> 03:43.760
 And why wouldn't XG based give you that if you had a particular model of, of, of incidents?

03:43.760 --> 03:52.320
 Well, in my mind, at least one of the disadvantages of XG based is the fact that,

03:52.320 --> 03:59.760
 you know, the issues, seeing a set of variable effects. You know, whereas in, in what we're dealing

03:59.760 --> 04:07.840
 with, we've got some really high, high crash roads, sort of, sort of a, a different

04:07.840 --> 04:14.800
 conditional probability relationship between the, the predictors and the response compared to

04:14.800 --> 04:22.160
 say the average. And so XG based does an excellent job in making predictions, but you've got no idea

04:22.720 --> 04:31.840
 as to, you know, the group of instances that they're actually making a prediction or the actual

04:31.840 --> 04:38.960
 variable effects. Okay, so I think I understand your question now. And I think the answer is

04:38.960 --> 04:45.440
 actually it does. And what I suggest you do, if you haven't already is read the chapter of the

04:45.440 --> 04:52.880
 first AI book on tabular modeling. And it will cover something very similar, which is random

04:52.880 --> 04:57.360
 forests, which is another ensemble of decision trees. And it will show you how to get

04:57.360 --> 05:06.320
 exactly the kind of insights that I think you're looking for. And all of the techniques there

05:06.320 --> 05:10.320
 would work equally well for random forests. And they also work equally well for deep learning.

05:10.320 --> 05:14.080
 So maybe after you've done that, you can come back and let us know whether you helped.

05:14.080 --> 05:17.600
 Yeah, well, I'm so glad to train them first, maybe,

05:18.160 --> 05:23.440
 maybe it doesn't really give you a random form.

05:23.440 --> 05:29.760
 I strongly suggest you read the chapter before you say that, because I'm pretty sure it will.

05:30.400 --> 05:32.960
 And if it doesn't, that would be very interesting to me. But I've,

05:34.400 --> 05:38.800
 yeah, in fact, I'll make you see last time that I'm really looking for the

05:39.760 --> 05:49.600
 technical data. Cool. Great. I'll show you what you, I'll show you guys what I've been working on,

05:49.600 --> 06:07.920
 which has been fun. So, the first thing I did, you know, after I got off our last call was I

06:07.920 --> 06:24.480
 basically just threw together the kind of like most obvious basic steps one would do for a standard

06:24.480 --> 06:30.800
 image recognition competition, just in order to show people that that can be quite good.

06:31.920 --> 06:36.800
 And it was actually a little embarrassing, because I didn't mean to do this. When I submitted it,

06:36.800 --> 06:43.600
 it turned out I got first on the lead board. So now I feel like I'm going to have to write down

06:43.600 --> 06:52.960
 exactly what I did, because, you know, during an active competition, everybody needs to share

06:52.960 --> 06:58.480
 what they're doing, if they share it with anybody, some eat publicly. So I thought I'd show you

06:58.480 --> 07:08.400
 what I did here, but I think this is about to go up quite a lot, because, you know, what we're doing

07:08.400 --> 07:19.280
 here is where they're interesting images for a couple of reasons. One is that they're kind of

07:19.280 --> 07:25.440
 like things that you see in ImageNet, like their pictures of natural objects, their photos.

07:25.440 --> 07:33.120
 But I don't think ImageNet has any kind of like categories about diseases, you know, they have

07:33.120 --> 07:37.680
 categories about like, what's the main object in this? So they might have a category about like,

07:37.680 --> 07:42.800
 I don't know if they do, like some different kinds of grass, or some different types of

07:42.800 --> 07:49.600
 evens and different types of, you know, fields or something. But I'm pretty sure they don't have

07:49.600 --> 07:54.800
 anything about different kinds of crop disease. So it's a bit different to ImageNet,

07:54.800 --> 08:02.000
 which is what most of our pre trained models are trained on. But it's not that different.

08:03.040 --> 08:08.400
 And it's also interesting because nearly all of the images are the same shape and size,

08:09.280 --> 08:11.360
 so we can kind of try to take advantage of that.

08:15.600 --> 08:19.840
 And, you know, so when we fine tune a pre trained model,

08:19.840 --> 08:26.400
 there's, let me pull up this Cacron notebook I just created.

08:34.560 --> 08:37.200
 So I just published this yesterday.

08:42.400 --> 08:46.560
 Kind of look at the, what are the best vision models of fine tuning? And so I kind of realized

08:46.560 --> 08:51.680
 that there are two key dimensions that really seem to impact how well a model can be fine

08:51.680 --> 08:56.960
 tuned, you know, whether it works well or not or how it's different. So what is what I just

08:56.960 --> 09:04.320
 talked about, which is how similar is your data set to the data set used for the pre trained model?

09:05.920 --> 09:15.040
 If it's really similar, like pets to ImageNet, then like the critical factor is how well does

09:15.040 --> 09:20.800
 the fine tuning of the model maintain the weights that are pre trained, you know, because you're

09:20.800 --> 09:24.240
 probably not going to be changing very, very much. And you're probably going to be able to take

09:24.240 --> 09:29.600
 advantage of really big, accurate models because they've already learned to do almost the exact

09:29.600 --> 09:35.920
 thing you're trying to do. On the other hand, so that's the pet's data set. On the other hand,

09:35.920 --> 09:46.080
 there's a data set called the planet data set, which is images of satellite images.

09:46.880 --> 09:52.880
 And these are not really at all like anything that ImageNet ever saw, you know, they're taken

09:52.880 --> 09:58.320
 from above, they're taken from much further away. There's no single main object.

09:58.320 --> 10:08.320
 So a lot of the weights of a pre trained model are going to be useless for fine tuning this because

10:08.320 --> 10:14.880
 they've learned specific features like, you know, what does text look like? What do eyeballs look like?

10:14.880 --> 10:19.440
 What does fur look like? You know, which none of which are going to be very useful.

10:20.720 --> 10:24.560
 So that's the first dimension. The second dimension is just how big the data set is.

10:24.560 --> 10:29.600
 So on a big data set, you've got time, you've got epochs to

10:34.560 --> 10:40.400
 take advantage of having lots of parameters in the model to learn to use them effectively.

10:40.400 --> 10:44.480
 And if you don't have much data, then you don't have much ability to do that.

10:47.360 --> 10:54.240
 So you might imagine that deep learning practitioners already know these answers of how do we,

10:54.240 --> 10:59.280
 you know, what's the best models for fine tuning? But in fact, we don't. As far as I know, nobody's

10:59.280 --> 11:04.080
 ever done an analysis before of which models are the best for fine tuning. So that's what I did

11:04.080 --> 11:09.280
 over the weekend. And not just over the weekend, but really over the last couple of weeks.

11:10.480 --> 11:17.120
 And I did this with Thomas Capel, who works at Weights at Bias is another first AI community

11:17.120 --> 11:25.920
 member slash alumni. And so what we did was we tried fine tuning lots of models on two data sets,

11:25.920 --> 11:33.200
 one which has 10 times over 10 times less images. And where those images are not at all like ImageNet,

11:33.200 --> 11:40.160
 that being the Kaggle Planet sample. And one which is a lot like ImageNet and has a lot more images

11:40.160 --> 11:47.280
 that being IIT pets. And I kind of figured like if we get some insights from those two, perhaps there

11:47.280 --> 11:56.000
 will be something that we can leverage more generally. So Thomas wrote this script,

11:58.720 --> 12:07.920
 which it's 86 lines, but really there's only like three or four lines and there will be lines

12:07.920 --> 12:14.480
 you recognize, right? The lines are untied data, image data loaders dot from blah,

12:15.840 --> 12:24.880
 and then vision learner dls model, etc. So there's the normal like three or four lines of code we see

12:24.880 --> 12:32.480
 over and over again. And then, you know, the rest of it is basically lets you pass into the script,

12:32.480 --> 12:42.400
 different choices about batch size epochs and so forth. And that's about it. So this is like how

12:42.400 --> 12:52.400
 simple the script was that we used. And then partly because Thomas works at weights and biases,

12:52.400 --> 12:59.440
 and partly because weights and biases is pretty cool. We used weights and biases then to feed in

12:59.440 --> 13:06.160
 different values for each of those parameters. So this is a yaml file that weights and biases

13:06.160 --> 13:12.960
 users where you can say, okay, try each of these different learning rates, try each of these different

13:12.960 --> 13:23.600
 models, try each of these different resize methods, each of these different polling methods,

13:23.600 --> 13:30.720
 this distribution of learning rates, you know, whatever, and it goes away and tries them. And then

13:32.480 --> 13:37.600
 you can use their web GUI to look at like the training results. So then you basically say,

13:37.600 --> 13:42.320
 okay, start training and it trains each of these models over each of these data sets with each of

13:42.320 --> 13:46.400
 these pool values and each of these resize methods and a few different selections from this distribution

13:46.400 --> 13:53.600
 of learning rates. And creates a web GUI that you can dive into. I personally hate web

13:53.600 --> 14:00.640
 GUI's, I would much rather use Python. So but they also thankfully have an API. So yeah, so once we

14:00.640 --> 14:10.160
 ran that script for a few hours, and then checked the results into a, into a gist.

14:10.160 --> 14:21.760
 So a gist is just a place to check text files basically if you haven't used it before.

14:29.600 --> 14:35.520
 So I can, I checked my CSV file in here. As you can see, it kind of displays it in a nice way,

14:35.520 --> 14:42.240
 or you can just click on raw to see the raw data. So I find that quite a nice place just to chuck

14:42.240 --> 14:49.120
 things, which I'm just going to share publicly. And so then I can take this, the URL to the gist.

14:49.120 --> 15:05.040
 And maybe, let me show you how I did that.

15:45.200 --> 15:48.400
 So I was kind of like everything to be automated.

15:48.400 --> 15:52.960
 So I can always easily redo it because I always assume my first effort's going to be crap. And

15:52.960 --> 16:00.480
 it always is. And normally my second, third efforts are crap as well. So here's my little

16:00.480 --> 16:08.320
 notebook I put together. So basically each time you do one of these sweeps on weights and biases

16:08.320 --> 16:12.960
 that generates a new ID. And so we ended up kind of doing five different ones as we realized we

16:12.960 --> 16:18.400
 wanted to add different models and change things a little bit. And so they have this API that you

16:18.400 --> 16:26.800
 can use. And so they basically can go through and say, go through each of the sweep IDs and

16:26.800 --> 16:32.320
 ask the API for that sweep and grab the runs from it. And then for each one, create a dictionary

16:32.320 --> 16:36.640
 containing a summary in the model name. So the details don't matter too much, but you kind of

16:36.640 --> 16:43.680
 get the idea hopefully and then turn that into a data frame. And so I kind of end up with this

16:44.400 --> 16:49.840
 with this data frame that contains all the different configuration parameters.

16:54.960 --> 17:02.240
 Along with their loss and their speed, accuracy, GPU, maximum memory usage and so forth.

17:02.240 --> 17:09.120
 So that's basically what I wanted to check into a, into a gist. And so specifically,

17:09.120 --> 17:13.520
 I really wanted to subset of the columns. So these are the columns I wanted. So I can grab those

17:13.520 --> 17:21.840
 columns and put them into a CSV. Now, one thing you might not realize is I would say for most

17:21.840 --> 17:27.280
 Python libraries or at least most well written ones, anyway, you can put a file names. And only

17:27.280 --> 17:32.800
 when you say to CSV, you put here a file or a path. You could instead put something called a string

17:32.800 --> 17:38.800
 IO object, which is something that behaves exactly like a file. But it actually just stores the,

17:39.920 --> 17:45.440
 it stores it into a string, because I don't want this sort into a file. It's what it into a string.

17:45.440 --> 17:52.240
 So if you then call dot get value, I actually get the string. And so even things like creating

17:52.240 --> 17:59.920
 the gist, I want to do that automatically. So there's a library I'm very fond of. I'm very

17:59.920 --> 18:06.400
 biased because I made it called GH API, which is an API for GitHub, where we can do things like say

18:06.400 --> 18:12.400
 create gist, and you give it a description. And here's the text, which is the contents of the CSV

18:13.760 --> 18:20.320
 and the file name, make it public. And then you can get the HTML URL of the gist. So that's how like I

18:20.320 --> 18:27.200
 use used in this case, a notebook as my kind of, you know, interactive,

18:28.160 --> 18:35.120
 REPL read of our print loop for manipulating this data set, putting it together, and then

18:35.120 --> 18:42.240
 uploading it to GitHub. Jeremy, I had a doubt in this pond of data frame. Here you have like,

18:42.240 --> 18:47.840
 in your, I just take good to clear up your game, this and it had in the data set entries with

18:47.840 --> 18:53.600
 plan that other data set, the pet status, etc. So how did you populate it?

18:54.880 --> 18:57.760
 So what's your question? How did I populate this data set?

18:58.480 --> 19:05.840
 Yeah, that pond of data. Yeah, just here. So I passed it a list of dictionaries.

19:06.960 --> 19:10.080
 And the list of dictionaries I created using a list comprehension,

19:10.080 --> 19:19.440
 and containing a bunch of dictionaries. Okay, content. And so that's going to make each key.

19:20.000 --> 19:23.600
 So that means all the dictionaries should have, you know, roughly the same keys.

19:23.600 --> 19:31.360
 Anyone sort of missing are going to end up being NA. And then I just fiddled around with it slightly.

19:31.360 --> 19:35.600
 So for example, except by everything had an error rate that was equal to one minus the accuracy.

19:35.600 --> 19:41.440
 On the planet data set, it's not called accuracy. So I copied accuracy, multi into accuracy.

19:41.440 --> 19:45.280
 Yeah, nothing very exciting. Thank you.

19:46.640 --> 19:53.200
 Jeremy, what's actual goal of this? Let me show you. So what we've now got

19:53.200 --> 20:04.240
 is a CSV, which I can then,

20:13.840 --> 20:21.920
 okay, a CSV, which I can then use pandas pivot table functionality to group by the data set,

20:21.920 --> 20:28.320
 the model family and name, and calculate the min of error rate fit time and GPU memory.

20:31.760 --> 20:36.640
 And I can then take the pets subset of that,

20:38.400 --> 20:45.040
 sort by score, where score represents a combination of error and speed, and take the top 15.

20:45.040 --> 20:53.840
 And this now shows me the top 15 best models for fine tuning on pets.

20:55.360 --> 21:00.800
 And this is gold, in my opinion. I don't think anybody's ever done anything like this before.

21:00.800 --> 21:04.960
 There's never been a list of like pure the best models for fine tuning.

21:05.680 --> 21:13.920
 And yeah, sorry, I have a question. So you, you, you fine tune different models with pets,

21:13.920 --> 21:18.000
 and then collected this information. Is that correct? That's correct.

21:18.000 --> 21:25.440
 And then based on the information that you collected from the fine tune of five or whatever number of

21:25.440 --> 21:33.440
 iterations. We did three runs for each model. Yes. And then you collected this information to

21:33.440 --> 21:39.920
 find out which one is the best behave model for this specific case in this case. Correct, correct,

21:39.920 --> 21:44.880
 correct, exactly. And best is going to involve two things. It's going to be which ones have the

21:44.880 --> 21:50.960
 lowest error rate and which ones are the fastest. Now, I created this kind of arbitrary scoring

21:50.960 --> 21:58.240
 function where I multiplied the error rate times fit time plus 80. Just because I felt like that

21:58.720 --> 22:02.720
 particular value of that constant gave me an ordering that I was reasonably comfortable with.

22:02.720 --> 22:10.720
 But you can kind of look through here and see like, okay, well, VIT base has a much better error

22:10.720 --> 22:16.880
 rate than ComfNext Tiny, but it's also much slower. Like you can decide for your needs

22:17.680 --> 22:23.440
 where you want to trade off. So that's what I kind of the first thing I did was to create this

22:23.440 --> 22:29.040
 kind of top 15. And it's interesting looking at the family, right? The family is like each of

22:29.040 --> 22:36.720
 these different architectures, you know, is kind of from different sizes of a smaller subset of

22:36.720 --> 22:43.360
 families, right? So there's ComfNext Tiny, ComfNext Base, ComfNext Tiny and 22K and so forth.

22:44.240 --> 22:48.080
 So you can kind of get a sense of like, if you want to learn more about architectures,

22:48.080 --> 22:52.480
 which ones seem most interesting. And you know, for fine tuning on pets, it looks like ComfNext,

22:52.480 --> 23:02.960
 VIT, SWIN, ResNet, the main ones. So that, you know, the first thing I did, the second thing I then did

23:02.960 --> 23:12.480
 was to take those most interesting families, actually also added this one called RegNetX

23:12.480 --> 23:23.040
 and created a scatter plot of them colored by family. And so you can kind of see like, for example,

23:23.040 --> 23:33.200
 ComfNext, which I'm rather fond of, is this kind of blue line, these blue ones, right? And so you

23:33.200 --> 23:44.320
 can see that the very best error rate actually was a ComfNext. So they're pretty good. You can see

23:44.320 --> 23:56.800
 this one here, which is RegNetX seems to be, had some pretty nice values. Feel like super fast,

23:57.680 --> 24:02.560
 seems like these tiny swings seem to be pretty good. So it kind of gives you a sense of like,

24:02.560 --> 24:05.680
 you know, depending on how much time you've got to run or how accurate you want to be,

24:06.560 --> 24:09.520
 what families are like most most useful.

24:13.200 --> 24:19.760
 And then the last thing I did for pets was I grabbed a subset of the, basically, the ones which are

24:19.760 --> 24:25.920
 in the top, basically smaller than the median and faster than the median. Because these are the

24:25.920 --> 24:30.160
 ones I generally care about most of the time, because most of the time I'm going to be, you know,

24:30.160 --> 24:35.040
 training quick iterations. And so, and then I just ordered those by error rate.

24:36.320 --> 24:43.120
 And so ComfNext's tiny has got the best error rate of those which are in the upper half of both

24:43.120 --> 24:57.440
 speed and accuracy. That's the maximum amount of GPU memory that was used. I can't remember what

25:01.920 --> 25:05.120
 the units have measured are, but they don't matter too much because it'll be different for your

25:05.120 --> 25:15.680
 data set. All that matters is the relative usage. And so if you want something, you know, if you

25:15.680 --> 25:22.720
 try to use this and sexually use too much GPU memory, you could try ResNet 50D, for example,

25:24.000 --> 25:33.120
 or, you know, it's interesting that like ResNet 26 is really good for memory and speed.

25:33.120 --> 25:40.480
 Or if you want something really lightweight on memory, ResNet Y004, but the error rates are

25:40.480 --> 25:47.680
 getting much worse once you get out to here, as you can see. So then, so then I looked at Planet.

25:48.720 --> 25:54.640
 And so as I said, Planet's kind of as different a data set as you're going to get in one sense.

25:56.240 --> 26:02.480
 Or it's very different. And so not surprisingly, it's top 15 is also very different.

26:02.480 --> 26:11.360
 And interestingly, all of the top six are from the same family. So this VIT family, these are

26:11.360 --> 26:19.120
 kind of model called transformers models. And what this is basically showing is that these models

26:20.240 --> 26:26.800
 are particularly good at rapidly identifying features of data types it hasn't seen before.

26:27.440 --> 26:31.200
 So, you know, if you're doing something like medical imaging or satellite imagery or something

26:31.200 --> 26:37.360
 like that, these would probably be a good thing to try and swing, by the way, is kind of another

26:38.400 --> 26:44.320
 transformers based model, which as you can see, it's actually the most accurate of all,

26:44.960 --> 26:48.160
 but it's also the smallest. This is SWINV2.

26:52.400 --> 26:59.360
 So I thought that was pretty interesting. And, you know, these VIT models, there are ones with

26:59.360 --> 27:03.680
 pretty good error rates that also have very little memory use and also run very quickly.

27:06.800 --> 27:12.400
 So I did the same thing for Planet. And so perhaps not surprisingly, but interestingly,

27:12.400 --> 27:19.440
 for Planet, these lines don't necessarily go down, which is to say that the really big models,

27:19.440 --> 27:26.000
 the big slow models don't necessarily have better error rates. And that makes sense, right? Because

27:26.000 --> 27:30.080
 if they've got heaps of parameters, but they're trying to learn something they've never seen before

27:30.080 --> 27:35.600
 on very little data, it's unlikely we're going to be able to take advantage of those parameters.

27:38.160 --> 27:41.760
 So when you're doing stuff that doesn't really look much like ImageNet,

27:43.200 --> 27:51.120
 you might want to be down more towards this end. So here's the VIT, for example.

27:51.120 --> 27:59.840
 And here's that really good SWIN model. And there's ConfNext Tiny.

28:00.880 --> 28:05.280
 So then we could do the same thing again of like, okay, let's take the top half, both in terms of

28:05.280 --> 28:13.920
 speed and memory use. ConfNext's tiny still looks good. These VIT models, this 224,

28:13.920 --> 28:20.400
 yeah, this is because you can only run these models on images of size 224 by 224.

28:22.400 --> 28:26.400
 They're not, you can't use different sizes. Whereas the ConfNext models, you can use any size.

28:27.760 --> 28:34.640
 So it's also interesting to see the classic resnet still, again, they do pretty well.

28:34.640 --> 28:45.360
 Yeah, so I'm pretty excited about this. It feels like exactly what we need to

28:49.360 --> 28:58.000
 kick ass on this Patty Doctor competition, or indeed any kind of computer vision classification

28:58.000 --> 29:14.480
 task needs this. And I ran this SWIP on three consumer RTX GPUs in 12 hours or something.

29:14.480 --> 29:24.080
 Like this is not big institutional resources required. And one of the reasons why is because

29:24.080 --> 29:38.400
 I didn't try every possible level of everything. I tried a couple of, you know, so Thomas did a

29:38.400 --> 29:42.560
 kind of a quick learning rate SWIP to kind of get a sense of the broad range of learning rates

29:42.560 --> 29:46.880
 that seemed pretty good. And then we just tried a couple of learning rates and a couple of the

29:46.880 --> 29:52.880
 best resized methods and a couple of the best polling types across a few broadly different

29:52.880 --> 30:01.680
 kinds of models across the two different datasets to kind of see if there was any common features.

30:01.680 --> 30:05.680
 And we found in every single case, the same learning rate, the same resized method, and the

30:05.680 --> 30:10.640
 same polling type was the best. So we didn't need to try every possible combination of everything.

30:10.640 --> 30:16.160
 You know, and this is where like a lot of the stuff you see from like Google and stuff,

30:16.160 --> 30:21.120
 they tend to do hundreds of thousands of experiments because I guess because they don't,

30:21.120 --> 30:23.840
 they have no need to do things efficiently. Right.

30:28.160 --> 30:31.840
 Yeah, but you don't have to do it the Google way. You can do it the fast AI way.

30:37.440 --> 30:44.240
 Quick question Jeremy. Which cards did you use? And another question is why do you keep?

30:44.240 --> 30:47.360
 Which cards did you say? Yeah, the GPU cards.

30:47.360 --> 30:53.360
 Oh, 3090. Oh, okay. So they were all three different.

30:54.800 --> 31:03.040
 They're all 3090s. Oh, okay. And you reset the index after the query. Why?

31:03.040 --> 31:09.680
 Oh, just because otherwise it shows the numeric ID here will be the new

31:09.680 --> 31:12.800
 America ID from the original data set. And I wanted to be able to quickly kind of say,

31:12.800 --> 31:16.720
 what's number six? What's number 10? That's all. That's all. This is crucial.

31:19.680 --> 31:22.720
 Okay. Jeremy, getting back to the earth,

31:23.760 --> 31:28.560
 said a lot of images. When you say, you know, like the class of potassium, what is it trying to

31:28.560 --> 31:45.760
 classify? In this case, the planet competition.

31:49.520 --> 31:57.760
 There's some examples. Basically, they try to classify for each area of the satellite

31:57.760 --> 32:06.320
 imagery. What's it a picture of? Forest or farmland or town or whatever.

32:08.320 --> 32:10.960
 What weather conditions to observe, if I remember correctly.

32:16.000 --> 32:23.280
 Question in this image space. Is it just these two major data sets? Or how do you find other

32:23.280 --> 32:27.200
 models that are trained on beside the planet and you imagine it?

32:29.520 --> 32:31.040
 You mean beside planet and pets?

32:32.080 --> 32:33.360
 Sorry. Yeah. That's it.

32:33.360 --> 32:35.200
 And so what was your question? How do you do what with them?

32:36.080 --> 32:41.520
 How do you find other trained pre trained models that have been worked on different

32:41.520 --> 32:42.320
 images?

32:42.320 --> 32:46.880
 Oh, these all use pre trained models pre trained on ImageNet. These are only using

32:46.880 --> 32:52.720
 pre trained models pre trained on ImageNet. So how do you find pre trained models pre trained

32:52.720 --> 33:00.240
 on other things? Mainly you don't. There aren't many. But you know, just Google.

33:02.960 --> 33:07.120
 Depends what you're interested in and academic papers.

33:11.440 --> 33:17.680
 There is a, I don't know how it's doing. There was a, yeah, here we are, model zoo.

33:17.680 --> 33:25.680
 So there is a model zoo which I've never had much success with, to be honest.

33:28.880 --> 33:36.000
 So these are a range of pre trained models that you can download.

33:38.240 --> 33:41.600
 Yeah. But as I say, I haven't found it particularly successful, to be honest.

33:41.600 --> 33:48.960
 You could also try papers with, papers with code.

33:52.400 --> 33:59.520
 And I think these, yeah, they have a link to the paper and the code. That doesn't necessarily

33:59.520 --> 34:04.400
 mean they've got a pre trained model. But then you can just click on the code and

34:04.400 --> 34:08.560
 then see.

34:16.800 --> 34:22.160
 And of course for NLP models, there's the hugging face model hub which we've seen before.

34:22.160 --> 34:29.280
 And that isn't easy answer to NLP. It's like, but lots of different pre trained models are on that hub.

34:29.280 --> 34:35.120
 And Jeremy, since you touch on academic papers and papers with code,

34:36.560 --> 34:42.960
 first question, will this comparison, will you, do you or Thomas intend to publish it?

34:45.520 --> 34:49.760
 And if not, if you were to do that, well, what would you go for actually?

34:50.640 --> 34:52.240
 What kind of journal would you look at?

34:52.240 --> 35:01.280
 Yeah. So I'm not a good person to ask that question because I very rarely publish anything, which is partly a philosophical thing.

35:01.280 --> 35:11.520
 You know, I find academia, you know, overly exclusive and I don't, you know, and I don't love PDFs as a publication form.

35:11.520 --> 35:17.520
 And I don't love the writing style, which is kind of required if you're going to get published as being like rather

35:17.520 --> 35:20.800
 difficult to follow.

35:22.880 --> 35:28.480
 I have published a couple of papers, but like only really one

35:30.160 --> 35:32.720
 significant deep learning one. And that was because

35:34.480 --> 35:41.280
 guy named Sebastian Ruder was doing his PhD at the time and he said it'd be really helpful to him if we could co publish

35:41.280 --> 35:50.400
 something and that he would kind of take the lead on writing the paper. And so that was good because I'm always very happy to help students and

35:52.080 --> 35:57.280
 you know, I didn't need to do a good job and he was of use terrific research at a work with.

35:58.240 --> 36:05.760
 The other time I've written a paper, the main time was when I wanted to get the message out about masks and I felt like it probably

36:05.760 --> 36:12.000
 not going to be taken seriously unless it's in an exclusive academic paper because medical people are very into exclusive things.

36:13.520 --> 36:17.280
 Yeah, so I don't know. Like I'd say like this kind of thing

36:19.120 --> 36:22.320
 I suspect would be quite hard to publish because most

36:23.440 --> 36:28.160
 deep learning academic venues are very focused on things with kind of

36:29.120 --> 36:33.920
 reasonably strong theoretical pieces and this kind of

36:33.920 --> 36:38.880
 field of like trying things and seeing what works

36:40.320 --> 36:51.920
 is you know, experiment based is certainly a very important part of science in other areas, but in the deep learning world it hasn't really yet been recognised as a

36:53.040 --> 36:55.920
 valid source of research as far as I can tell.

36:56.640 --> 37:01.760
 Oh, I could concur with all the domains and feel the same quandary to be nice to you.

37:01.760 --> 37:03.920
 Fair enough. What's your domain?

37:05.200 --> 37:08.960
 Hydrology, but more the computational science part of it.

37:14.400 --> 37:20.240
 Okay, so then what I did

37:25.120 --> 37:25.920
 was I

37:25.920 --> 37:31.920
 I mean this is kind of a bit at the same time, but I went back to Patty

37:35.440 --> 37:46.160
 and I wanted to try out a few of these interesting looking models reasonably quickly.

37:46.160 --> 37:54.320
 So what I did was I kind of took our standard, well in this case three lines of code because I

38:01.200 --> 38:08.160
 already untied it earlier, took our three lines of code so I could basically say train

38:08.160 --> 38:21.360
 and pass in an architecture and pass in some per item preprocessing, in this case resizing everything to the same square using squish,

38:21.360 --> 38:30.560
 and some per batch preprocessing, which is in this case is the standard fast AI data augmentation transforms targeting a final size of 224,

38:30.560 --> 38:33.200
 which is what most models tend to be trained at.

38:33.200 --> 38:43.200
 And so I then trained a model using those parameters and then finally it would use test time augmentation.

38:43.200 --> 38:51.200
 So test time augmentation is where I think we briefly mentioned it last time we on this case on the validation set.

38:51.200 --> 39:11.200
 I basically run the model the pre the fine tuned model four times using random data augmentation each time, and then I run it one more time with no data augmentation at all and take an average of all of those five predictions.

39:11.200 --> 39:21.200
 Basically, and that gives me some predictions and then I take an error rate for TTA for the test time augmentation.

39:21.200 --> 39:29.200
 So that basically spits out a number which is an error rate for Paddy.

39:29.200 --> 39:35.200
 And I use a fixed random seed when picking out my validation set.

39:35.200 --> 39:41.200
 So each time I run this, it's going to be with the same validation set, and so I can compare.

39:41.200 --> 39:45.200
 So I've got a few different conch next small models over run.

39:45.200 --> 39:50.200
 First of all, by squishing when I resize.

39:50.200 --> 39:57.200
 And then by cropping when I resize.

39:57.200 --> 40:01.200
 So that was two three five is also two three five.

40:01.200 --> 40:10.200
 And then instead of resizing to a square I resize to a rectangle.

40:10.200 --> 40:16.200
 In theory, this wouldn't have been necessary. I thought they were all 480 by 680 sorry 480 by 640.

40:16.200 --> 40:18.200
 But what I ran this I got an error.

40:18.200 --> 40:29.200
 And then I looked back at the results of that parallel image sizing thing we ran and I realized there was actually three or four images that were the opposite aspect ratio.

40:29.200 --> 40:34.200
 That's why. So the most the vast majority of the images this resizing does nothing at all.

40:34.200 --> 40:38.200
 Like this three or four that are the opposite aspect ratio.

40:38.200 --> 40:42.200
 And then for the augmentation yeah picker.

40:42.200 --> 40:47.200
 Size based on two to four.

40:47.200 --> 40:53.200
 Of a similar aspect ratio. But what I'm actually aiming for here.

40:53.200 --> 40:59.200
 Is something that is a multiple of 32 on both edges.

40:59.200 --> 41:05.200
 And the reason for that we'll kind of get into later when we learn about how convolutional networks really well really work.

41:05.200 --> 41:11.200
 But it basically turns out that the kind of the final patch size in a conf net is 32 by 32 pixels.

41:11.200 --> 41:18.200
 So you generally want both of your sides normally you want them to be multiples of 32.

41:18.200 --> 41:23.200
 So this one got a pretty similar result again to 40.

41:23.200 --> 41:27.200
 And then I wasn't sure about my contention that they need to be multiples of 32.

41:27.200 --> 41:34.200
 I thought maybe it's better if they like a really crisp resizing by using an exact multiple.

41:34.200 --> 41:37.200
 So I tried that as well.

41:37.200 --> 41:44.200
 And that as I suspected was a bit worse.

41:44.200 --> 41:50.200
 And.

41:50.200 --> 41:57.200
 Oh, what's this? I've got some which which ones are the right way around now I'm confused.

41:57.200 --> 42:01.200
 I think.

42:01.200 --> 42:08.200
 Let's check.

42:08.200 --> 42:14.200
 Some of these originally I had my aspect ratio backwards.

42:14.200 --> 42:20.200
 That's why I've got both. It looks like I never got around to removing the ones that were unnecessary.

42:20.200 --> 42:27.200
 Oops.

42:27.200 --> 42:41.200
 Muscle size.

42:41.200 --> 42:45.200
 First off.

42:45.200 --> 42:59.280
 method equals add, oops, pad mode.

42:59.280 --> 43:09.240
 This just, this makes it a bit easier to see what's going on if you do padding with black

43:09.240 --> 43:19.160
 around them.

43:19.160 --> 43:26.240
 There we go.

43:26.240 --> 43:27.240
 Okay, yeah.

43:27.240 --> 43:28.800
 So you can clearly see this is the one way around, right?

43:28.800 --> 43:32.920
 I've tried to make them wide, but actually they were tall.

43:32.920 --> 43:43.000
 So the best way around is actually 640 by 480.

43:43.000 --> 43:45.520
 That's more like it.

43:45.520 --> 43:48.320
 So 640 by 480 is best.

43:48.320 --> 43:53.040
 So let's get rid of the ones that were the wrong way around.

43:53.040 --> 43:55.040
 Okay.

43:55.040 --> 43:57.200
 All right.

43:57.200 --> 44:07.360
 Yeah, so that was all various different transforms, preprocessing for conflict next small, and

44:07.360 --> 44:14.120
 then I did the same thing for one of the VITs, VIT small.

44:14.120 --> 44:21.920
 Now, VIT, remember I mentioned it can only work on 224 by 224 images.

44:21.920 --> 44:25.680
 So these rectangular approaches aren't going to be possible.

44:25.680 --> 44:32.080
 So I've just got the squish and the crop versions.

44:32.080 --> 44:35.880
 The crop version doesn't look very good.

44:35.880 --> 44:52.120
 The squish version must look pretty good.

44:52.120 --> 44:57.760
 I also tried a pad version, which looks pretty good.

44:57.760 --> 45:00.440
 And then, yeah, I also tried to swin.

45:00.440 --> 45:04.120
 So here's swin V2.

45:04.120 --> 45:12.800
 And this one is slow and memory intensive.

45:12.800 --> 45:17.000
 So I had to go down to the 192 pixel version.

45:17.000 --> 45:20.200
 But actually it seems to work very well.

45:20.200 --> 45:23.440
 This is the first time we've had one that's better than.02.

45:23.440 --> 45:33.120
 This is interesting.

45:33.120 --> 45:34.520
 This one's also very good.

45:34.520 --> 45:42.040
 So it's interesting that this slow memory intensive model works better even on smaller

45:42.040 --> 45:50.040
 size, 192 pixel size, which I think is pretty interesting.

45:50.040 --> 45:53.560
 And then there was one more swin, which seemed to do pretty well.

45:53.560 --> 45:59.000
 So I included that, which I was able to do at 224.

45:59.000 --> 46:05.640
 That one had okay results.

46:05.640 --> 46:08.920
 So like I kind of did that for all these different small models.

46:08.920 --> 46:11.760
 And as you can see, they run pretty quickly, right?

46:11.760 --> 46:15.800
 Five or ten minutes.

46:15.800 --> 46:20.240
 So then I picked out the ones that look pretty fast.

46:20.240 --> 46:21.240
 That sounded pretty fast.

46:21.240 --> 46:23.760
 Pretty accurate.

46:23.760 --> 46:32.680
 And created just a copy of that, which I called Petty Large.

46:32.680 --> 46:41.200
 And this time I just replaced small with large.

46:41.200 --> 46:42.200
 And actually I've made a mistake.

46:42.200 --> 46:45.760
 I'm going to have to rerun this because there should not be a small.

46:45.760 --> 46:46.760
 And I see it equals 42.

46:46.760 --> 46:50.800
 I actually want to run this on a different subset each time.

46:50.800 --> 46:54.320
 And the reason why is my plan is to train.

46:54.320 --> 47:01.320
 So basically what I did is I deleted the ones that were less good in Petty Small.

47:01.320 --> 47:06.360
 And so now I'm just running the large ones.

47:06.360 --> 47:12.240
 Now some of these, particularly something like this one, which is 288 by 224, they ran

47:12.240 --> 47:13.320
 out of memory.

47:13.320 --> 47:16.640
 They were too big for my graphics card.

47:16.640 --> 47:21.120
 And a lot of people at this point say, oh, I need to go buy a more expensive graphics

47:21.120 --> 47:22.120
 card.

47:22.120 --> 47:24.920
 But that's not true.

47:24.920 --> 47:27.560
 You don't.

47:27.560 --> 47:37.280
 So if you guys remember our training loop, we get the gradients.

47:37.280 --> 47:41.080
 We add the gradients times a learning rate to the weights.

47:41.080 --> 47:44.440
 And then we zero the gradients.

47:44.440 --> 47:52.480
 What you could do is half the batch size, so for example from 64 to 32, and then only

47:52.480 --> 47:57.600
 zero the gradients every two iterations.

47:57.600 --> 48:00.680
 And so and only do the update every two iterations.

48:00.680 --> 48:06.160
 So basically you can calculate in two batches what you used to calculate in one batch and

48:06.160 --> 48:08.640
 it will be mathematically identical.

48:08.640 --> 48:11.560
 And that's called gradient accumulation.

48:11.560 --> 48:18.320
 And so for the ones which ran out of memory, I added this little acume equals true, which

48:18.320 --> 48:20.360
 is here in my function.

48:20.360 --> 48:24.680
 And I said, yeah, I said if a queue equals true, then set the batch size to 32, because

48:24.680 --> 48:27.080
 by default it's 64.

48:27.080 --> 48:31.080
 And add this thing called a callback.

48:31.080 --> 48:34.160
 Callbacks are basically things that change the behavior of the training.

48:34.160 --> 48:52.920
 And there's a thing called gradient accumulation callback, which gradient accumulation.

48:52.920 --> 48:56.000
 And this is like just the people that are interested.

48:56.000 --> 49:00.480
 This is not like massively complex stuff.

49:00.480 --> 49:05.240
 The entire gradient accumulation callback is that many lines of code.

49:05.240 --> 49:06.240
 Right?

49:06.240 --> 49:07.600
 These are not big things.

49:07.600 --> 49:16.360
 And like literally all it does is it keeps account of how many iterations it's been.

49:16.360 --> 49:25.560
 And it adds the keeps track of the count.

49:25.560 --> 49:31.000
 And as long as we're not up to the point where we, there's the number of accumulations we

49:31.000 --> 49:38.280
 want, we skip the step and the zero gradient basically.

49:38.280 --> 49:45.720
 So it's, yeah, things like gradient accumulation, they sound like big complex things, but they

49:45.720 --> 49:56.840
 are turned out not to be at least when you have a nice code base like fasting eyes.

49:56.840 --> 50:00.840
 Jeremy, can I get a question here to scratch?

50:00.840 --> 50:01.840
 Of course.

50:01.840 --> 50:10.200
 How exactly do the batch size mass notions worry?

50:10.200 --> 50:13.520
 So we will get into that in detail in the course.

50:13.520 --> 50:16.960
 But certainly we get into it in detail in the book.

50:16.960 --> 50:25.040
 But basically all that happens is we randomly shuffle the data set and we grab, so if the

50:25.040 --> 50:31.920
 batch size is 64, we grab the next 64 images.

50:31.920 --> 50:37.480
 We resize them all to be the same size and we stack them on top of each other.

50:37.480 --> 50:46.520
 So if it's black and white images, for example, we would have 64, whatever, 640 by 480 images.

50:46.520 --> 50:55.880
 And so we end up with a 64 by 640 by 480 range tensor.

50:55.880 --> 51:06.120
 And pretty much all the like functionality provided by trytorch will work fine for a

51:06.120 --> 51:13.840
 mini batch of things just as it would for a single thing on the whole.

51:13.840 --> 51:25.280
 So in the large scheme of things, you know, like some huge process that's trying to characterize

51:25.280 --> 51:28.440
 what role is the batch sort of claim?

51:28.440 --> 51:33.400
 Well, it's just about trying to get the most out of your GPU.

51:33.400 --> 51:37.280
 Your GPU can do 10,000 things at once.

51:37.280 --> 51:42.920
 So if you just give it one image at a time, you can use it.

51:42.920 --> 51:48.600
 So if you give it 64 things, it can do one thing on each image and then on each channel

51:48.600 --> 51:49.600
 in that image.

51:49.600 --> 51:54.000
 And then you don't have another few other kind of degrees of paralyzation it can do.

51:54.000 --> 51:55.880
 And so that's where you start.

51:55.880 --> 52:03.320
 But we saw that Nvidia SMI Dmon command that shows you the utilization of your symmetric

52:03.320 --> 52:04.640
 multiprocessor.

52:04.640 --> 52:10.120
 If you use a batch size of one, you'll see that SM will be like 1%, 2%, and everything

52:10.120 --> 52:12.480
 will be useless.

52:12.480 --> 52:18.360
 It's a bit tricky at inference time, you know, in production or whatever, because, you know,

52:18.360 --> 52:21.200
 most of the time you only get one thing to do at a time.

52:21.200 --> 52:28.800
 And so often inference is done on CPU rather than GPU because we don't get to benefit from

52:28.800 --> 52:29.800
 batching.

52:29.800 --> 52:39.120
 Or people will queue a few of them up and stick the model on the GPU at once and stuff

52:39.120 --> 52:40.120
 like that.

52:40.120 --> 52:44.080
 But for training, it's pretty easy to take advantage of many branches.

52:44.080 --> 52:45.880
 No worries.

52:45.880 --> 52:52.680
 Jamie, you've trained so many models.

52:52.680 --> 52:58.280
 Will you consider using a majority vote or something like that?

52:58.280 --> 53:03.800
 No, I wouldn't because a majority vote throws away information.

53:03.800 --> 53:07.720
 It throws away the probabilities.

53:07.720 --> 53:14.720
 So I pretty much always find I get better results by averaging the probabilities.

53:14.720 --> 53:22.280
 So each of the models after I've trained it, I'm exporting to a uniquely named model, which

53:22.280 --> 53:26.960
 is going to be the name of the architecture and then an underscore and then some description,

53:26.960 --> 53:29.200
 which is just the thing I pass in.

53:29.200 --> 53:35.600
 And so that way, yeah, when I'm done training, I can just have a little loop which opens

53:35.600 --> 53:45.400
 each of those models up, grabs the TTA predictions, sticks them into our list, and then at the

53:45.400 --> 53:53.760
 end, I'll average those TTA predictions across the models and that will be my ensemble prediction.

53:53.760 --> 53:54.760
 So that's my next step.

53:54.760 --> 53:56.760
 I'm not up to that yet.

53:56.760 --> 53:57.760
 Okay.

53:57.760 --> 53:58.760
 All right.

53:58.760 --> 54:04.240
 Well, I think that's it.

54:04.240 --> 54:09.520
 So that's really more of a little update on what I've been doing over my weekend.

54:09.520 --> 54:17.600
 Hopefully, yeah, gives you some ideas for things to try.

54:17.600 --> 54:24.960
 And hopefully you find the Kaggle book useful.

54:24.960 --> 54:34.000
 So, Jeremy, so how many are those videos spending all these explanations because you spend a

54:34.000 --> 54:36.000
 lot of experience here?

54:36.000 --> 54:44.040
 So, you know, it's like a week or two of work to do the fine tuning experiments, but that

54:44.040 --> 54:49.280
 was like a few hours here and a few hours there.

54:49.280 --> 55:05.000
 The final sweep was probably maybe six hours of 3 GPUs.

55:05.000 --> 55:10.800
 The Paddy Competition stuff was maybe four hours a day over the last four days since I

55:10.800 --> 55:19.920
 last saw you guys and riding the notebook was maybe another four hours.

55:19.920 --> 55:20.920
 Thanks.

55:20.920 --> 55:21.920
 It helps.

55:21.920 --> 55:22.920
 No worries.

55:22.920 --> 55:23.920
 All right.

55:23.920 --> 55:24.920
 Bye, everybody.

55:24.920 --> 55:25.920
 Nice to see you all.

55:25.920 --> 55:26.920
 Bye.

55:26.920 --> 55:27.920
 Thanks, Jeremy.

55:27.920 --> 55:28.920
 Bye, everyone.

55:28.920 --> 55:41.040
 Bye.

