WEBVTT

00:00.000 --> 00:08.320
 Okay. All right. So this is the repo.

00:08.320 --> 00:10.380
 Fasti, paper space set up.

00:10.380 --> 00:12.560
 I've started a machine.

00:12.560 --> 00:15.120
 I'll cd to my home directory.

00:15.120 --> 00:17.480
 I'll get clone the repo.

00:18.480 --> 00:21.720
 I'll cd into the thing I get cloned.

00:21.720 --> 00:25.520
 I'll run.slash setup.sh.

00:25.520 --> 00:27.760
 Okay. It says install complete.

00:27.760 --> 00:29.040
 Please start a new instance.

00:29.040 --> 00:31.040
 So then I'll stop the machine.

00:35.360 --> 00:37.360
 And then I'll start a machine.

00:48.400 --> 00:54.560
 And that's going to install a prerun.sh script,

00:54.560 --> 00:58.400
 which is going to set up all these things

00:58.400 --> 01:00.440
 and all these things.

01:00.440 --> 01:06.160
 And it's going to install a.bash.local script,

01:06.160 --> 01:07.400
 which will set up our path.

01:07.400 --> 01:09.000
 And it's going to also install things

01:09.000 --> 01:10.640
 to set up things for installing software.

01:10.640 --> 01:15.000
 PIP I for PIP install and Mamba I for Mamba install.

01:15.000 --> 01:25.000
 So we now have a machine running.

01:25.000 --> 01:40.000
 And so we should now put a credit terminal.

01:40.000 --> 01:50.000
 I just pressed press terminal.

02:12.680 --> 02:13.680
 Something's happening.

02:13.680 --> 02:14.520
 Great.

02:14.520 --> 02:15.520
 Okay.

02:15.520 --> 02:18.080
 Check creating a terminal here then.

02:18.080 --> 02:20.280
 Okay. Much better.

02:20.280 --> 02:20.720
 All right.

02:20.720 --> 02:23.920
 So in theory, if we look at our home directory,

02:23.920 --> 02:24.840
 oh, look at that.

02:24.840 --> 02:30.640
 Well, this stuff is now sim linked to slash storage.

02:30.640 --> 02:47.640
 So I should be able to PIP I fast AI and get the latest version.

02:47.640 --> 02:54.440
 I wonder if I can add a minus U to say upgrade.

02:57.840 --> 02:59.640
 Yes, I can.

02:59.640 --> 03:02.400
 So that's how I get the latest version.

03:02.400 --> 03:11.640
 And so that should have installed it locally.

03:11.640 --> 03:13.880
 There it is.

03:13.880 --> 03:16.880
 And okay.

03:16.880 --> 03:33.760
 So now if I create a notebook.

03:33.760 --> 03:35.680
 Import fast AI.

03:35.680 --> 03:39.960
 Fast AI version.

03:39.960 --> 03:41.760
 Oh, look, that's a good start.

03:41.760 --> 03:42.280
 Okay.

03:42.280 --> 03:44.080
 Next question.

03:44.080 --> 03:50.120
 Can we install binaries?

03:50.120 --> 03:52.880
 For example, universal syntax.

03:52.880 --> 04:17.840
 I remember install universal syntax.

04:17.840 --> 04:18.600
 There we go.

04:18.600 --> 04:22.840
 So you see the nice thing about this is even all this persistent stuff we're installing

04:22.840 --> 04:30.600
 into, you know, all works on the free paper space as well.

04:30.600 --> 04:34.400
 So we should now be able to check C tags.

04:34.400 --> 04:36.320
 Ta da.

04:36.320 --> 04:38.360
 It works.

04:38.360 --> 04:42.080
 And which one is it?

04:42.080 --> 04:47.080
 And that is actually in our storage.

04:47.080 --> 04:48.920
 Oh, so I think we've done it.

04:48.920 --> 04:53.160
 What do you guys think?

04:53.160 --> 04:55.160
 Simple enough?

04:55.160 --> 04:58.000
 It's good.

04:58.000 --> 05:00.000
 All right.

05:00.000 --> 05:02.080
 It's good.

05:02.080 --> 05:03.080
 Okay.

05:03.080 --> 05:12.800
 So next step is I thought we might try to fix a, I don't know if you call it fixing a

05:12.800 --> 05:19.800
 bug or maybe it's probably we could generously call it adding an enhancement to fast AI, which

05:19.800 --> 05:26.280
 is to add normalization to Tim models.

05:26.280 --> 05:34.000
 So all right.

05:34.000 --> 05:41.000
 So it's grab fast AI.

05:41.000 --> 05:50.240
 Now this is where, so when I get blown this, so let's go to notebooks.

05:50.240 --> 05:55.320
 So slash notebooks is persistent on a particular machine.

05:55.320 --> 05:58.680
 And I think this will not work because I'm using SSH.

05:58.680 --> 06:04.960
 Oh, it's already there.

06:04.960 --> 06:05.960
 That's interesting.

06:05.960 --> 06:23.640
 Oh, you know, so there's a bug in our script, which is I didn't pop D. So let's fix that.

06:23.640 --> 06:24.640
 Prerun.sh.

06:24.640 --> 06:26.880
 I did a push D at the start.

06:26.880 --> 06:31.720
 No pop D at the end.

06:31.720 --> 06:33.720
 Okay.

06:33.720 --> 06:36.720
 Fixed.

06:36.720 --> 06:40.720
 All right.

06:40.720 --> 06:52.720
 No worries.

06:52.720 --> 06:58.720
 That means.

06:58.720 --> 06:59.720
 Okay.

06:59.720 --> 07:00.720
 Yes.

07:00.720 --> 07:08.720
 No worries.

07:08.720 --> 07:09.720
 All right.

07:09.720 --> 07:13.720
 So let's restart this.

07:13.720 --> 07:22.720
 And then I'll tell you about the bug we're fixing while we wait for it.

07:22.720 --> 07:23.720
 Okay.

07:23.720 --> 07:46.440
 So, so, so normalization is where we subtract the means and divide by the standard deviation

07:46.440 --> 07:50.240
 of each channel for vision.

07:50.240 --> 07:56.160
 And that goes, that's a transform called normalize.

07:56.160 --> 08:06.640
 And we need to use the same standard deviation and mean that was used in the, when the model

08:06.640 --> 08:10.480
 was pre trained.

08:10.480 --> 08:14.280
 Because you know, there is, you know, so some people will normalize.

08:14.280 --> 08:17.120
 So it's everything's between zero and one, some will normalize.

08:17.120 --> 08:22.680
 So it's got a mean of zero and a standard deviation of one.

08:22.680 --> 08:26.080
 So we need to make sure we use the same.

08:26.080 --> 08:30.800
 You don't divide by the same thing to track the same thing.

08:30.800 --> 08:36.680
 If you look at vision learner.

08:36.680 --> 08:39.400
 Vision learner has a normalized parameter.

08:39.400 --> 08:50.200
 And if it's true, then it will attempt to add the correct normalization here.

08:50.200 --> 08:54.120
 So if it's not a pre trained model, it doesn't do anything because it doesn't know what to

08:54.120 --> 08:55.120
 normalize by.

08:55.120 --> 09:01.360
 Otherwise, it's going to try and get the correct statistics from the models metadata.

09:01.360 --> 09:08.680
 So the models metadata is here, model underscore meta.

09:08.680 --> 09:23.440
 And it's just a list of models with, with metadata and the metadata here, stats, image

09:23.440 --> 09:24.560
 net stats.

09:24.560 --> 09:30.200
 So the image net stats is the mean and standard deviation of image net, which I can't quite

09:30.200 --> 09:37.200
 remember where that comes from, but that's something we import from somewhere.

09:37.200 --> 09:39.120
 So none of these are 10 models.

09:39.120 --> 09:47.680
 And so that means currently 10 models aren't normalized.

09:47.680 --> 09:49.160
 Now.

09:49.160 --> 10:13.320
 Tim has its own stats.

10:13.320 --> 10:35.920
 Not this, not this.

10:35.920 --> 10:37.720
 There's a lot of stuff in TMI still I haven't looked into.

10:37.720 --> 10:49.960
 I actually haven't used this transforms factory.

10:49.960 --> 11:03.160
 Maybe in fast AI three, we should consider using more of this functionality from Tim.

11:03.160 --> 11:10.760
 It's like a configuration for them.

11:10.760 --> 11:19.360
 I guess we can just try and find it.

11:19.360 --> 11:36.320
 I actually forgot to edit this.

11:36.320 --> 11:38.320
 My bad.

11:38.320 --> 12:03.840
 Stop the machine.

12:03.840 --> 12:05.280
 Here we go.

12:05.280 --> 12:06.280
 Okay.

12:06.280 --> 12:13.560
 So we can just do this locally now.

12:13.560 --> 12:17.800
 All right.

12:17.800 --> 12:28.200
 So this happens in vision learner.

12:28.200 --> 12:30.440
 And Tim is optional.

12:30.440 --> 12:35.600
 You don't have to use it.

12:35.600 --> 12:43.040
 But if you do, then we have a create Tim model, which you don't normally call yourself.

12:43.040 --> 12:47.520
 Normally you just call vision learner and you pass in an architecture as a string.

12:47.520 --> 12:56.400
 And if it's a string, it will create a Tim model for you.

12:56.400 --> 13:00.680
 So there's a best models, for example.

13:00.680 --> 13:05.760
 Let's say conv next or something like that.

13:05.760 --> 13:07.560
 I don't know what conv it is.

13:07.560 --> 13:09.200
 Never tried that one.

13:09.200 --> 13:15.440
 Let's do a tiny.

13:15.440 --> 13:18.520
 So we can create a model using create model.

13:18.520 --> 13:23.000
 We pass in a string.

13:23.000 --> 13:28.520
 And I have a feeling that's, yeah, that's got a config.

13:28.520 --> 13:40.280
 Yeah, see, and it's got a mean and a standard deviation.

13:40.280 --> 13:43.480
 So models equals Tim dot list models.

13:43.480 --> 13:53.840
 Maybe just do pre trained ones.

13:53.840 --> 14:02.720
 I wonder if they all have this for M in models.

14:02.720 --> 14:14.760
 So let's create a model and have a look at M dot default config.

14:14.760 --> 14:23.640
 I mean.

14:23.640 --> 14:31.200
 And to the deviation.

14:31.200 --> 14:34.120
 Yeah.

14:34.120 --> 14:54.920
 Yeah, so you can see a lot of them use.5.

14:54.920 --> 15:01.080
 And then some of them use image stats stats.

15:01.080 --> 15:08.360
 And guessing they're the only two options.

15:08.360 --> 15:17.360
 So okay, so hopefully you get the idea.

15:17.360 --> 15:24.080
 Jim, you just read out usually putting the image in the mean to the user and standard

15:24.080 --> 15:29.240
 duration should be one by.

15:29.240 --> 15:31.080
 I mean, not necessarily.

15:31.080 --> 15:40.480
 Sometimes people make the minimum zero in the maximum one.

15:40.480 --> 15:45.000
 But what we need to do is use the same stats that it was pre trained with.

15:45.000 --> 15:49.280
 Because we want our range to be the same as the range that was pre trained with otherwise

15:49.280 --> 15:56.920
 our data has a different meaning.

15:56.920 --> 16:05.240
 So let's go to add norm.

16:05.240 --> 16:08.480
 Okay, so here's add norm.

16:08.480 --> 16:16.120
 And it's being passed a meta.

16:16.120 --> 16:21.760
 Stats.

16:21.760 --> 16:31.560
 So this only works for non team.

16:31.560 --> 16:36.640
 So how about we put this here and we'll create an else.

16:36.640 --> 16:41.880
 Or I guess really an L if.

16:41.880 --> 16:51.640
 And then here we'll have for Tim if normalized.

16:51.640 --> 16:58.640
 We could have a Tim normalized.

16:58.640 --> 17:14.600
 We can refactor out some duplicate code later.

17:14.600 --> 17:40.800
 And basically for Tim, we're going to be passing in the architecture.

17:40.800 --> 17:47.120
 We don't need to pass in the architecture, we can just pass in the model.

17:47.120 --> 17:56.840
 And to protect against future like ability to pass in other types of the strings that

17:56.840 --> 18:01.120
 aren't Tim, do you think there's any benefit in having like default normalization function

18:01.120 --> 18:05.680
 that if you pass through, you can actually do your own normalization?

18:05.680 --> 18:14.080
 No, because my answer to all of those questions is always you won't get a native.

18:14.080 --> 18:24.360
 So I very intentionally don't do like, you know, dealing with things that might not happen

18:24.360 --> 18:25.360
 in the future.

18:25.360 --> 18:29.760
 It'd be simpler just to create your own vision liner because that looks like there's not

18:29.760 --> 18:33.240
 much going on there that you can duplicate if you wanted to have support for a different

18:33.240 --> 18:34.240
 model.

18:34.240 --> 18:35.240
 Yeah, exactly.

18:35.240 --> 18:43.640
 I mean, this is just a small little wrapper really.

18:43.640 --> 18:48.120
 You can call create Tim model or create vision model.

18:48.120 --> 18:49.440
 You can call learner.

18:49.440 --> 18:53.840
 You can call create head.

18:53.840 --> 18:55.440
 Yep.

18:55.440 --> 18:56.440
 Okay.

18:56.440 --> 19:00.320
 So we'll call that.

19:00.320 --> 19:17.160
 So the normalize takes a mean and a standard deviation.

19:17.160 --> 19:25.200
 So it should be just those two things, I guess.

19:25.200 --> 19:40.160
 So, all right.

19:40.160 --> 19:47.920
 Okay.

19:47.920 --> 19:57.560
 Tim normalize using the model and pretread.

19:57.560 --> 20:05.480
 Okay.

20:05.480 --> 20:20.000
 I see I already had an else there.

20:20.000 --> 20:24.200
 So do that.

20:24.200 --> 20:30.480
 There we go.

20:30.480 --> 20:32.920
 And okay.

20:32.920 --> 20:37.800
 So let's test this out.

20:37.800 --> 20:42.280
 Tls.

20:42.280 --> 20:51.520
 So what happens when you add a transform?

20:51.520 --> 21:08.040
 Add a transform to each data loader in it.

21:08.040 --> 21:09.040
 Okay.

21:09.040 --> 21:10.040
 So what does that do?

21:10.040 --> 21:11.040
 Yeah.

21:11.040 --> 21:12.040
 What did I do wrong?

21:12.040 --> 21:13.040
 Okay.

21:13.040 --> 21:38.560
 Oh, it's part of I see it's part of.

21:38.560 --> 21:48.560
 Okay.

21:48.560 --> 21:57.560
 That's a bit confusing.

21:57.560 --> 21:59.560
 Right.

21:59.560 --> 22:01.560
 Okay.

22:01.560 --> 22:07.560
 So let's find sometimes it's just easiest to look at the code.

22:07.560 --> 22:15.560
 Add trifons.

22:15.560 --> 22:24.680
 I see.

22:24.680 --> 22:29.080
 So it's just calling add.

22:29.080 --> 22:31.920
 I see for this particular event.

22:31.920 --> 22:37.600
 And we're adding it.

22:37.600 --> 22:40.400
 I see we're adding it to the after batch event.

22:40.400 --> 22:46.640
 So we should find there's a after batch event.

22:46.640 --> 22:47.640
 Here we are.

22:47.640 --> 22:48.640
 I see.

22:48.640 --> 22:50.000
 And there's our transforms.

22:50.000 --> 22:57.760
 So if we call vision learner, that should change our data loader.

22:57.760 --> 22:58.760
 Yep.

22:58.760 --> 23:05.280
 Now I've got normalize using image net stats.

23:05.280 --> 23:10.640
 And if we now try it for a string version.

23:10.640 --> 23:18.880
 Oh, no, that's interesting.

23:18.880 --> 23:22.640
 Okay.

23:22.640 --> 23:31.640
 What happened differently?

23:31.640 --> 23:33.360
 Oh, I see.

23:33.360 --> 23:40.840
 We need to recreate the data loaders for this test.

23:40.840 --> 23:47.040
 So that doesn't have normalize anymore.

23:47.040 --> 23:48.040
 And that gives us.

23:48.040 --> 23:49.040
 Okay.

23:49.040 --> 23:50.040
 That gives us an error.

23:50.040 --> 23:52.840
 This is as we're passing a sequential object.

23:52.840 --> 23:54.320
 Okay.

23:54.320 --> 23:57.480
 That makes sense.

23:57.480 --> 24:07.800
 Because create Tim model actually, yeah, modifies things.

24:07.800 --> 24:10.880
 That's why.

24:10.880 --> 24:17.200
 And it creates a sequential model because it's got the head and the body in it.

24:17.200 --> 24:24.200
 So we need to change how we do this.

24:24.200 --> 24:27.920
 All right.

24:27.920 --> 24:38.040
 This is Tim body here is the model.

24:38.040 --> 24:46.240
 And oh, look here we use default config to get stuff here.

24:46.240 --> 24:59.520
 So Tim body is called from here.

25:21.520 --> 25:27.440
 I guess like it would be nice to know how Tim does this exactly.

25:27.440 --> 25:32.240
 Where does that default config come from?

25:32.240 --> 26:02.200
 So when we call Tim.create model, I wonder if we should.

26:02.200 --> 26:09.200
 Take a look.

26:09.200 --> 26:18.200
 default config.

26:18.200 --> 26:23.000
 It's probably a lot.

26:23.000 --> 26:31.400
 Here's data config.py.

26:31.400 --> 26:32.400
 Where does it get set?

27:31.400 --> 27:43.400
 It seems like this button needs some restructuring.

27:43.400 --> 27:44.400
 It's not surprising.

27:44.400 --> 27:51.400
 It was originally built not to expect between stuff with Tim.

27:51.400 --> 28:03.400
 Create vision model.

28:03.400 --> 28:22.400
 Create body.

28:22.400 --> 28:40.400
 Maybe we should change how these work.

28:40.400 --> 28:46.400
 So let's do something we think about doing some redesign maybe.

28:46.400 --> 28:56.400
 So the idea of the redesign I guess would be that this doesn't instantiate the model.

28:56.400 --> 29:19.400
 But it assumes it's already instantiated.

29:19.400 --> 29:29.400
 So we remove that.

29:29.400 --> 29:51.400
 We're creating body with model.

29:51.400 --> 30:17.400
 So we may as well just do that directly.

30:17.400 --> 30:29.400
 Okay.

30:29.400 --> 30:58.400
 Make this a function.

30:58.400 --> 31:13.400
 It's new on each time.

31:13.400 --> 31:33.400
 Okay. So in this refactoring, we now are passing around models, not architectures.

31:33.400 --> 31:48.400
 The model model doesn't change.

31:48.400 --> 32:01.400
 Okay. So this changes.

32:01.400 --> 32:08.400
 So now we say model equals arch pre change.

32:08.400 --> 32:32.400
 So we're going to do the same thing for Tim.

32:32.400 --> 32:56.400
 So we're going to pass in a model.

32:56.400 --> 33:07.400
 Okay.

33:07.400 --> 33:29.400
 So it's going to be the same here.

33:29.400 --> 33:36.400
 Let's see a vision minus two works.

33:36.400 --> 33:55.400
 Okay, it does.

33:55.400 --> 34:16.400
 So maybe we should move keep moving this back further and further.

34:16.400 --> 34:33.400
 So we're going to make Tim work.

34:33.400 --> 34:47.400
 Do that.

34:47.400 --> 35:07.400
 Maybe we'll just call that the Tim model.

35:07.400 --> 35:26.400
 Okay.

35:26.400 --> 35:39.400
 So the problem with that is the keyword arguments.

35:39.400 --> 35:47.400
 So there's a lot of this is crazy.

35:47.400 --> 35:49.400
 There's a lot of keyword arguments.

35:49.400 --> 36:10.400
 So I think actually what we'll do is we'll do it up here.

36:10.400 --> 36:21.400
 Okay.

36:21.400 --> 36:24.400
 And so Tim body.

36:24.400 --> 36:34.400
 Doesn't need quags anymore.

36:34.400 --> 36:42.400
 So what we might do is we'll say this is the result.

36:42.400 --> 36:48.400
 And we'll return the things.

36:48.400 --> 36:53.400
 Or even return.

36:53.400 --> 36:57.400
 Those two things.

36:57.400 --> 37:07.400
 Now we've got the config.

37:07.400 --> 37:11.400
 And so we can pass the config.

37:11.400 --> 37:39.400
 Like so.

37:39.400 --> 37:49.400
 So how much we broke.

37:49.400 --> 37:50.400
 Okay.

37:50.400 --> 37:57.400
 So create Tim model.

37:57.400 --> 38:01.400
 Yes, we do pass in an architecture after all.

38:01.400 --> 38:11.400
 So we'll just change that back.

38:11.400 --> 38:15.400
 Oh, that looks hopeful.

38:15.400 --> 38:22.400
 So we should find that if we create a.

38:22.400 --> 38:32.400
 And check its default config.

38:32.400 --> 38:40.400
 Yep, that looks good.

38:40.400 --> 38:48.400
 Now, comfnext tiny on the other hand uses image nets stats.

38:48.400 --> 39:06.400
 That looks very hopeful.

39:06.400 --> 39:19.400
 So somebody feels like an interesting and valuable problem to solve.

39:19.400 --> 39:25.400
 Making create unit model work with Tim would be super helpful.

39:25.400 --> 39:26.400
 All right.

39:26.400 --> 39:29.400
 Now create unit model.

39:29.400 --> 39:32.400
 Needs to do the same thing.

39:32.400 --> 39:37.400
 So we can do this.

39:37.400 --> 39:47.400
 It has create vision model, which is to actually instantiate the model.

39:47.400 --> 39:51.400
 Is anybody potentially interested in having a go at doing unit models with Tim?

39:51.400 --> 39:54.400
 If so, did you want to talk about it?

39:54.400 --> 39:56.400
 I'd be interested.

39:56.400 --> 39:57.400
 Okay.

39:57.400 --> 40:00.400
 So.

40:00.400 --> 40:10.400
 Are you somewhat familiar with using units in general and in the dynamic unit?

40:10.400 --> 40:12.400
 A little bit. I'm training one at the moment.

40:12.400 --> 40:14.400
 That's my maximum experience.

40:14.400 --> 40:16.400
 And then I've been through some notebooks to walk through.

40:16.400 --> 40:18.400
 Last day I wanted everything.

40:18.400 --> 40:19.400
 Great.

40:19.400 --> 40:21.400
 So, okay. So.

40:21.400 --> 40:22.400
 The interesting.

40:22.400 --> 40:26.400
 Okay. So you know, the basic idea of a unit is.

40:26.400 --> 40:34.400
 That it has.

40:34.400 --> 40:37.400
 Not just the usual kind of.

40:37.400 --> 40:44.400
 Downward sampling path where the image is getting kind of effectively smaller and smaller as it goes through convolutions with strides.

40:44.400 --> 40:46.400
 And we end up with.

40:46.400 --> 40:49.400
 You know, a kind of a very small set of patches.

40:49.400 --> 40:56.400
 And then rather than averaging those to get a vector and using those as our features to our head.

40:56.400 --> 41:01.400
 Instead, we go through reverse convolutions, which are things which make it bigger and bigger.

41:01.400 --> 41:13.400
 And as we do that, we also don't just take the input from the previous layer of the up sampling, but also the input from the equivalently sized down sampling size down sampling there.

41:13.400 --> 41:21.400
 Before fast AI, all units had to be only handled a fixed size.

41:21.400 --> 41:37.400
 But Karen did was he created this thing called the dynamic unit, which would look to see how big each size was on the downward path and automatically create an appropriate size thing on the upward path.

41:37.400 --> 41:42.400
 And that's what the dynamic unit does.

41:42.400 --> 42:01.400
 Fast AI has been very aggressive in like using pre trained models everywhere. So something we added to this idea is this idea that the downward sampling path can be can have a pre trained model, which is not rocket science.

42:01.400 --> 42:09.400
 Obviously, it's like this this one line of code.

42:09.400 --> 42:21.400
 The

42:21.400 --> 42:30.400
 So to understand, like at the moment I'm using say like a resident 34 does that mean the down path is a resident 34 backbone and then there's a reverse resident 34 being automatically generated.

42:30.400 --> 42:58.400
 And not a reverse, it's not a reverse resident 34. It's, it is a resident 34 backbone. So here's our dynamic unit unit. The upward Sam, the up sampling path is has a fixed architecture, which is

42:58.400 --> 43:02.400
 a bar indeed res blocks.

43:02.400 --> 43:10.400
 But they're not like, if you use as a downward sampling part, you know, down sampling of the I T the upward sampling is not going to be a reverse V it, you know, it's not a mirror.

43:10.400 --> 43:12.400
 No, exactly. It's just a bit.

43:12.400 --> 43:15.400
 Would there be an advantage in doing that or is it just not really helpful.

43:15.400 --> 43:17.400
 I don't see why there would be.

43:17.400 --> 43:19.400
 I'd also don't see why there wouldn't be.

43:19.400 --> 43:30.400
 But he's tried it as far as I know I don't even know if there's such a thing as an up sampling transformer block. I, there may well be without digressing.

43:30.400 --> 43:34.400
 Yeah, there's no need to worry about that.

43:34.400 --> 43:40.400
 The key thing is that in the downward sampling path.

43:40.400 --> 43:46.400
 What we do is we, we have the downward sampling bit we call the encoder.

43:46.400 --> 43:59.400
 Okay. And what we do is we do a dummy eval. Now a dummy eval is basically to take a, I can't remember like I either a zero length batch or one length batch like a very small batch and pass it through.

43:59.400 --> 44:02.400
 At some image size.

44:02.400 --> 44:19.400
 And we use, I believe we use hooks if I remember correctly.

44:19.400 --> 44:25.400
 Seven to my screen. My screen's gone crazy.

44:25.400 --> 44:27.400
 Okay.

44:27.400 --> 44:32.400
 Yeah, so we've got these hawks.

44:32.400 --> 44:48.400
 Yes. Okay, so we use fast AI's walk outputs function, which says, I want to use pytorch walks to grab the outputs of these layers.

44:48.400 --> 44:52.400
 And so.

44:52.400 --> 44:57.400
 What is the CCCH indexes.

44:57.400 --> 45:09.400
 So this is yeah okay so that's a great question so this is the indices of this is the key thing this is the indices of the layers where the size changes.

45:09.400 --> 45:10.400
 Right.

45:10.400 --> 45:12.400
 So that's where you want to.

45:12.400 --> 45:14.400
 That's where you want the cross connection.

45:14.400 --> 45:15.400
 Right.

45:15.400 --> 45:18.400
 Either just before that or just after that, you know.

45:18.400 --> 45:22.400
 So get, get, get the indices with the size changes.

45:22.400 --> 45:27.400
 So the sizes.

45:27.400 --> 45:34.400
 Here, model sizes.

45:34.400 --> 45:37.400
 So we hook outputs.

45:37.400 --> 45:41.400
 We do a dummy of owl and we find the shape.

45:41.400 --> 45:49.400
 Of each thing and yeah so he can see dummy of owl is using just a single image.

45:49.400 --> 45:57.400
 And so yeah this just returns the shape of the output of every layer.

45:57.400 --> 46:07.400
 That's going to be in sizes and so then this is just a very simple function, which just goes through and finds where the size changes.

46:07.400 --> 46:12.400
 Okay.

46:12.400 --> 46:15.400
 And so this is the indices of those things.

46:15.400 --> 46:21.400
 So now that we know where the size changes we know where we want our cross connections to be.

46:21.400 --> 46:33.400
 Now for each of the cross connections we need to store the output of the model at that point because that's, that's going to be an input in the up sampling block.

46:33.400 --> 46:36.400
 So these sfs.

46:36.400 --> 46:40.400
 For each unit block we create.

46:40.400 --> 46:48.400
 So for each change in the index for each of sampling block, you have to pass in that that.

46:48.400 --> 46:51.400
 Those outputs.

46:51.400 --> 46:56.400
 Sampling side. So this is the index where it happened. And so this will be the actual.

46:56.400 --> 47:00.400
 So if we go to the unit block.

47:00.400 --> 47:06.400
 So it looks like it's so it's the size of that list minus one is that having a unit blocks get created on the other side.

47:06.400 --> 47:17.400
 So it's going to be past the hook, right, which is, and so that's just the hook that was used.

47:17.400 --> 47:22.400
 That's the hook that was used on the down sampling side.

47:22.400 --> 47:28.400
 And from that we can get the stored activations.

47:28.400 --> 47:32.400
 And so those stored activations then.

47:32.400 --> 47:39.400
 So this is the shape of those stored activations.

47:39.400 --> 47:48.400
 And this is a minor tweak. So let's just ignore this if block for a moment. Basically, all we then do is we take those activations, stick them through a batch norm.

47:48.400 --> 47:59.400
 And then we can concatenate them with the previous layers up sampling.

47:59.400 --> 48:03.400
 And check that through a rally. And then we do.

48:03.400 --> 48:15.400
 And the comm comms aren't just comms. They're fast AI comms, which can include all kinds of things like batch norm activation, whatever.

48:15.400 --> 48:27.400
 So it's, it's a, some combination of batch norm, you know, activation, convolution.

48:27.400 --> 48:32.400
 You can, you can also do up sampling. So it's transpose.

48:32.400 --> 48:42.400
 That's not can go first or last, whatever. So that's quite a, you know, a very rich convolutional layer.

48:42.400 --> 48:53.400
 Okay, so then this if part here is that it's possible that things didn't quite round off nicely, so that the cross connection doesn't quite have the right size.

48:53.400 --> 49:03.400
 And if that happens, then we'll interpolate the cross connection to be the same shape as the up sampling connection.

49:03.400 --> 49:11.400
 And again, I don't know if anybody else does this, but this is to kind of make it so that the dynamic unit always just works.

49:11.400 --> 49:14.400
 That's the basic idea.

49:14.400 --> 49:20.400
 Yeah, so to make this work for Tim.

49:20.400 --> 49:25.400
 You know, this encoder.

49:25.400 --> 49:29.400
 It needs to know about the spots right. Oh, no, it wouldn't do checks with spots.

49:29.400 --> 49:33.400
 So, so, so honestly, this, this might almost just work.

49:33.400 --> 49:37.400
 Like, I don't like, I don't think it does. I think somebody tried it at night.

49:37.400 --> 49:39.400
 Right.

49:39.400 --> 49:44.400
 But yeah, it, it would.

49:44.400 --> 49:48.400
 You know, to, to, to figure out what doesn't work.

49:48.400 --> 49:57.400
 You know, you would need to change this line to say, Oh, if it's a string create trim model otherwise do this, you know.

49:57.400 --> 50:04.400
 And then you'd like create body would need to be creative. Tim body if it's a string. So like at minimum, do the same stuff that create vision model does.

50:04.400 --> 50:07.400
 And then, yeah, and then see if this works.

50:07.400 --> 50:08.400
 Right.

50:08.400 --> 50:12.400
 Well, now I will say, if you do get it working.

50:12.400 --> 50:19.400
 Tim does have an API to actually tell you where the feature size has changed.

50:19.400 --> 50:28.400
 So like you could actually optimize out that dummy of all stuff, but I don't even know if I bother because it makes the code more complex for no particular benefit.

50:28.400 --> 50:29.400
 Yeah, sure.

50:29.400 --> 50:35.400
 So, look, I think if you know, this you commit, this is a PR will definitely be looking at it.

50:35.400 --> 50:39.400
 I was actually going to try conf next in my unit. So I had no idea.

50:39.400 --> 50:48.400
 Actually, so that would have been, I would have noticed that already, but I just had no time. So I'd love to, because I, you know, tried resident 32. I've got particular results and I'd like to see.

50:48.400 --> 50:58.400
 We can push it with a different model. Yeah. No, I mean, I think there'd be a lot of benefit to that. So, all right. So now we should run the tests.

50:58.400 --> 51:10.400
 Just to know, would that all likely be in the same, a notebook that you're editing the vision letter is that when most of the source code is the unit learners, or is it a different.

51:10.400 --> 51:14.400
 I don't know. I was just using this.

51:14.400 --> 51:36.400
 To whatever automatically and VIMS so I was using VMC tags to jump around. So I don't, I have no idea where I was.

51:36.400 --> 51:46.400
 So, yeah, so there's a models unit, so where the dynamic unit lives.

51:46.400 --> 51:47.400
 Okay.

51:47.400 --> 51:59.400
 So, is there anything unique about the fact that the Tim model doesn't, that's sort of an option that it cut the tail and head off. Does that need to be done with the unit architecture.

51:59.400 --> 52:05.400
 Oh, got an error here.

52:05.400 --> 52:24.400
 So, yeah, you absolutely have to cut the head off, because it comes with a default classifier head. So you only, you know, so you know, once you get it working, you'll probably find you can factor out some duplicate code between the unit and the vision letter.

52:24.400 --> 52:32.400
 But yeah, basically have to cut off the classifier head in the same way that create Tim body does.

52:32.400 --> 52:38.400
 And I don't think you'll need to change any input processing as far as I know.

52:38.400 --> 52:53.400
 The vision, create vision model, you know, handles like, you know, if you've only got one or two or four channel inputs in the models of three channel input it handles that automatically.

52:53.400 --> 53:11.400
 But Tim actually, I think Ross and I independently invented this as I know we both kind of automatically handle like copying weights if necessary or deleting weights if necessary or whatever but yeah so the same stuff them vision minor should should work there as well.

53:11.400 --> 53:21.400
 So, interestingly, layers, the layers notebook.

53:21.400 --> 53:30.400
 It doesn't work because it is actually creating a model, which is curious.

53:30.400 --> 53:37.400
 And that will be easily fixed.

53:37.400 --> 53:55.400
 That's interesting.

53:55.400 --> 53:56.400
 Okay.

53:56.400 --> 54:02.400
 Okay.

54:02.400 --> 54:12.400
 So, the big question then is,

54:12.400 --> 54:34.400
 do we still predict rice disease.

55:03.400 --> 55:07.400
 That's compare. I don't know if it's going to make much difference or not.

55:07.400 --> 55:13.400
 Because we're pretty careful about fine tuning the batch norm layers.

55:13.400 --> 55:18.400
 It's actually interesting to see whether normalization matters as much as it used to.

55:18.400 --> 55:29.400
 It used to be absolutely critical.

55:29.400 --> 55:46.400
 Is it possible to create like a layer that learns the normalization sort of thing. Yeah, I mean that's basically what batch norm does, you know.

55:46.400 --> 56:03.400
 It's a those weights in the batch norm layer are basically learning the aggregate of that batch that optimally give the best activations for the next layer. Yeah, exactly. Yeah. Yeah, it's just, it's just, you know, multiply by something and add something.

56:03.400 --> 56:08.400
 So it's finding what's the best thing to multiply by and add by.

56:08.400 --> 56:13.400
 So, let's take a look. So I mean, all right, so this got.

56:13.400 --> 56:18.400
 It's got 44% error.

56:18.400 --> 56:30.400
 Yeah, so I mean, it's a bit disappointing after all that work. It doesn't actually, I mean, this is fascinating. Like, yeah, when you fine tune the way we do.

56:30.400 --> 56:36.400
 Basically doesn't really matter, you know.

56:36.400 --> 56:42.400
 And let's just double check it actually is.

56:42.400 --> 56:48.400
 It actually is working.

56:48.400 --> 56:55.400
 It would be fair to say that the one advantage would be if you wanted to use pre train models without fine tuning, you definitely want the statistics in there.

56:55.400 --> 56:59.400
 Yes, absolutely.

56:59.400 --> 57:07.400
 I mean, I don't know if that's an actual thing that people do, but yes, if you did.

57:07.400 --> 57:12.400
 All right, so we did deals.train.afterbatch.

57:12.400 --> 57:15.400
 Yep, there it is.

57:15.400 --> 57:17.400
 Groovy.

57:17.400 --> 57:25.400
 Yeah, it's funny these things that, you know, we've been doing for years and I guess never question.

57:25.400 --> 57:37.400
 I have a question relating to that because one of the things I wanted to do is get this unit into a mobile app. So I use the latest torch script and it works with the demo app. I took similar analogs is broken from PyTorch.

57:37.400 --> 57:43.400
 But of course, in there you need to provide the averaging statistics for the app. So it's like inference mode.

57:43.400 --> 57:54.400
 So I wonder, I know that at the moment, the fast AI is kind of idea is that you dump everything as like a pickle, but conceivably it would be helpful if you could maybe extract those new fine tunes.

57:54.400 --> 57:59.400
 Statistics or something for your deployment in particular environments.

57:59.400 --> 58:02.400
 Is that how would I go about doing that?

58:02.400 --> 58:07.400
 I mean, they're just parameters in batch, normally as you know, they're just parameters.

58:07.400 --> 58:10.400
 So they'll be in the parameters.

58:10.400 --> 58:15.400
 That tribute of the model.

58:15.400 --> 58:27.400
 But like they're not really parameters that make sense independently of all the other parameters at all. So I don't think you would treat them any differently.

58:27.400 --> 58:36.400
 If you use say image nets statistics when you're fine tuning and that's the result of your model, right? You're going to use that down the track as well.

58:36.400 --> 58:53.400
 Well, yes and no, like that's what you'd normalize with, but you've got batch norm layers, which then obviously dividing and subtracting themselves.

58:53.400 --> 59:05.400
 So yeah, I mean, you're those normalizations to that site going to change, but there isn't really any reason to, you know, it would only be if you

59:05.400 --> 59:10.400
 trained a new model from scratch.

59:10.400 --> 59:16.400
 I just want to have a look at this come the next one. So this is 27 to 18 24.

59:16.400 --> 59:24.400
 Yeah, this is actually kind of what I thought might happen is on a slightly better model. We may be getting slightly better errors initially.

59:24.400 --> 59:26.400
 And then as it trains a bit.

59:26.400 --> 59:34.400
 Thanks. No difference.

59:34.400 --> 59:38.400
 Cool.

59:38.400 --> 59:50.400
 All right, so, yeah, I'd love people to try out fast AI from master because,

59:50.400 --> 1:00:00.400
 tell me if any of your models look substantially better or even more important substantially worse.

1:00:00.400 --> 1:00:10.400
 So, I'm going to want to normalize.

1:00:10.400 --> 1:00:14.400
 Tim models.

1:00:14.400 --> 1:00:17.400
 Okay.

1:00:17.400 --> 1:00:39.400
 All right.

1:00:39.400 --> 1:00:47.400
 Anybody have any questions before we wrap it up.

1:00:47.400 --> 1:00:49.400
 Just with normalize.

1:00:49.400 --> 1:00:55.400
 It's just the initial, it will be a bit more less than early on approach.

1:00:55.400 --> 1:01:00.400
 Yeah, so like that, that, that, you know, well.

1:01:00.400 --> 1:01:02.400
 At first you have random head.

1:01:02.400 --> 1:01:08.400
 So at first it doesn't actually matter right it randoms random whether you normalize or not.

1:01:08.400 --> 1:01:13.400
 So,

1:01:13.400 --> 1:01:17.400
 maybe, you know, the after 10 batches.

1:01:17.400 --> 1:01:22.400
 It's better or something. But, yeah, I don't know.

1:01:22.400 --> 1:01:27.400
 It might be interesting to see if anybody notices a difference.

1:01:27.400 --> 1:01:33.400
 I mean, it's just, this used to matter a lot, right, for a couple of reasons. One is that.

1:01:33.400 --> 1:01:39.400
 Most people didn't fine tune models. Most people train most models and scratch until.

1:01:39.400 --> 1:01:42.400
 Until fast AI came along pretty much.

1:01:42.400 --> 1:01:45.400
 And then secondly,

1:01:45.400 --> 1:01:47.400
 well, we didn't have batch nom.

1:01:47.400 --> 1:01:50.400
 Right. So.

1:01:50.400 --> 1:01:56.400
 It was totally critical. And then even when batch nom came along, we didn't know how to fine tune models with batch nom.

1:01:56.400 --> 1:01:58.400
 So we just fine tuned the head.

1:01:58.400 --> 1:02:04.400
 At that point, we didn't realize that you had to fine tune the batch nom as well.

1:02:04.400 --> 1:02:07.400
 So I remember.

1:02:07.400 --> 1:02:16.400
 Emailing Francois, the creator of Keras. And I was saying to him, like, I'm trying to fine tune your Keras model. And it's like.

1:02:16.400 --> 1:02:20.400
 Bizarrely bad. Like, why, why is that?

1:02:20.400 --> 1:02:25.400
 Well, probably doing the wrong thing. His documentation, whatever. And like, no, I'm pretty sure I'm doing the right thing.

1:02:25.400 --> 1:02:33.400
 And I, yeah, I spent like three months trying to answer this question. Eventually, I realized it's like, holy shit, it's the batch normalize.

1:02:33.400 --> 1:02:36.400
 I sent him an email and said, Oh, we can't fine tune.

1:02:36.400 --> 1:02:40.400
 Keras models like this. You have to fine tune batch nom.

1:02:40.400 --> 1:02:46.400
 Which I don't think they changed for years. Actually.

1:02:46.400 --> 1:02:51.400
 Anyway, so those there, so those changes is why I guess.

1:02:51.400 --> 1:02:55.400
 This whole normalization layer thing is.

1:02:55.400 --> 1:03:01.400
 Much less interesting than I guess we thought, which is why we hadn't really noticed it wasn't working before.

1:03:01.400 --> 1:03:11.400
 Because I'm also training fine.

1:03:11.400 --> 1:03:19.400
 Anybody else have any questions before we wrap up.

1:03:19.400 --> 1:03:22.400
 Thank you.

1:03:22.400 --> 1:03:23.400
 See you.

1:03:23.400 --> 1:03:25.400
 Good luck with you.

1:03:25.400 --> 1:03:26.400
 Yeah.

1:03:26.400 --> 1:03:55.400
 Thank you.

