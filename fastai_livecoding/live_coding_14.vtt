WEBVTT

00:00.000 --> 00:03.200
 Peter, there we go. Yeah.

00:04.800 --> 00:06.240
 Another question. Yeah.

00:06.800 --> 00:15.840
 So Joe, the training sort of process in fast AI, like is there a concept or capability to do like early

00:15.840 --> 00:22.480
 stopping or best kind of thing? Or if there isn't, why is there a reason why you chose not to do that?

00:22.480 --> 00:31.520
 I never remember because I don't use it myself. So what I would check and just checking now is the callbacks,

00:32.640 --> 00:36.160
 which is under trainings. I'm just going to the docs training callbacks.

00:44.800 --> 00:46.960
 And if anybody else knows, please shout out.

00:46.960 --> 00:55.920
 There's a guy back. Early stopping callback. Yeah, I found it. Okay. It's under tracking callbacks.

00:55.920 --> 01:00.880
 So if you go to the docs training callbacks tracker, there's an early stopping callback.

01:03.200 --> 01:08.400
 So perhaps the more interesting part then is like, why do I not use it? So I don't even know

01:08.400 --> 01:20.080
 whether it exists. There's a few reasons. One is that it doesn't play nicely with

01:24.880 --> 01:32.480
 one cycle training or fine tuning. If you stop early, then the learning rate hasn't got a chance

01:32.480 --> 01:44.960
 to go down. And for that reason, it's almost never the case that earlier APOCs have better

01:46.000 --> 01:53.680
 accuracy because the set learning rate hasn't settled down yet. If I was doing one cycle training

01:53.680 --> 02:00.960
 and I saw that an earlier APOC had a much better accuracy, then I would know that

02:00.960 --> 02:07.360
 I'm overfitting, in which case I would be adding more data augmentation rather than

02:07.360 --> 02:12.800
 doing early stopping because it's good to train for the amount of time that you have.

02:14.160 --> 02:20.400
 So yeah, I can't think offhand of a situation where I haven't come across a situation where I've

02:20.400 --> 02:27.680
 personally wanted to use early stopping. So like in some of the training examples,

02:27.680 --> 02:34.160
 like where you had the error rate, but some of the prior runs may have had a better

02:35.680 --> 02:41.200
 lower error rate. Oh, I mean, in the ones I've shown like a tiny bit better, yeah, but like

02:42.160 --> 02:51.920
 not enough to be like meaningful, you know, and yeah, so that there's no reason to believe that

02:52.560 --> 02:56.160
 those are actually better models and there's plenty of very prior reason to believe that

02:56.160 --> 03:00.080
 they're actually not, which is that the learning rate still hasn't settled down at that point.

03:00.080 --> 03:07.760
 So we haven't let it fine tune into the best spot yet. So yeah, if it's kind of going down,

03:07.760 --> 03:12.720
 down and down, it's kind of bottoming out and just bumps a little bit at the bottom,

03:12.720 --> 03:14.560
 that's not a reason to use early stopping.

03:18.160 --> 03:23.760
 It's also, I think, important to realize that the validation sets

03:23.760 --> 03:30.000
 relatively small as well. So it's only a representation of, you know, of the distribution that the data is

03:30.000 --> 03:38.400
 coming from. So reading too much into those small fluctuations can be very counterproductive.

03:38.400 --> 03:42.480
 I know that I've wasted a lot of time in the past, you know, doing that, but

03:44.320 --> 03:49.760
 a lot of time. But yeah, we're looking for changes that dramatically improve things,

03:49.760 --> 03:55.520
 you know, like changing from ResNet 26 to ComfNEXT, we improved by what 400 or 500%.

03:56.160 --> 03:58.320
 And he's like, okay, that's an improvement.

04:01.200 --> 04:07.200
 Over the weekend, I went on my own server that I have here behind me,

04:07.200 --> 04:20.640
 that I have a 10, 10 ADI, and I run all like 35 models for the patty thing. And I was just,

04:21.200 --> 04:29.280
 I didn't do the example, but I was thinking about this that when I was taking algebra back in

04:29.280 --> 04:34.080
 in high school or college, you have some of these expressions that you have the function of

04:34.080 --> 04:43.280
 x is equal to x squared for the x greater than something. And the absolute value of x when you

04:43.280 --> 04:53.920
 have x equal to something. So it just got me my idea that the idea is that maybe some of the data set

04:54.960 --> 05:00.720
 is going to fail the target value for every single one of the models that we tried. But if we try

05:00.720 --> 05:08.880
 different models, it's going to be successful. So can we do that? I mean, of course we can,

05:08.880 --> 05:16.320
 but I mean, what would be the easiest approach to say for this validation when x is equal to this

05:16.320 --> 05:22.720
 or greater than that, this is the model to use. But then if this is the other model, this is

05:22.720 --> 05:31.920
 the way you have to use. Yeah, I mean, you could do that, right? And like a really simple way to do

05:31.920 --> 05:44.160
 that, which I've seen used for some success on Kaggle is to train lots of models. And then to

05:44.160 --> 05:52.160
 trade a gradient boosting machine, whose inputs are those model predictions that whose output is

05:52.160 --> 06:01.120
 the targets. And so that'll do exactly what you just described. It's very easy to overfit when you

06:01.120 --> 06:11.280
 do that. And you're only going to get it. If you've trained them well, you're only going to get a

06:11.280 --> 06:19.680
 tidy increase, right? Because because the neural nets are flexible, it shouldn't have that situation

06:19.680 --> 06:24.800
 where this part of the space, it has bad predictions. And this part of the space, it has good predictions.

06:24.800 --> 06:33.520
 Like it's, that's not really how neural nets work. If you had a variety of types of like

06:33.520 --> 06:37.920
 different, totally different types of model, like a random forest, energy, BAM, and a neural net,

06:38.880 --> 06:47.360
 I could see that maybe. But most of the time, one of those will be dramatically better than the

06:47.360 --> 06:54.000
 other ones. And so like I don't that often find myself wanting to ensemble across totally different

06:54.000 --> 06:59.920
 types of model. So, you know, I'd say it's another one of these things like early stopping, which like

07:02.720 --> 07:08.160
 a lot of people waste huge amounts of time on, you know, and it's not really where the big

07:09.120 --> 07:15.280
 benefits are going to be seen. But yeah, if you're like in gold metal zone on a Kaggle competition,

07:15.280 --> 07:22.720
 and you need another 0.002% or something, then these are all things you can certainly try at that point.

07:25.520 --> 07:31.440
 And I think you can remind me of auto and now, when you kind of write me of auto and now,

07:31.440 --> 07:36.800
 like the regime of tools. I don't know how you feel about how you feel about those things.

07:38.320 --> 07:44.000
 Yeah, last night, actually, so you'll have to catch up to see what I said. If you haven't seen

07:44.000 --> 07:53.280
 the lesson, yeah. I'll mention also reading Kaggle winners descriptions of their approaches is

07:53.280 --> 08:00.800
 great. But you've got to be very careful because remember, like Kaggle winners, the people who did

08:00.800 --> 08:07.920
 get that last 0.002%, you know, because like everybody found all the low hanging fruit and the

08:07.920 --> 08:13.520
 people who won grab the really high hanging fruit. And so every time you win a Kaggle winner's

08:13.520 --> 08:20.720
 description, they almost always have complex, ensembleing methods. And that's why, you know,

08:20.720 --> 08:25.680
 in like something like a big image recognition competition, it's very hard to win or probably

08:25.680 --> 08:31.440
 impossible to win with a single model, unless you invent some amazing new architecture or something.

08:31.440 --> 08:36.240
 And so you're kind of, you might get the impression then that

08:37.280 --> 08:41.600
 ensembleing is the big thing that gets you all the low hanging fruit, but it's not.

08:41.600 --> 08:46.400
 Ensembling is the thing which, or particularly complex, ensembleing is a thing that gets you

08:46.400 --> 08:49.360
 that last fraction of a fraction of a percent.

08:49.360 --> 08:56.480
 Yeah, of course.

08:56.480 --> 08:57.280
 Yeah, of course.

08:57.280 --> 09:00.400
 Yeah, TTA will concept, right?

09:00.400 --> 09:01.760
 So I'm trying to...

09:01.760 --> 09:01.760
 I mean, TTA?

09:01.760 --> 09:11.680
 TTA, sorry. Yeah, TTA. Yeah, no time to do. Yeah, so if I understand, like I'm trying to understand

09:11.680 --> 09:20.640
 conceptually why TTL improves the score, because technically when you're training, it is using

09:20.640 --> 09:26.800
 those augmented sort of pictures and providing them, providing a percentage number. But when you're

09:28.640 --> 09:33.600
 when you run that TTA function, why is it able to predict better?

09:33.600 --> 09:42.400
 Sure. So like, you know how sometimes you're like, are looking at some like, I don't know,

09:43.760 --> 09:48.560
 a sprue head or a pla or a socket or something that's really small,

09:48.560 --> 09:53.840
 that you can't quite see like what, how many pins are in it or what type is it or whatever.

09:53.840 --> 09:56.640
 And you're kind of like looking at it from different angles, and you're kind of like

09:56.640 --> 10:01.120
 put it up to the light and you try to like, and at some point you're like, okay, I see it,

10:01.120 --> 10:08.640
 right? And there's like some angle and some lighting that you can see it.

10:09.520 --> 10:13.120
 That's what you're doing for the computer. You're giving it different angles and you're giving

10:13.120 --> 10:17.760
 it different lighting in the hope that in one of those it's going to be really clear.

10:19.600 --> 10:24.160
 And for the ones where it's easy, it's not going to make any difference, right? But for the ones

10:24.160 --> 10:29.440
 where it's like, oh, I don't know if it's this disease or that disease, but oh, you know, when

10:29.440 --> 10:34.560
 it's a bit brighter and you kind of zoom into that section like, oh, now I can see. And so

10:35.280 --> 10:39.680
 when you then average them out, you know, or the other ones are all like, oh, I don't know which

10:39.680 --> 10:45.200
 kind of is, which kinds of it's like 0.5, 0.5, and then this one is like 0.6. And so that's the one

10:45.200 --> 10:49.600
 that in the average it's going to end up picking. That's basically what happens.

10:49.600 --> 10:57.280
 It also has another benefit, which is

11:01.200 --> 11:07.040
 when we train our models, I don't know if you've noticed, but our training loss generally gets

11:07.040 --> 11:14.960
 much lower than our validation loss. And sometimes our validate sometimes

11:14.960 --> 11:27.920
 our, well, so basically like what's happening there is that on the training set, the model is

11:27.920 --> 11:32.160
 getting very confident, right? So even though we're using data augmentation, it's seeing

11:32.160 --> 11:36.880
 slightly different versions of the same image dozens of times. And it's like, oh, I know how

11:36.880 --> 11:45.040
 to recognize these. And so what it does is that the probabilities that associates with them is like

11:45.040 --> 11:51.840
 0.9, 0.99, you know, like it's like, I'm very confident of these. And it actually gets overconfident,

11:53.840 --> 12:01.600
 which actually doesn't necessarily impact our accuracy, you know, to be overconfident.

12:01.600 --> 12:12.960
 But at some point it can. And so we are systematically going to have like

12:12.960 --> 12:18.880
 overconfident predictions of probability. Even when it doesn't really know,

12:19.760 --> 12:23.440
 just because it's really seen that kind of image before. So then on the validation set,

12:23.440 --> 12:31.920
 it's going to be, you know, over picking probabilities as well. And so one nice benefit is that when

12:31.920 --> 12:40.400
 you average out a few augmented versions, you know, it's like, oh, 0.9, 0.9 probability is this one.

12:40.400 --> 12:44.080
 And then on the next, it's like, next, or bent version with the same image, it's like, oh, no, 0.1

12:44.080 --> 12:53.840
 probability is that one. And they'll kind of average out to much more reasonable probabilities,

12:53.840 --> 13:03.200
 which can, you know, allow it sometimes to, yeah, combine these ideas into an average that

13:04.160 --> 13:11.200
 that makes more sense. And so that can improve accuracy, but in particular, it improves the actual

13:11.200 --> 13:14.160
 probabilities to get rid of that overconfidence.

13:16.640 --> 13:23.920
 It's a fair to say that when you train without, when you train, it's not able to separate the

13:23.920 --> 13:30.160
 the replicated sort of images or the distorted slightly the variant of the original image. But

13:30.160 --> 13:35.360
 when you use the TTA, it is able to group all the four images and it.

13:35.360 --> 13:42.080
 Well, wait, that's what TTA is. We present them all together and average out that group. Yes.

13:42.080 --> 13:46.880
 But in training, we don't indicate in any way that they're the same image,

13:46.880 --> 13:48.560
 or that they're the same underlying object.

13:54.320 --> 14:00.800
 One other question, Jeremy, we, I'm going to be touching and stumbling and how to pick the best

14:00.800 --> 14:08.800
 and stumbling I was going to ask about that. But another question is, we have a fairly

14:08.800 --> 14:14.000
 unbalanced data set, I guess, with the normal versus the disease states.

14:14.000 --> 14:14.560
 Yeah.

14:14.560 --> 14:20.160
 You're doing augmentation. Is there any benefit to sort of over representing the minority classes?

14:21.760 --> 14:25.840
 So let's pull away augmentation. So it's actually got nothing to do with augmentation.

14:25.840 --> 14:30.720
 So more generally, when you're training, does it make sense to over represent the minority class?

14:33.360 --> 14:43.840
 And the answer is maybe. Yeah, it can. Right. And so, okay, so just for those who aren't

14:43.840 --> 14:49.520
 following the issue, Matt's talking about is that there was, you know, a couple of diseases,

14:49.520 --> 14:59.120
 which appear lots and lots in the data, and a couple which hardly appear at all. And so, you know,

14:59.120 --> 15:04.640
 do we want to try to balance this out more? And one thing that people often do to balance it out

15:04.640 --> 15:11.680
 more is that they'll throw away some of the images in their more represent, highly represented classes.

15:12.960 --> 15:16.720
 And I can certainly tell you straight away, you should never ever do that. You never want to throw

15:16.720 --> 15:28.400
 away data. But Matt's question was, well, could we, you know, over sample the less common diseases?

15:29.600 --> 15:36.320
 And the answer is, yeah, absolutely you could. And in first AI, if you go into the docs,

15:36.320 --> 15:52.960
 yeah, where is it? There is a weighted data loader somewhere. Weighted. Search for that. Here we go.

15:53.520 --> 15:58.960
 Of course, it's a callback. So if you go to the callbacks data section, you'll find a weighted DL

15:58.960 --> 16:06.960
 callback or a weighted data loader's method. Are you sharing your screen?

16:06.960 --> 16:14.480
 I'm not. No, I'm just telling you where to look. Thanks for checking. So,

16:17.440 --> 16:23.840
 yeah, I mean, let's look at that today, right? Because I kind of want to look at like things we

16:23.840 --> 16:32.000
 can do to improve things today. It doesn't necessarily help. Because it does mean,

16:33.280 --> 16:38.000
 you know, given that you're, you know, let's say you do 10 epochs of 1000 images,

16:38.640 --> 16:42.640
 it's going to get to look at 10,000 images, right? And if you over sample a class,

16:43.440 --> 16:49.120
 then that also means that it's going to get it's going to see less of some images and going to get

16:49.120 --> 16:56.240
 more repetition of other images, which could be a problem, you know, and really it just depends on

16:58.000 --> 17:04.800
 depends on a few things. If it's like really unbalanced, like 99% all of one type, then you're

17:04.800 --> 17:09.680
 going to have a whole lot of batches that it never sees anything of the underrepresented class. And

17:09.680 --> 17:14.080
 that's so basically there's nothing for it to learn from. So at some point, you probably certainly need

17:14.080 --> 17:21.600
 weighted sampling. It also depends on the evaluation. You know, if people like say in the

17:21.600 --> 17:27.440
 evaluation, okay, we're going to kind of average out for each disease, how accurate you were.

17:27.440 --> 17:31.440
 So every disease will then be like equally weighted. Then you would definitely need to use weighted

17:31.440 --> 17:38.000
 sampling. But in this case, you know, presuming, presuming that the test set

17:38.000 --> 17:43.920
 has a similar distribution as a training set, weighted sampling might not help.

17:45.840 --> 17:50.640
 Because they're going to care the most about how well we do on the highly represented diseases.

17:56.480 --> 18:03.200
 I'll note my experience with like oversampling and things like that. I think one time I had done

18:03.200 --> 18:10.640
 with I think diabetic retinopathy, there was a competition for that. And I had used weighted sampling or

18:10.640 --> 18:15.760
 oversampling and it did seem to help. And then also a while back, I did an experiment where

18:17.040 --> 18:22.560
 I think this was back with Fassai version one, where I took like the minced data set and then I

18:24.160 --> 18:32.560
 I like artificially added some sort of imbalance. And then I trained with and without weighted

18:32.560 --> 18:38.640
 sampling. And I saw like there was an improvement with the weighted sampling on accuracy on like

18:38.640 --> 18:44.640
 just a regular means the validation set. So I so from that, from those couple experiments,

18:44.640 --> 18:49.520
 I'd say like, I've at least seen some help and improvement with weighted sampling.

18:50.080 --> 18:55.600
 Cool. And was that cases where the data set was like highly unbalanced or was it more like

18:55.600 --> 18:57.920
 the data set that we're looking at at the moment?

18:57.920 --> 19:06.160
 It wasn't highly unbalanced. It was maybe like, I don't know, like maybe like, yeah, just 75%

19:06.160 --> 19:12.720
 versus 25% or something like that. It's not like 99 versus 1%, nothing like that. It was more

19:13.520 --> 19:18.480
 well, which isn't that bad. Let's go. Let's try it today. Yeah. I see we've got a new face today as

19:18.480 --> 19:25.840
 well. Hello, Zach. Thanks for joining. Hey, hey, glad I could finally make these. Yeah.

19:25.840 --> 19:32.320
 Are you joining from Florida? No, I'm in Maryland now. Maryland now.

19:32.320 --> 19:36.400
 Oh, I have my own place. That's quite a change. Yes. Much more up north.

19:36.400 --> 19:51.360
 Okay, great. So let's try something.

19:59.280 --> 20:00.800
 Okay, so let's connect.

20:00.800 --> 20:09.040
 Right. To my little computer upstairs.

20:19.920 --> 20:23.840
 There are a way to shrink my zoom out of the way. It takes up so much space.

20:23.840 --> 20:30.000
 Hide floating meeting controls. I guess that's what I want. Control Alt Shift H.

20:31.360 --> 20:37.760
 Well, press Escape to show floating meeting controls. That doesn't work very well with VIM.

20:39.360 --> 20:42.240
 Oh, well, control Alt Shift H. Okay.

20:42.240 --> 20:50.480
 All right, we're not doing tabular today, so let's get rid of that.

20:58.480 --> 21:02.160
 So I think what I might do is, you know, because we're iterating,

21:02.160 --> 21:13.040
 well, I guess we could start with the multi task one. This is our kind of like

21:15.840 --> 21:20.240
 thing is to try to improve version.

21:20.240 --> 21:36.160
 So close that. I'll leave that open just in case we want it. Okay.

21:39.920 --> 21:43.680
 By the way, if you've got multiple GPUs, this is how you just use one of them.

21:43.680 --> 21:50.800
 You can just set an environment variable.

21:55.840 --> 21:57.040
 Okay, so this is where we,

21:57.040 --> 22:09.200
 this is where we did the multi target model.

22:09.200 --> 22:15.040
 Okay.

22:22.160 --> 22:28.400
 Okay, just moved everything slightly.

22:32.000 --> 22:32.320
 Comp.

22:32.320 --> 22:36.080
 Comp.

22:42.160 --> 22:43.200
 Not comp pass.

22:45.600 --> 22:48.000
 Right, that's where we were. Okay.

22:51.600 --> 22:51.840
 So,

22:51.840 --> 22:58.320
 oh, now what?

23:01.200 --> 23:02.000
 Let's break it in.

23:06.320 --> 23:07.040
 Data block.

23:07.040 --> 23:18.960
 Get image files.

23:22.960 --> 23:24.400
 Well, this is working the other day.

23:28.080 --> 23:30.080
 So I guess we better try to do some

23:32.240 --> 23:32.800
 debugging.

23:32.800 --> 23:38.160
 So the obvious thing to do would be to call this thing here,

23:38.160 --> 23:43.680
 get image files on the thing that we passed in here, which is train pass.

23:45.680 --> 23:47.760
 Okay, so that's working.

23:48.320 --> 23:54.240
 Then the other thing to do would be to check our data by doing show batch.

23:54.240 --> 24:00.560
 Okay, that's working.

24:00.560 --> 24:07.520
 Then I guess, all right, and it's showing our two different things.

24:07.520 --> 24:08.080
 That's good.

24:12.560 --> 24:13.840
 Oh, is it?

24:13.840 --> 24:23.040
 We've got the two category blocks, so we can't use this one.

24:26.080 --> 24:27.120
 We have to use this one.

24:33.040 --> 24:34.640
 So fit one cycle.

24:34.640 --> 24:40.720
 Yeah, okay.

24:42.960 --> 24:46.720
 So to remind you, we have, this is the one where we had two

24:50.720 --> 24:54.400
 categories and one input.

24:55.680 --> 25:00.320
 And to get the two categories, we used the parent label and this function,

25:00.320 --> 25:05.440
 which looked up the variety from this dictionary.

25:10.400 --> 25:13.040
 Okay, and then when we fine tuned it,

25:16.560 --> 25:18.800
 and let's just check yeah, c equals 42.

25:18.800 --> 25:21.520
 So that's our standard set.

25:21.520 --> 25:30.640
 We should be able to then compare that to small models, train for 12 epochs.

25:36.000 --> 25:37.440
 And then that was this one.

25:37.440 --> 25:45.680
 Part two.

25:57.360 --> 26:01.520
 And let's see.

26:01.520 --> 26:14.320
 They're not quite the same because this was 480 squish.

26:17.440 --> 26:21.840
 Or else this was rectangular pad.

26:21.840 --> 26:31.200
 Let's do five epochs.

26:31.200 --> 26:37.200
 Let's do it the same as this one.

26:41.920 --> 26:45.120
 Yeah, let's do this one because we want to be able to do quick iterations.

26:45.120 --> 26:55.760
 Let's see, resize 192 squish.

26:55.760 --> 27:09.360
 There we go.

27:09.360 --> 27:19.520
 And then we trained it for

27:19.520 --> 27:34.800
.01 with FP 16 with five epochs.

27:38.480 --> 27:38.880
 All right.

27:41.200 --> 27:42.480
 So this would be our base case.

27:42.480 --> 27:49.840
 This is our base case 0.045.

27:51.120 --> 27:52.400
 This will be our next case.

27:56.720 --> 28:03.040
 Okay, so while that's running, the next thing I wanted to talk about is

28:03.040 --> 28:12.480
.progressive resizing.

28:12.880 --> 28:24.560
 So this is training at a size of 128, which is

28:24.560 --> 28:33.840
 not very big.

28:33.840 --> 28:35.360
 And we wouldn't expect it to do very well.

28:36.880 --> 28:40.560
 So, but it's certainly better than nothing.

28:40.560 --> 28:43.920
 And as you can see, it's not error.

28:43.920 --> 28:44.640
 Disease error.

28:44.640 --> 28:47.840
 It's down to 7.5% error already, and it's not even done.

28:47.840 --> 28:51.920
 So that's not bad.

28:51.920 --> 28:58.960
 And in the past, what we've done is we've said, okay, well, that's working pretty well.

29:00.240 --> 29:05.920
 Let's throw that away and try bigger.

29:08.960 --> 29:12.560
 But there's actually something more interesting we can do,

29:12.560 --> 29:16.000
 which is we don't have to throw it away.

29:17.200 --> 29:26.960
 What we could do is to continue training it on larger images.

29:26.960 --> 29:33.120
 So we're basically saying, okay, this is a model which is fine tuned to recognize 128 by 128 pixel images

29:35.680 --> 29:37.520
 of rice.

29:37.520 --> 29:44.560
 That's fine tuned to recognize 192 by 192 pixel images of rice.

29:45.680 --> 29:49.680
 And we could even like, there's a few benefits to that.

29:49.680 --> 29:54.320
 One is it's very fast to do the smaller images.

29:55.680 --> 29:57.680
 And it can recognize the key features of it.

29:57.680 --> 30:05.760
 So this lets us do a lot of epochs quickly.

30:06.640 --> 30:11.040
 And then the difference between small images of rice disease and large images of rice disease

30:11.040 --> 30:12.000
 isn't very big difference.

30:13.040 --> 30:17.280
 So you would expect to probably fine tune to bigger images of rice disease quite easily.

30:18.320 --> 30:24.560
 So we might get most of the benefit of training on big images, but without most of the time.

30:24.560 --> 30:31.280
 The second benefit is it's a kind of data augmentation, which is we're actually giving it

30:32.160 --> 30:33.280
 different sized images.

30:34.320 --> 30:36.000
 So that should help.

30:37.040 --> 30:38.560
 So here's how we would do that.

30:39.840 --> 30:43.440
 Let's grab this data block.

30:43.440 --> 30:45.520
 Let's make it into a function.

30:48.080 --> 30:48.720
 Get DL.

30:49.520 --> 30:52.080
 Okay, and the key thing I guess we're going to do.

30:52.080 --> 30:56.400
 Well, let's just do the item transforms and the batch transforms as usual.

30:57.920 --> 30:58.320
 Oops.

31:07.280 --> 31:09.840
 So the things we're going to change are the item transforms.

31:16.960 --> 31:18.160
 And the batch transforms.

31:18.160 --> 31:27.520
 And then we're going to return the data loader for that, which is here.

31:35.440 --> 31:35.680
 Okay.

31:35.680 --> 31:42.800
 Okay, so let's try

31:52.400 --> 31:53.200
 going up a bit.

31:57.360 --> 31:58.720
 DL equals

31:58.720 --> 32:03.120
 get DL.

32:04.400 --> 32:08.400
 I guess it should be get DLs really, because it returns data loaders get DLs.

32:13.920 --> 32:14.800
 Okay, so let's

32:19.120 --> 32:20.480
 see what we did last time.

32:20.480 --> 32:27.360
 Let's be scaled up a bit.

32:30.880 --> 32:32.560
 So this is going to be data augmentation as well.

32:32.560 --> 32:34.000
 We're going to change how we scale.

32:36.400 --> 32:39.440
 So we'll scale with zero padding.

32:42.160 --> 32:44.800
 And that's got to 160.

32:44.800 --> 32:47.920
 Okay.

32:47.920 --> 33:02.880
 Okay.

33:31.840 --> 33:36.880
 So our, where's our squish one here?

33:36.880 --> 33:45.760
 So the squish here got.45.

33:45.760 --> 33:47.920
 Our multitask got.48.

33:47.920 --> 33:49.040
 So it's actually a little bit worse.

33:53.040 --> 33:57.600
 This might not be a great test actually, because I feel like one of the reasons that doing a

33:57.600 --> 34:02.080
 multitask model might be useful, because it might be able to train for more epochs.

34:02.080 --> 34:07.120
 Because we're kind of giving it more signal.

34:07.120 --> 34:12.080
 So we should probably revisit this with like 20 epochs.

34:12.080 --> 34:22.080
 Any questions or comments about progressive resizing while we wait for this to train?

34:22.080 --> 34:30.080
 Sorry, I can't see how you progress with the change.

34:30.080 --> 34:34.080
 I messed it up.

34:34.080 --> 34:38.080
 Whoops.

34:38.080 --> 34:40.080
 Thank you.

34:40.080 --> 34:41.120
 I have to do that again.

34:44.160 --> 34:45.120
 I actually did it.

34:45.120 --> 34:47.360
 Oh, and we need to get our deals back as well.

34:47.360 --> 34:47.760
 Okay.

34:47.760 --> 34:48.480
 Let's start again.

34:52.000 --> 34:52.320
 Okay.

34:53.280 --> 34:56.240
 And let's, in case I mess this up again, let's export this.

34:56.240 --> 34:59.680
 I'll call this like stage one.

34:59.680 --> 34:59.840
 See.

35:05.520 --> 35:07.440
 Yeah, the problem was we created a new learner.

35:08.320 --> 35:18.000
 So what we should have done is gone learn.thels equals deals.

35:18.000 --> 35:27.200
 That's actually, so that would actually change the data loaders inside the learner without recreating it.

35:28.480 --> 35:30.400
 Was that where you were heading with your comment?

35:33.440 --> 35:35.520
 Give me the first unfreeze method, right?

35:35.520 --> 35:37.040
 Like, I'm going to be using that.

35:37.040 --> 35:37.760
 Sorry.

35:38.960 --> 35:43.120
 There was an unfreeze method like in the same thing in the book actually mentioned,

35:43.120 --> 35:44.880
 it's using the unfreeze method.

35:44.880 --> 35:48.400
 So, there is an unfreeze method.

35:48.400 --> 35:48.800
 Yes.

35:48.800 --> 35:50.320
 What were you saying about the unfreeze method?

35:51.360 --> 35:53.920
 Isn't an unfreeze required for progress you're resizing?

35:53.920 --> 35:54.960
 Am I wrong?

35:54.960 --> 35:58.880
 No, because fine tune has already unfrozen.

35:59.440 --> 36:02.240
 Although I actually want to fine tune again.

36:04.080 --> 36:05.840
 So if anything, I kind of actually want to,

36:05.840 --> 36:08.400
 actually want to rephrase it.

36:11.920 --> 36:13.200
 Because we've changed the

36:13.200 --> 36:21.520
 resolution, I think fine tuning the head might be a good idea to do again.

36:23.920 --> 36:29.920
 Which line of code is doing the regressive resizing part?

36:29.920 --> 36:30.720
 Just to be clear.

36:31.360 --> 36:32.720
 It's not online of code.

36:32.720 --> 36:34.560
 It's basically this.

36:34.560 --> 36:38.000
 It's basically saying our current learner is getting new data loaders.

36:38.000 --> 36:42.720
 And the new data loaders have a size of 160,

36:42.720 --> 36:47.040
 or else the old data loaders had a size of 128.

36:48.320 --> 36:51.840
 And the old data loaders did a pre sizing of 192's quish,

36:51.840 --> 36:55.920
 and our newly data loaders are doing a pre sizing of rectangular padding.

36:57.200 --> 36:57.840
 Does that make sense?

36:57.840 --> 37:00.720
 Why are you calling it progressive in this case?

37:01.600 --> 37:04.960
 You're going to keep changing the size or something like that.

37:04.960 --> 37:10.320
 Yeah, it's changing the size of the images without resetting the learner.

37:13.840 --> 37:15.280
 Just looked it up because I was curious.

37:15.280 --> 37:16.880
 Fine tune calls a freeze first.

37:17.600 --> 37:18.480
 I had a feeling it did.

37:18.480 --> 37:19.200
 For that first set.

37:19.760 --> 37:20.720
 Thanks for checking, Zach.

37:25.920 --> 37:30.080
 So this time, let's see, it'll be interesting to see how it does.

37:30.080 --> 37:36.400
 So after the initial epoch, it's got 0.09.

37:37.680 --> 37:39.600
 Realize previously it had 0.27.

37:39.600 --> 37:44.160
 So obviously it's better than last time, but it's actually worse than the final point.

37:45.040 --> 37:47.840
 This time it got all the way to 0.418.

37:48.720 --> 37:51.200
 Whereas this time it has got worse.

37:51.200 --> 38:00.800
 So it's got some work to do to learn to recognize what 160 pixel images look like.

38:00.800 --> 38:02.640
 Can I just clarify Jeremy?

38:04.000 --> 38:08.560
 You're doing one more step in the progressive resizing here.

38:08.560 --> 38:10.880
 It's not an automated resizing.

38:11.440 --> 38:12.240
 Correct.

38:12.240 --> 38:13.040
 Correct.

38:13.040 --> 38:13.200
 Yeah.

38:14.160 --> 38:16.800
 There isn't anything in Fast.io to do this for you.

38:16.800 --> 38:24.800
 And in fact, this technique is something that we invented so it doesn't exist in other

38:24.800 --> 38:26.800
 bibaries at all.

38:27.440 --> 38:30.000
 So yeah, it's the name of a technique.

38:30.000 --> 38:33.200
 It's not the name of like a method in Fast.io.

38:34.080 --> 38:37.680
 And yeah, the technique is basically to replace the data loaders with one's

38:38.880 --> 38:39.920
 analog size.

38:41.760 --> 38:46.160
 And we invented it as part of a competition called Dawnbench,

38:46.160 --> 38:52.880
 which is where we work very well on a competition for ImageNet training.

38:54.080 --> 39:01.840
 And Googled and talked the idea and studied it a lot further as part of a paper called

39:01.840 --> 39:07.760
 EfficientNet V2 and found ways to make it work even better.

39:09.040 --> 39:10.240
 Oh my gosh, look at this.

39:10.240 --> 39:15.840
 So we've gone from 0.418 to 0.0336.

39:18.320 --> 39:21.600
 Have we done training at 160 before?

39:22.720 --> 39:24.080
 I don't think we have.

39:28.400 --> 39:29.520
 Oh, I should be checking this one.

39:31.040 --> 39:32.720
 128, 128.

39:32.720 --> 39:39.840
 128, 171 by 128.

39:46.640 --> 39:48.000
 No, we haven't.

39:49.600 --> 39:52.320
 This is a 256 by 192.

39:52.320 --> 39:54.080
 So eventually I guess we're going to get to that point.

39:55.840 --> 39:58.560
 So let's keep going.

39:58.560 --> 40:02.720
 So we're down a 2.9% error.

40:04.240 --> 40:05.840
 How did you come up with that idea for this?

40:05.840 --> 40:09.280
 Is this something that you just wanted to try or did it happen?

40:09.280 --> 40:12.320
 Did you stumble upon it while looking at something else?

40:12.320 --> 40:17.520
 Oh, I mean, it just seemed very obviously to me like something which obviously we should do

40:17.520 --> 40:21.920
 because like we were spending, okay, so on Dawnbench, we were training on ImageNet.

40:21.920 --> 40:26.480
 It was taking 12 hours, I guess, to train a single model.

40:26.480 --> 40:36.000
 And the vast majority of that time, it's just recognizing very, very basic things about images.

40:38.000 --> 40:41.040
 It's not learning the finer details of different cat breeds or whatever,

40:41.040 --> 40:44.800
 but it's just trying to understand about the concepts of like fur or sky or metal.

40:45.760 --> 40:50.400
 I thought, well, there's absolutely no reason to need 224 by 224 pixel images

40:51.520 --> 40:52.560
 to be able to do that.

40:52.560 --> 40:59.440
 It just seemed obviously stupid that we would do it.

41:00.560 --> 41:03.760
 And partly it was like also like I was just generally interested in

41:05.680 --> 41:07.120
 changing things during training.

41:08.080 --> 41:10.480
 So one of the particular learning rates, right?

41:11.520 --> 41:17.440
 So the idea of changing learning rates during training goes back a lot longer than Dawnbench.

41:17.440 --> 41:22.000
 That people had been generally training them by having a learning rate that kind of dropped

41:22.000 --> 41:24.720
 by a lot from then stayed flat and dropped by a lot of stayed flat.

41:25.360 --> 41:31.520
 And Leslie Smith in particular came up with this idea of gradually increasing it over a curve

41:31.520 --> 41:34.000
 and then gradually decreasing it following another curve.

41:34.720 --> 41:38.240
 And so I was definitely in the mindset of like, oh, there's kind of interesting things we can

41:38.240 --> 41:39.200
 change during training.

41:39.200 --> 41:44.720
 So I was looking at like, oh, what if we change data augmentation during training, for example,

41:44.720 --> 41:50.320
 like maybe towards the end of training, we should turn off data augmentation so it could learn what

41:50.320 --> 41:54.720
 unalgmented images look like because that's what we really care about, for example.

41:58.240 --> 42:03.520
 So yeah, that was the kind of stuff that I was kind of interested in at the time.

42:03.520 --> 42:05.360
 And so yeah, definitely this thing of like,

42:08.320 --> 42:15.680
 you know, why are we looking over 224 by 224 pixel images the entire time?

42:15.680 --> 42:17.680
 Like that just seemed obviously stupid.

42:17.680 --> 42:20.080
 And so it wasn't something where I was like, wow, here's a crazy idea.

42:20.080 --> 42:21.200
 I bet it won't work.

42:21.200 --> 42:24.480
 As soon as I thought of it, I just thought, okay, that this is definitely going to work.

42:24.480 --> 42:26.240
 You know, that did.

42:28.000 --> 42:28.960
 Interesting. Thanks.

42:28.960 --> 42:29.760
 Yeah, I'm worries.

42:32.160 --> 42:34.000
 One question that I have for you, Jeremy.

42:35.360 --> 42:40.960
 There was a paper that came out like in 2019 called Fixing the Test Train Resolution Discrepancy.

42:41.520 --> 42:42.400
 Yeah, I've very clear.

42:42.400 --> 42:50.080
 Yeah, were they like trained on 224 and then did inference finally on like 320 by 320?

42:50.080 --> 42:50.240
 Yeah.

42:51.840 --> 42:54.400
 Have you seen that still sort of work?

42:54.400 --> 42:57.200
 Have you done that at all in your workflows?

42:57.200 --> 42:58.800
 I mean, honestly, I don't remember.

42:58.800 --> 43:01.040
 I need to revisit that paper because you're right.

43:01.040 --> 43:02.080
 It's important tonight.

43:04.720 --> 43:05.440
 I,

43:05.440 --> 43:07.440
 I'm going to be

43:11.360 --> 43:15.920
 You know, I would generally try to fine tune on the final size.

43:15.920 --> 43:18.640
 I was going to be predicting on anyway.

43:21.360 --> 43:24.720
 So yeah, I guess we'll kind of see how we go with this, right?

43:24.720 --> 43:29.520
 I mean, you can definitely take a model that was trained on 224 by 224 images

43:30.960 --> 43:34.320
 and use it to predict 360 by 360 images.

43:34.320 --> 43:36.400
 And it will generally go pretty well.

43:37.520 --> 43:41.440
 But I think it'll go better if you first fine tune it on 360 by 360 images.

43:42.960 --> 43:49.040
 Yeah, I don't think they tried pre training and then also training them like 320 versus just

43:49.040 --> 43:50.400
 320 and the 224.

43:50.800 --> 43:51.040
 Yeah.

43:51.040 --> 43:53.280
 So that would definitely be an interesting experiment.

43:53.280 --> 43:54.720
 Yeah, it would be an interesting experiment.

43:54.720 --> 43:59.440
 And it's definitely something that any of us here could do, you know, I think it'd be cool.

44:00.560 --> 44:01.120
 Right.

44:01.120 --> 44:02.960
 So let's try scaling this up.

44:02.960 --> 44:07.200
 So we can change these two lines to one.

44:07.200 --> 44:12.960
 And so this is one something I often do is I do things like, yeah, I think we don't have yours.

44:12.960 --> 44:13.760
 Okay.

44:14.640 --> 44:14.800
 So.

44:18.560 --> 44:22.320
 I was just saying previously I had like two cells to do this.

44:22.320 --> 44:24.240
 And so now I'm just going to combine it into one cell.

44:24.240 --> 44:34.240
 So this is what I tend to do as I fiddle around as I title like gradually make things a little bit more concise, you know.

44:40.400 --> 44:40.960
 Okay.

44:46.400 --> 44:49.280
 Does it make sense to go smaller than the

44:49.280 --> 44:56.000
 by the original pre training like,

44:56.000 --> 44:56.560
 Convinate.

44:57.760 --> 44:58.720
 Cock Comve next.

45:00.400 --> 45:02.720
 Yeah, I mean, you can fine tune to any size you like.

45:04.640 --> 45:05.280
 Absolutely.

45:06.240 --> 45:11.520
 I'm just going to get rid of the zero padding because again, I want to like try to change it a little bit each time just to kind of.

45:13.440 --> 45:15.040
 You know, it's a kind of augmentation, right?

45:15.040 --> 45:15.680
 Okay.

45:17.680 --> 45:19.680
 So, okay. So it's got to 192.

45:27.920 --> 45:34.400
 You know, one thing I find encouraging is that, you know, my training loss isn't getting way underneath the validation loss.

45:34.400 --> 45:35.040
 It's not like we're.

45:35.040 --> 45:43.920
 Yeah, it feels like we could do this for ages before our error rates start going up.

45:43.920 --> 46:01.280
 Interestingly, when I reran this, my error rate was much better point 418.

46:01.280 --> 46:09.280
 You've got a good memory to remember these these old papers, it's very helpful to be able to do that.

46:09.280 --> 46:15.280
 Usually what I wind up doing is my dad and I will email back and forth papers to each other.

46:15.280 --> 46:29.280
 So I can just go through my scent, look at archive and usually if I don't remember the name of it, I remember the subject of it in some degree.

46:29.280 --> 46:31.280
 Yeah, I can just go through it all.

46:31.280 --> 46:47.280
 I mean, it's a very, very good idea to use a paper manager of some sort to save papers, you know, whether it be Mendelay or the Nodo or archive sanity or whatever or bookmarks or something.

46:51.280 --> 46:53.280
 Yeah, because otherwise these things disappear.

46:53.280 --> 46:59.280
 Personally, I just tend to like.

46:59.280 --> 47:03.280
 Tweet or favorite tweets about papers I'm interested in.

47:03.280 --> 47:15.280
 And then I've set up pin board.in. I don't know if you guys have seen that, but it's really nice little thing, which basically any time you're on a website.

47:15.280 --> 47:34.280
 You can click a button and the extension and it adds it to pin board, but it also automatically adds all of your tweets and favorites and it's got a full text search of the thing that the URL is linked to, which is very helpful.

47:34.280 --> 47:37.280
 See you favorite it's something that just says, Oh, shit.

47:37.280 --> 47:40.280
 No, I actually wrote something that just said, Oh, shit.

47:40.280 --> 47:41.280
 That was me.

47:41.280 --> 47:50.280
 I think it was this. I mean, totally off topic, but this absolutely disaster. I hope it's wrong.

47:50.280 --> 47:54.280
 But it's absolutely disastrous sounding.

47:54.280 --> 47:57.280
 Paper that came out yesterday.

47:57.280 --> 48:02.280
 That basically, where was this key thing?

48:02.280 --> 48:19.280
 People who've had one covered infection have a list of one sequel, 8.4% to infections 23% three infections 36%. It's like my worst nightmare is like the more people get infected with covert, the more likely it is that they'll get long term.

48:19.280 --> 48:37.280
 Simptance, which is horrifying. That was my post sheet. That's very horrifying. It's really awful. Okay, so keeps going down right which is cool. Let's keep bringing along as pose.

48:37.280 --> 48:44.280
 I guess, you know what we could do is just grab this whole damn thing here.

48:44.280 --> 48:49.280
 We kind of have a bit of a comparison. So we're basically going to run.

48:49.280 --> 48:53.280
 Exactly the same thing we did earlier.

48:53.280 --> 48:55.280
 But this time.

48:55.280 --> 49:05.280
 With some pre sizing first.

49:05.280 --> 49:28.280
 All right.

49:28.280 --> 49:31.280
 So that'll be an interesting experiment.

49:31.280 --> 49:35.280
 So while that's running.

49:35.280 --> 49:42.280
 This is where I hit the old duplicate button.

49:42.280 --> 49:55.280
 And this is why it's nice if you can to have the second card, because while something's running you can try something else.

49:55.280 --> 50:03.280
 And visible devices.

50:03.280 --> 50:05.280
 There we go.

50:05.280 --> 50:16.280
 So we can keep working.

50:16.280 --> 50:26.280
 Okay.

50:26.280 --> 50:32.280
 So, wait a day to later.

50:32.280 --> 50:38.280
 So this is something I added.

50:38.280 --> 50:46.280
 To fast a while ago and haven't used much myself since.

50:46.280 --> 50:52.280
 But if I just search for weighted, here it is.

50:52.280 --> 50:54.280
 Here it is.

50:54.280 --> 50:57.280
 So you can see in the docs.

50:57.280 --> 51:05.280
 It shows you exactly how to use weighted data loaders. And so.

51:05.280 --> 51:09.280
 We pass in a batch size. We passed in some weights.

51:09.280 --> 51:12.280
 This is the weights is going to be 1234578.

51:12.280 --> 51:14.280
 It's actually zero.

51:14.280 --> 51:18.280
 And then some item transforms.

51:18.280 --> 51:23.280
 So like these are kind of really interesting in the docs.

51:23.280 --> 51:25.280
 In some ways.

51:25.280 --> 51:28.280
 It's extremely advanced. And otherwise it's extremely simple.

51:28.280 --> 51:31.280
 Which is to say if you look at this example in the docs.

51:31.280 --> 51:33.280
 Everything is totally manual.

51:33.280 --> 51:37.280
 So our labels are some random integers.

51:37.280 --> 51:41.280
 Kind of the.

51:41.280 --> 51:44.280
 I've even added a comment here.

51:44.280 --> 51:47.280
 It's going to be in the training set.

51:47.280 --> 51:53.280
 So our data block.

51:53.280 --> 51:57.280
 It's going to contain one category block.

51:57.280 --> 51:59.280
 Because we just got the one thing.

51:59.280 --> 52:04.280
 And rather than doing get X and get Y.

52:04.280 --> 52:07.280
 You can also just say getters.

52:07.280 --> 52:10.280
 Because get X and get Y basically become getters.

52:10.280 --> 52:12.280
 Which is a list of.

52:12.280 --> 52:14.280
 Transformations to do.

52:14.280 --> 52:18.280
 And so this is going to be a single getter or a single get X.

52:18.280 --> 52:19.280
 If you like.

52:19.280 --> 52:21.280
 Which is going to return the I.

52:21.280 --> 52:22.280
 Label.

52:22.280 --> 52:23.280
 And a splitter.

52:23.280 --> 52:26.280
 Which is going to decide whether something's valid or not based on this function.

52:26.280 --> 52:29.280
 So you can see this whole thing is like totally.

52:29.280 --> 52:31.280
 Manual you know.

52:31.280 --> 52:37.280
 So we can create a data set by passing in a list of the numbers from not nine.

52:37.280 --> 52:41.280
 And a single item transform that's going to convert that to a tensor.

52:41.280 --> 52:45.280
 And then our weights will be the numbers from not to seven.

52:45.280 --> 52:48.280
 And so then we can take our data set.

52:48.280 --> 52:50.280
 Or data sets.

52:50.280 --> 52:52.280
 And turn them into data loaders.

52:52.280 --> 52:57.280
 And then you can those are weights.

52:57.280 --> 52:58.280
 So.

52:58.280 --> 53:02.280
 With a batch size of one, we say show batch.

53:02.280 --> 53:04.280
 We get back a single number.

53:04.280 --> 53:05.280
 Okay.

53:05.280 --> 53:06.280
 And it's not doing random shuffling.

53:06.280 --> 53:08.280
 So we get the number zero because that was.

53:08.280 --> 53:11.280
 The first thing in our data set.

53:11.280 --> 53:18.280
 Let's see. What do we do next?

53:18.280 --> 53:22.280
 Now we got to do an equals 160.

53:22.280 --> 53:27.280
 So now we've got all of the numbers from not to 159.

53:27.280 --> 53:31.280
 Or getters.

53:31.280 --> 53:34.280
 Yes, forgetters. Yep.

53:34.280 --> 53:38.280
 You mentioned this is for X or Y.

53:38.280 --> 53:40.280
 This is a list.

53:40.280 --> 53:43.280
 That's whatever. Right. This is so there is just one thing.

53:43.280 --> 53:45.280
 I don't know if you call that X or you call it Y.

53:45.280 --> 53:46.280
 It's just one thing.

53:46.280 --> 53:52.280
 So if you have a get X and a get Y, that's the same as having a getters with a list of two things.

53:52.280 --> 53:54.280
 Okay.

53:54.280 --> 53:57.280
 So yeah.

53:57.280 --> 54:00.280
 I think I could just write getters. This has been a just since I've got this,

54:00.280 --> 54:03.280
 but I think I could just write get X here and put this not in a list.

54:03.280 --> 54:05.280
 Would probably be the same thing.

54:05.280 --> 54:06.280
 Okay.

54:06.280 --> 54:11.280
 I'll probably handle a little bit of mystery that might be happening as well.

54:11.280 --> 54:12.280
 Yeah.

54:12.280 --> 54:20.280
 So the block has an input parameter, correct, which is how it determines what of the getters is X versus Y.

54:20.280 --> 54:25.280
 Correct, which we actually looked at last time.

54:25.280 --> 54:26.280
 Yeah.

54:26.280 --> 54:29.280
 When we created our multi image block.

54:29.280 --> 54:33.280
 Before you joined sec.

54:33.280 --> 54:40.280
 But yes, useful reminder. Okay. So.

54:40.280 --> 54:43.280
 So here we.

54:43.280 --> 54:49.280
 See in a histogram of how often so our.

54:49.280 --> 54:53.280
 We created like a little synthetic learner that doesn't really do anything,

54:53.280 --> 54:57.280
 but we can pass callbacks to it. And there's a call back called collect data call back,

54:57.280 --> 55:02.280
 which just collects the data that it's part that is called in the learner.

55:02.280 --> 55:08.280
 And so this is how we can then find out what data was passed to the learner,

55:08.280 --> 55:12.280
 and we can see that the number 160.

55:12.280 --> 55:17.280
 Was received a lot more often when we trained this learner,

55:17.280 --> 55:19.280
 which is what you would expect.

55:19.280 --> 55:25.280
 This is the source of the weighted data loaded class.

55:25.280 --> 55:28.280
 Here.

55:28.280 --> 55:31.280
 And as you can see, other than the boilerplate.

55:31.280 --> 55:35.280
 It's one, two, three, four, five lines of code.

55:35.280 --> 55:40.280
 And then the weighted data loaders method is one, two lines of code.

55:40.280 --> 55:45.280
 So there's actually a lot more lines of examples than there is of actual code.

55:45.280 --> 55:47.280
 So often it's easier just to.

55:47.280 --> 55:49.280
 Read the source code.

55:49.280 --> 55:53.280
 Because, you know, thanks to the very layered approach to fast,

55:53.280 --> 55:56.280
 we can do so much stuff.

55:56.280 --> 55:59.280
 With so little code.

55:59.280 --> 56:04.280
 And so in this case, if we look through the code, we're passing in some weights.

56:04.280 --> 56:09.280
 And basically the key thing here is that we set if the,

56:09.280 --> 56:14.280
 if you pass in no weights at all, then we're just going to set it equal to the number one,

56:14.280 --> 56:17.280
 repeated n times.

56:17.280 --> 56:20.280
 So everything's going to get one, a weight of one.

56:20.280 --> 56:26.280
 And then we divide the weights by the sum of the weights so that the sum of the weights ends up summing up to one,

56:26.280 --> 56:28.280
 which is what we want.

56:28.280 --> 56:33.280
 And then,

56:33.280 --> 56:43.280
 if you're not shuffling, then there's no weighted anything to do.

56:43.280 --> 56:45.280
 So we just pass back the indexes.

56:45.280 --> 56:52.280
 And if we are shuffling, we will grab a random choice of,

56:52.280 --> 56:57.280
 based on the, based on the weights.

56:57.280 --> 57:07.280
 Cool.

57:07.280 --> 57:12.280
 All right, so there's going to be one weight per row.

57:12.280 --> 57:15.280
 All right, let's come back to that because I want to see how our things gone.

57:15.280 --> 57:17.280
 It looks like it's finished.

57:17.280 --> 57:22.280
 Notice that the fav icon in Jupyter will change depending on whether something is running or not.

57:22.280 --> 57:26.280
 So that's how you can quickly tell if something's finished.

57:26.280 --> 57:31.280
 Point two, one, six.

57:31.280 --> 57:47.280
 Point two, two, one. Okay. I mean, it's not a huge difference, but maybe it's a tiny bit better. I don't know, like it's.

57:47.280 --> 57:51.280
 Two.

57:51.280 --> 57:59.280
 You know, the key thing though is this lets us use our resources better, right? So we often will end up with a better answer.

57:59.280 --> 58:06.280
 But you can train for a lot less time. In fact, you can see that the error was at point two, one, six back here.

58:06.280 --> 58:10.280
 So, you know, we could probably have trained for a lot less epochs.

58:10.280 --> 58:17.280
 So that's progressive resizing.

58:17.280 --> 58:22.280
 Sir, is there a way to look at that and go.

58:22.280 --> 58:28.280
 Actually, I'd like to take the outputs from the top nine.

58:28.280 --> 58:32.280
 Because we had a better answer.

58:32.280 --> 58:35.280
 That was the question we got earlier about that's called early stopping.

58:35.280 --> 58:39.280
 And the answer is no, you probably wouldn't want to do early stopping.

58:39.280 --> 58:43.280
 But you can't go back to a previous.

58:43.280 --> 58:47.280
 Like epoch, there's no history.

58:47.280 --> 58:50.280
 You can. You have to use the early stopping call back to do that.

58:50.280 --> 58:53.280
 Cool. Okay. Okay. I'll look at that.

58:53.280 --> 58:57.280
 Or there's other things you can use.

58:57.280 --> 58:59.280
 As I say, I don't think you should.

58:59.280 --> 59:00.280
 But you can.

59:00.280 --> 59:04.280
 If I go training callbacks.

59:04.280 --> 59:06.280
 Tracker.

59:06.280 --> 59:13.280
 So, I think the other part of that is, yeah, is it counterproductive for.

59:13.280 --> 59:16.280
 Yeah, it's kind of true if it works, but not if it doesn't.

59:16.280 --> 59:18.280
 It won't. It's probably not a good idea.

59:18.280 --> 59:20.280
 It probably will make it worse. Yeah. Okay.

59:20.280 --> 59:24.280
 So the other thing you can do is a safe model call back, which saves,

59:24.280 --> 59:27.280
 which is kind of like early stopping, but it doesn't stop it.

59:27.280 --> 59:29.280
 It saves the.

59:29.280 --> 59:35.280
 Parameters of the best model during training, which is probably what you want instead of early stopping.

59:35.280 --> 59:37.280
 So, I think that's a good idea.

59:37.280 --> 59:41.280
 I'm going to do that either for the same reason we discussed earlier.

59:41.280 --> 59:47.280
 Why shouldn't you do this? It seems like you could just ignore it if you didn't want it or.

59:47.280 --> 59:50.280
 Like it might not hurt you.

59:50.280 --> 59:56.280
 Well, so this, this actually automatically loads.

59:56.280 --> 59:59.280
 The best set of parameters at the end.

59:59.280 --> 1:00:02.280
 And.

1:00:02.280 --> 1:00:05.280
 You know, you're just going to end up.

1:00:05.280 --> 1:00:10.280
 You know, with this kind of like model that just so happened to.

1:00:10.280 --> 1:00:16.280
 Look a tiny bit better on the validation set at an earlier epoch, but at that earlier epoch.

1:00:16.280 --> 1:00:21.280
 The landing rate hadn't yet stabilized and it's very unlikely it really is better.

1:00:21.280 --> 1:00:24.280
 So you've probably actually just picked something that's slightly worse.

1:00:24.280 --> 1:00:27.280
 And, you know,

1:00:27.280 --> 1:00:30.280
 made your process slightly more complicated for no good reason.

1:00:30.280 --> 1:00:32.280
 So, you know,

1:00:32.280 --> 1:00:35.280
 it's a big big big big big better on money.

1:00:35.280 --> 1:00:39.280
 It doesn't necessarily say anything about the phone or hidden.

1:00:39.280 --> 1:00:41.280
 Yeah.

1:00:41.280 --> 1:00:43.280
 Yeah. Yeah.

1:00:43.280 --> 1:00:49.280
 We have a strong prior belief that it will improve each epoch.

1:00:49.280 --> 1:00:52.280
 Unless you're overfitting.

1:00:52.280 --> 1:00:56.280
 And if you're overfitting, then you shouldn't be doing early stopping.

1:00:56.280 --> 1:00:58.280
 You should be doing more augmentation.

1:00:58.280 --> 1:01:03.280
 And I also have a great opportunity for somebody to document the arguments.

1:01:03.280 --> 1:01:07.280
 Because I'm like curious what add in does.

1:01:07.280 --> 1:01:11.280
 Yes, that would be a great opportunity for somebody to document the arguments.

1:01:11.280 --> 1:01:14.280
 And if somebody is interested in doing that.

1:01:14.280 --> 1:01:17.280
 We have a.

1:01:17.280 --> 1:01:20.280
 Really cool thing called documents,

1:01:20.280 --> 1:01:25.280
 which I only invented after we.

1:01:25.280 --> 1:01:30.280
 Oh, this is like,

1:01:30.280 --> 1:01:31.280
 it's not.

1:01:31.280 --> 1:01:34.280
 I should delete this because this is the old version.

1:01:34.280 --> 1:01:35.280
 Yeah.

1:01:35.280 --> 1:01:37.280
 That a fast core.

1:01:37.280 --> 1:01:40.280
 And documents.

1:01:40.280 --> 1:01:43.280
 You document each parameter by putting a comment after it.

1:01:43.280 --> 1:01:47.280
 And you document the return by putting a comment after it.

1:01:47.280 --> 1:01:49.280
 And.

1:01:49.280 --> 1:01:56.280
 Exactly.

1:01:56.280 --> 1:02:01.280
 So, I'm going to go ahead and add documents to add documents comments to

1:02:01.280 --> 1:02:06.280
 everything in fast AI, which of course is not finished because fast AI is pretty big.

1:02:06.280 --> 1:02:10.280
 And so here's an example of something that doesn't yet have documents comments.

1:02:10.280 --> 1:02:13.280
 So if somebody wants to go and add a comment to each of these.

1:02:13.280 --> 1:02:18.280
 Things and put that into a PR.

1:02:18.280 --> 1:02:23.280
 I'm.

1:02:23.280 --> 1:02:25.280
 I know what we.

1:02:25.280 --> 1:02:27.280
 Once you do that to do it.

1:02:27.280 --> 1:02:29.280
 I was just going to say that kid.

1:02:29.280 --> 1:02:33.280
 Something we should do Zach is to actually include an example in the documents.

1:02:33.280 --> 1:02:36.280
 Documentation of what it ends up looking like in N.B. Dev.

1:02:36.280 --> 1:02:38.280
 Because I can see that's missing.

1:02:38.280 --> 1:02:39.280
 That might be a good idea.

1:02:39.280 --> 1:02:41.280
 I can see if I can get on that tomorrow.

1:02:41.280 --> 1:02:42.280
 Yeah. Sorry.

1:02:42.280 --> 1:02:44.280
 How about what you're saying?

1:02:44.280 --> 1:02:48.280
 I just wanted to encourage everybody that, like.

1:02:48.280 --> 1:02:53.280
 Writing the documentation is like an excellent way to learn deeply what.

1:02:53.280 --> 1:02:55.280
 How everything works.

1:02:55.280 --> 1:02:57.280
 And like you get on.

1:02:57.280 --> 1:03:01.280
 You know what ends up happening is you get you write this documentation and, you know,

1:03:01.280 --> 1:03:06.280
 somebody like Jeremy will review it carefully and let you know what you don't understand.

1:03:06.280 --> 1:03:08.280
 And that's.

1:03:08.280 --> 1:03:12.280
 Yeah, that's how I learned about some other, you know, some other fast AI library.

1:03:12.280 --> 1:03:16.280
 So I highly recommend it going, you know, going doing that.

1:03:16.280 --> 1:03:18.280
 And here's what it ends up looking like.

1:03:18.280 --> 1:03:21.280
 Right. So here's optimizer and you can see it's got a little table underneath.

1:03:21.280 --> 1:03:24.280
 And if we look at the source of optimizer.

1:03:24.280 --> 1:03:26.280
 You'll see that.

1:03:26.280 --> 1:03:29.280
 Each parameter has a comment next to it.

1:03:29.280 --> 1:03:32.280
 So there's parameters automatically turned into.

1:03:32.280 --> 1:03:37.280
 Into this table.

1:03:37.280 --> 1:03:39.280
 All right.

1:03:39.280 --> 1:03:42.280
 So I'm going to make a super cool.

1:03:42.280 --> 1:03:44.280
 Yeah, super cool. This sounds like a good place to wrap up.

1:03:44.280 --> 1:03:46.280
 Anybody got any.

1:03:46.280 --> 1:03:50.280
 Questions or comments or anything before we wrap up.

1:03:50.280 --> 1:03:54.280
 I have a question regarding to.

1:03:54.280 --> 1:03:58.280
 Decising. Yes.

1:03:58.280 --> 1:04:02.280
 We didn't do actually.

1:04:02.280 --> 1:04:09.280
 I'm fine after you just that don't you think is something helpful.

1:04:09.280 --> 1:04:11.280
 The L I find, did you say?

1:04:11.280 --> 1:04:13.280
 Yeah. Yeah.

1:04:13.280 --> 1:04:14.280
 Yeah, I.

1:04:14.280 --> 1:04:17.280
 To be honest, I don't use our find much anymore.

1:04:17.280 --> 1:04:20.280
 Nowadays, because.

1:04:20.280 --> 1:04:27.280
 You know, at least for object recognition in computer vision.

1:04:27.280 --> 1:04:30.280
 The optimal learning rates pretty much always the same.

1:04:30.280 --> 1:04:34.280
 It's always around point.

1:04:34.280 --> 1:04:41.280
 Yeah, there's a reason to believe that we have any need to change it just because we changed the.

1:04:41.280 --> 1:04:43.280
 Resolution.

1:04:43.280 --> 1:04:46.280
 So yeah, I would, I wouldn't bother.

1:04:46.280 --> 1:04:50.280
 Just leave it where it was.

1:04:50.280 --> 1:04:55.280
 German, if you're training and validation loss is still decreasing after 12 people.

1:04:55.280 --> 1:04:59.280
 Can you pick up and train for a little longer without restarting?

1:04:59.280 --> 1:05:00.280
 You can.

1:05:00.280 --> 1:05:03.280
 The first thing I say is you shouldn't be looking at the validation loss to see if you're overfitting.

1:05:03.280 --> 1:05:05.280
 You should be looking at the error rate.

1:05:05.280 --> 1:05:13.280
 So the validation loss can get worse whilst the error rate gets better and that doesn't count as overfitting because the thing you want is to improve as the error rate.

1:05:13.280 --> 1:05:17.280
 That can happen if it gets over confident, but it's still improving.

1:05:17.280 --> 1:05:22.280
 Yeah, you can keep training for longer.

1:05:22.280 --> 1:05:31.280
 Because we're using if you're using fit one cycle or fine tune and fine tune users fit one cycle behind the scenes.

1:05:31.280 --> 1:05:39.280
 Continuing to train further, your learning rates going to go up and then down and then up and then down each time, which is not necessarily a bad thing.

1:05:39.280 --> 1:05:43.280
 But you know, if you.

1:05:43.280 --> 1:05:46.280
 Yeah, if you basically want to keep training at that.

1:05:46.280 --> 1:05:50.280
 At that.

1:05:50.280 --> 1:05:56.280
 You know, at that point, you would probably want to like decrease the learning rate by maybe 4x or so.

1:05:56.280 --> 1:06:02.280
 And in fact, you know, I think after this, I'm going to rerun this whole notebook.

1:06:02.280 --> 1:06:07.280
 But half the learning rate each time.

1:06:07.280 --> 1:06:09.280
 So I think that would be.

1:06:09.280 --> 1:06:12.280
 Potentially a good idea.

1:06:12.280 --> 1:06:16.280
 I have a question.

1:06:16.280 --> 1:06:25.280
 I don't know if it's too late, but I think it might be useful to discuss when you do the progressive resizing.

1:06:25.280 --> 1:06:30.280
 What part of the model gets dropped?

1:06:30.280 --> 1:06:35.280
 Like, what, you know, is there some part of the model that needs to be.

1:06:35.280 --> 1:06:39.280
 Reinitialized for the new nothing needs to be re initialized.

1:06:39.280 --> 1:06:42.280
 No, I found this on the web.

1:06:42.280 --> 1:06:47.280
 You're talking to me, but you're talking to Siri.

1:06:47.280 --> 1:06:50.280
 I'm offended.

1:06:50.280 --> 1:06:53.280
 Siri, teach me deep learning.

1:06:53.280 --> 1:07:01.280
 Yeah, conf next is what we call a resolution independent architecture, which means it doesn't.

1:07:01.280 --> 1:07:05.280
 It works for any input resolution.

1:07:05.280 --> 1:07:08.280
 And.

1:07:08.280 --> 1:07:11.280
 Time permitting in the next lesson.

1:07:11.280 --> 1:07:17.280
 We will see how convolutional neural networks actually work, but I guess a lot of you probably already knows so.

1:07:17.280 --> 1:07:19.280
 For those of you to do.

1:07:19.280 --> 1:07:25.280
 If you think about it, it's basically going patch by patch and doing this kind of.

1:07:25.280 --> 1:07:29.280
 Many matrix multiply for each patch.

1:07:29.280 --> 1:07:34.280
 So if you change the input resolution, it just has more patches to cover.

1:07:34.280 --> 1:07:36.280
 But it doesn't change the parameters at all.

1:07:36.280 --> 1:07:38.280
 So there's nothing to.

1:07:38.280 --> 1:07:41.280
 Reinitialized.

1:07:41.280 --> 1:07:43.280
 Does that make sense, Hamill?

1:07:43.280 --> 1:07:44.280
 Yeah, that makes sense.

1:07:44.280 --> 1:07:46.280
 I was just asking for the.

1:07:46.280 --> 1:07:48.280
 For the record.

1:07:48.280 --> 1:07:49.280
 Fair enough.

1:07:49.280 --> 1:07:50.280
 Yeah.

1:07:50.280 --> 1:07:52.280
 Just a question.

1:07:52.280 --> 1:07:53.280
 Yeah.

1:07:53.280 --> 1:07:58.280
 I was just going to quit night to say his, his resonant resolution independent.

1:07:58.280 --> 1:07:59.280
 Yep.

1:07:59.280 --> 1:08:00.280
 Good.

1:08:00.280 --> 1:08:01.280
 Yeah.

1:08:01.280 --> 1:08:08.280
 Everything we use is normally, but in the, like.

1:08:08.280 --> 1:08:11.280
 Have a look at that, like best fine tuning.

1:08:11.280 --> 1:08:15.280
 Models notebook and you'll see that two of the best ones are called.

1:08:15.280 --> 1:08:17.280
 V it and swing.

1:08:17.280 --> 1:08:19.280
 And also swing V two.

1:08:19.280 --> 1:08:23.280
 None of those are resolution independent.

1:08:23.280 --> 1:08:29.280
 Although there is a trick you can use to kind of make them as a independent, which we should try out.

1:08:29.280 --> 1:08:31.280
 In a future walkthrough.

1:08:31.280 --> 1:08:35.280
 Is that fiddling with the head.

1:08:35.280 --> 1:08:37.280
 Oh, there's a Tim.

1:08:37.280 --> 1:08:39.280
 There's a thing you can pass to Tim.

1:08:39.280 --> 1:08:45.280
 I don't know if we can use it to support progressive resizing or not.

1:08:45.280 --> 1:08:47.280
 It'll be interesting to experiment with.

1:08:47.280 --> 1:08:51.280
 It's basically changing the positional encodings.

1:08:51.280 --> 1:08:53.280
 I have a question.

1:08:53.280 --> 1:08:54.280
 Interest.

1:08:54.280 --> 1:08:55.280
 Yeah.

1:08:55.280 --> 1:09:00.280
 After you've done your experiments, progressive resizing.

1:09:00.280 --> 1:09:02.280
 In fine tuning.

1:09:02.280 --> 1:09:06.280
 How do you infest AI train with the whole train?

1:09:06.280 --> 1:09:09.280
 I never got around to do that.

1:09:09.280 --> 1:09:10.280
 Do you.

1:09:10.280 --> 1:09:12.280
 I almost never do.

1:09:12.280 --> 1:09:22.280
 Like instead I do what we saw in the last walkthrough, which is, I just train on a few different.

1:09:22.280 --> 1:09:27.280
 Randomly selected valid training sets.

1:09:27.280 --> 1:09:32.280
 Because that way.

1:09:32.280 --> 1:09:34.280
 You know, you get the benefit on sampling.

1:09:34.280 --> 1:09:37.280
 You're going to end up seeing all the images at least one anyway.

1:09:37.280 --> 1:09:42.280
 And you can also kind of see if something's messed up because you've still got a validation set each time.

1:09:42.280 --> 1:09:48.280
 So yeah, I used to like do this thing where I would create a validation set with a single item in.

1:09:48.280 --> 1:09:53.280
 To like get that last bit of juice, but I don't, I don't even do that anymore.

1:09:53.280 --> 1:09:55.280
 Okay, thanks.

1:09:55.280 --> 1:09:57.280
 No worries.

1:09:57.280 --> 1:09:59.280
 All right gang.

1:09:59.280 --> 1:10:01.280
 Enjoy the rest of your day slash evening.

1:10:01.280 --> 1:10:03.280
 Nice to see you all.

1:10:03.280 --> 1:10:04.280
 Bye.

1:10:04.280 --> 1:10:06.280
 Goodbye. Thanks.

1:10:06.280 --> 1:10:08.280
 Thank you.

1:10:08.280 --> 1:10:29.280
 Bye.

