 follow you up to the lesson nine and I can get to number 10 is never so close to you before. So amazing. Thank you. Okay. Yes, I saw you on the leaderboard, Surrata, you were 10th in the Paddy competition. That's very cool. So to catch people up, the most recent news on the Paddy competition is I did two more entries. And I don't remember. I think I might have shown you I can't remember if I showed you one or both. But yeah, so I unzombled the bottles that we had. And that improved the submission from 987.6 to 988. And then the other thing I did was I, you know, since the VIT models are actually definitely better than the rest, I kind of doubled their weights. And that got it from 988. See one to 988. And let's see, Surrata, Surrata, you're down to 11th. You're going to have to put in another effort to. Yeah, my friend. Yeah. Anybody else here on this leaderboard somewhere? Yeah, I'm down at, I don't know, was it 37 last my check? 37, that's not bad. What's your username? I think this is not you. Matt. Matt. Matt. Ah, Matt Resinski 45. Oh, yeah, it's looking better. Oh, yeah. You just, you can't stop for a moment with these things or somebody or jumping ahead. I tried to do the 60s. Yeah, 60s is pretty good. I've had promises that paper space so I can train again. Oh, no. I know. I've been successful. Like still just not being able to log in. Just error. And I subscribed to the Bade immersion still. I'm not sure maybe the restructuring something. An error. Oh, well, feel free to share it on the forum if it's an error that we might be able to help you with. I think it's just a generic when you try to set up a machine and just says error. Oh, paper space error. Yeah. Oh, yeah. That's annoying. The quite receptive if you use the support email and now I had any share and I got right back to me. Another thing is if the error is your fault, i.e. if you put something in pre run.sh that breaks things, then just fire up a pytorch instance rather than a fast AI instance because that doesn't run pre run.sh and so then you can fix it. I'll give you the traffic. And I had to say thank you for writing to set up the competition to help us get started. What a graduate there. He also shared in the forum to set up the local for us. Oh, yeah. Yeah. So I think thank you for him to get me back on the Kaku. Yes. Awesome. So now Radik's next job will be to become a Kaggle notebook's grandmaster. That's what I'm going to be watching out for. I think he's got what it takes personally. Nice, right. Radik, a gold medal. You've had a gold on Kaggle for notebooks. No, it looks not. I'm not sure what I have for notebooks. Just I haven't done that many notebooks ever. I think I have. What's your username on Kaggle? Let's find you. Radik one. Radik one with a number, not written, not word. Yeah, that's me. Two silvers. Okay. So this one actually is on the way to being a gold. It's got so close. You need 50 votes from regulars, I guess. I don't know what counts as a regular. Well, this how it works. So it's not in the relative terms. No, it's just 50 votes full stop. So I definitely noticed like it makes a big difference to yeah. So therefore it makes a big difference to put notebooks in popular competitions because that's where people are looking. So like this one got 400 votes, right? And I'm not sure it's necessarily my best notebook, but it was part of the patent competition, which had a lot of people working on it. So that's one trick. Yeah, so things which are not actually attached to any competition, it's much harder to get votes for. Yeah, I'm getting pretty close to notebooks, Grandmaster actually. So pretty excited about that. What's your or something to do with loving science? I'm guessing what's your it's actually? Well, yeah, my the link is slightly different actually. It's T A N T A M L N L I K E S M A T H. Oh, math not science. Okay. Okay, let's take a look. Oh, look at you 74. Very nice. And you need two more golds. Now's these nine silvers. Well, that's I'm gonna go up both this stuff right now. Let's see. Huh, I'm gonna go up. Oh, there we go. Yeah, that's that's some channel or enthusiasm to getting to niche, into notebooks, Grandmaster. That would be cool. Yeah, so just have to get those silver ones over the line. All right, so I've I've somebody asked about where the the gist uploading thing is. So let me dig that up. Oh, and actually when I do what I might do here is I'm going to I'm going to connect to my server. Someone asked about the with the gist uploading that are questioned asked in the forum somewhere. Yeah, yeah, yeah, on the forum, exactly. And we'll see when I connect to this computer, it's busy doing stuff. And specifically, this is what it looks like when you're busy training a model using weights and biases. So you can see I've got three windows here. I think you've written the dots. I always don't know. Oh, that just means that I've got another T. Max session running on a different computer, which has a smaller screen than this one. And there is worth some way to get rid of it by disconnecting other sessions. Let's connect other clients. Perfect D gives you connected clients, whichever you select is disconnected. Let's try that. No, that's not right. Oh, they probably mean shift D. There we go. All right, this is the one I just created. So if I hit this, there we go. So shift D and then select the one to disconnect. Oh, nice. Okay. Learn something new. Oh, we've got another new face today. Hello, Sophie. I don't think you've joined us before. Is that right? I've been here just quietly in the background sometimes. Okay. Thank you for joining. We're about to you visiting us from. In Brisbane. Oh, good on you. And what do you work with AI stuff where you're just getting started? No, not at all background in psychology, doing a postdoc in psych and sort of, yeah, trying to move over into data science. Okay, cool. Have you done a lot of the statistical side of psychology? Yeah, yeah, quite a bit. And quite a bit of coding in art, but I'm pretty new to Python. Okay, great. Big learning curves. Well, you know what, you're our target marker, right? So if you have any questions along the way, please jump in. Even things that you feel like everybody else must know. I guarantee not everybody else knows them. Yeah, definitely. These have been really helpful and really great. Awesome. So we're running. Awesome. Thanks for joining. Okay. So... You're training three models parallel right now? Yeah. So I've got three GPUs in this machine. And so... Yeah, one nice thing with weights and biases is you... Basically, let me show you. Okay, so here's weights and biases. And see, I don't use my Mac very much because nothing's locked in. All right. And so you can see it's running this thing called a sweep, right? There's going to be 477 runs. So I don't know why it says create 31 seconds ago because that's certainly not true. It's currently running. And so it's coming from this git repo. I feel like there's a... There's a... There's a sweep view because this is a particular run. This is a particular run. That's right. I'm terrible with it, to be honest. Okay. So let's go to the project. There we go. And the project has sweeps. And then... Okay. This one here I can kill because... Okay. Okay. So basically, you kind of say on the Linux side, WNB, sweet, create, or something like that. And then... Interesting. It's all grouped under this thing. Oh, okay. All right. So then... Yeah. So then basically it runs... There's lots of copies of your program feeding at different configurations. And yeah, you can run the client as many times as you like. So I've run it three times. At each time I've set it to a different computer device. Oh, you can turn your models into Python scripts and to able to do this or... Yes. Exactly. So... So this is finetune.py. So it's just calling... So it calls as parse arcs. So that's going to just go through and check what batch size, etc, etc, etc, you asked for, right? Sticks them all into... This parser thing. And then it calls train passing in those arguments. And so then train is going to initialize weights and biases for this particular project for this particular entity, which is FastAI, using the configuration that you requested. And so then you can say, for example, okay, there's got some particular data set, some particular batch size, some image size, etc. And then it creates a learner for some particular model name, some particular pooling type. Finetunes it. And then at the end, it logs how much GPU memory it used, what model it was, how long it took. And you don't have to log much because the FastAI weights and biases integration automatically tracks everything in the learner. So you can see here, there's all this like... learner.architecture, learner.loss function, etc. etc. So the curiosity, was this process of refactoring into a script painful? So actually, you can probably actually tell I didn't do this. Thomas Capel did this. If I had done it, I would have used FastCore.script instead of this stuff, I guess. Thomas Capel did this. But no, I wouldn't have been painful. I would have just chucked an MB Dev export on the cell that I had in my notebook. And that would have become my script. So... Hi Jeremy. Painful. Hi. I have a question. Wouldn't it need to be interesting to track power consumption, for example? I mean, to some people it might be, not for me. As to how you would track power consumption, I have no idea. You'd have to have some kind of sense connected to your power supply, I guess. They track a lot of system metrics in the runs. So like, if you look on a run, they will track like GPU memory, CPU memory, yeah. Like, yeah, if you click on the thing on the left, that it looks like a CPU chip, that thing, yeah. There's a lot of... So maybe there's power in here. I don't see how it can be, right? Because like, well, unless the NVIDIA... Do you power your usage app? Yeah, it does. Here you go. GPU power. So NVIDIA tells you the GPU power usage apparently. Although that won't tell you about your CPU, etc. power. The thing that's useful about this, I think, is the memory screen, the graph. Yeah, well, I mean, the key thing is the maximum memory use. So we actually track that here in the script. Yeah, we put it into GPU memory. Oh, good GPU, man. Okay. Isn't that so fast? Got to get GPU memory. Oh, okay. Watch.cou. So Thomas did that as well. I don't know why it's... To the power of negative three. What's that? What is that about? It's seeing. Curious. I'll have to ask him what that's... That's doing. Thomas works at way devices, right? Correct, correct, correct. Yeah, so he... I'd never used it before. So... So... Yeah, so probably most people have never heard of this, but first day I actually has a thing called fast GPU, which is what I've previously used for doing this kind of thing. So in general, when you've got all the one GPU, or just even if you got any one GPU and you got a bunch of things, you want to run, it's helpful to have some way to say like, okay, here's the things to run, and then set a script off to go and run them all and check the results. So fast GPU is the thing I built to do that. And the way fast GPU works is that you have a whole list of the whole directory of scripts in a folder, and it runs each script run at a time and puts them in... Then it runs and it puts them into a separate directory, you know, to say this is completed, and it tracks the results, and you can do it on like as many or few GPUs as you like, and it'll just go ahead and run it. And this is fine, but it's very basic. And I've kind of been planning to make it a bit more sophisticated. And yeah, weights and biases takes a lot further, you know, by... And I kind of want to redo or add something on top of fast GPU, so it is fairly compatible with weights and biases, but you could do everything locally. So the key thing... So the thing it's actually using for that config file is it goes through basically the Cartesian product of all the values in this YAML. So it's going to do each of these two datasets, planets and bets, for this one running rate, for every one of these models, for every one of these poolings, for... Okay, this is just the one resize method, and for every one of these experiment numbers. So yeah. So that's sort of a project I'd have to do at some point. The sweep allows you to run arbitrary programs, it doesn't have to be a script. So potentially you could just stay in the notebook and use tiny kernel or sorry, like nb client thing or whatever it's called. Yeah, X can be. Yeah. Yeah, yeah, it'd be fun to work on this to make the whole thing run with notebooks and stick stuff in a local SQL Live database. And because all this stuff, all this web GUI stuff, honestly, I don't like it at all. But the nice thing is it actually doesn't matter because I don't have to use it because they provide an API. So before I realized they have a nice API, I kept on sending Thomas's messages saying, how do I do this? How do I do that? Why isn't this working? When you'd have to like send me these like pages of screenshots, like click here, click there, turn this off, then you have to redo this three times and just like, oh, I hate this. Yeah, and then I found that, then here's like, we do have an API. And I was like, I looked at the API, it is so well documented. It's got examples. Yeah, it's really nice. Yes. So I've put all the stuff I'm working on into this Git repo. And so here's a tip, by the way, the information about if you're in a Git repo, the or in a Git directory, your clone directory, the information about your Git repo all lives in a file called.git.config. So you can see here, this is the Git repo. So if we now go to GitHub. One cool thing is that runs, is it tracks your Git commit? Like the run, you can get back to what code version. Yeah, that is very cool, isn't it? Yeah. Yeah, I mean, I do think we could pretty easily create a a local only version of this without all the fancy GUI, you know, which would also have benefits. And then people who want the fancy GUI and run stuff from multiple sites, stuff like that would use weights and biases, but you know, you could also do stuff without weights and biases. Anyway, here's our repo. And this analysis.ipinb is the thing that I showed you yesterday, if you want to check it out. And I'll put that in the chat. Best. There you go. Oh, by the way, you know, I think something else which would be good is we should start keeping a really good list for every walkthrough of like all the like key resources, key like, you know, links, key commands, examples we wrote and stuff like that. So I think to do that, what we should do is we should turn all of the walkthrough top topics into wikis. I don't know if you folks have used wiki topics before, but basically a wiki topic simply means that everybody will end up with an edit button. So if I just click. Okay, this one already is a wiki. So everybody should find on walkthrough one that you can click edit, right? One thing we'd put in an edit, for example, would be probably like often Daniel has these really nice pull walkthrough listings, which would have like a link to his reply, which you can get by the way by, I think you click on this little date here. Yeah, yes, and that gives you a link directly to the post, which is handy. What about this one? Okay, make that a wiki. Sorry, this is going to be a little bit boring for you guys to watch, but I might as well do it while I'm here. And if anybody else has any questions or comments, well, I do that. Yeah, Jeremy, you did the first GPU is possible to extend to high performance computing to do it on the note. Sorry, to do what? Appai in high performance computing. So in the digital environment, is it possible to track it as well? I mean, I don't know. I mean, yeah, I mean, anything that's running on in Python on a Linux computer should be fine. I think some HPC things are like use their own weird job scheduling systems and stuff. But yeah, as long as it's running a normal in video, it doesn't even have to be in video, honestly. But yeah, as long as it's running a normal Linux environment, which should be fine. It's pretty generic, you know, pretty general. Okay, so they are now all wikis. And so something I did the other day, for example, was in walkthrough for I added something saying like, oh, this is the one where we actually had a bug and you need to add CD at the end, you know, and I tried to recruit a little list of what was covered. So for example, maybe Matt's fantastic timestamps. We could copy and paste this items into here, for instance. Some of Radix examples, maybe, or even just a link to it. But yeah, so for this walkthrough, we should certainly include this link to the analysis.ipineb. Anyway, so you could see, yeah, with the API, it was just so easy just to go API.sweep.runs. Comes in as a dictionary, which we can then check a list of dictionaries into a data frame. Okay, I'm rerunning the whole lot, by the way, because it turns out I made a mistake at some point. I thought that Thomas had told me that Squish was always better than Crop for resizing and he told me I was exactly wrong. And it's actually that Crop's always better than Squish for resizing. So I'm rerunning the whole lot. It's annoying, but shouldn't take too long. Did you find that analyzing the sweep results like this was useful and relative to what you can see in the UI? Oh god, it was so much better, Hamil. Yes, so much. I mean, they've done a good job with that. With that UI, it's very sophisticated and clever and stuff, but I just never got to be friends with it. And as soon as I turned it into a data frame, I was just like, okay, now I can get exactly what I want straight away. It was absolutely breath of fresh air, frankly. I really like their parallel coordinates chart. And I find it very difficult to reproduce that in any visualization library. Do you? Like in a pathway. It's chart, but yeah, I mean, there must be parallel coordinates chart. No, there is. There's like a plotly one, but it's not that nice. Okay, because I don't like parallel. Because I don't like parallel. So like, cover over it and stuff and see, you know, what is. Did they write their own? I think so. Yeah, that's impressive. And they kind of wrote their own data frame kind of language, their own visualization library in like in a sense. Because like those weights and biases reports, you know, they have their own syntax. Okay. There isn't one in plotly or something. Yeah, there's one in plotly for sure. Plotly things are normally interactive. So have you tried that one? Yeah. Do you know if it's? Yeah, it works. It's just it's not as nice, but yeah, it works. Like when you hover over, like there's a there is at least a version. This one doesn't. Yeah, that one. It's like it's very fiddly. You might have to draw a box around it. To to to highlight it. Oh, there you go. Okay, so you just drag over it. That's not terrible. Yeah, I mean, it's okay. It's not the best UI, but you know. Oh, okay. This is thanks for telling me about this. It's cool. But you don't think you don't you don't like this that much. It's not that useful for you. I haven't managed to I mean, I know other people like it. So I don't doubt that it's useful for something. It's just apparently not useful for the things I've tried to use it for yet somehow. Cool. Cool. I mean, how do you kind of like drag over the the end bit to see where they come from or something? Yeah, I mean, it might be useful if you want to look at the way some biases one. Because I think it renders one by default for you for the runs. Yeah, yeah, it does. And it's easier to like. Let's check it out. Operate that. Yeah. W and B. W and B. Flash. All right. I think it could be in the sweeps thing. Most likely. Okay. Okay. And then yeah, pick a sweep. That one has zero runs, but. I think maybe that one. Okay. And then. Yeah. Okay. So here we go. Okay. Let. And then when you just hover over a section. See, I mean, I don't see how this is helping me. Well, I guess like. I'm saying so. No, no, I mean, so there's not that much variance in the well, I guess like what is the metric we're trying to optimize? It doesn't really seem like it's even on this chart. Like, you know what I mean? Oh, you know what you probably have to tell it what your metric is. And we probably didn't. So the far right hand thing is. Resize method rather than. So that's. Is there some way to tell it that we care about. Yeah, there's an edit. There's like a little pencil. Let's see. Okay. Add column for. We add. Loss or something. Yeah, let's do error. Wait, this is no, let's do accuracy. Multi. Okay. Okay, now we're talking. All right. You probably want to get rid of pool and resize methods since they don't have any variance. And they're not adding any information. All right. There we go. Now you can like cover over. I actually want to do the thing. Oh, here we go. Can I do this drag? There we are. Yeah, that does. I mean, this is definitely not going to tell me more than. And the number of experiments is not. No, that's true. Because there. This is some auditory thing. Anyway, there's a thing. Yeah, I'm a. Yeah. Sometimes I learn something. Sometimes I don't find that visualization, you know, it's not always. Okay. So. It's control P D to attach. Do you generally like to do the grid search thing or the Bayesian exploration? I. So like I'm all very new to all this, right? So but like in general, I don't do hyperparameter Bayesian hyperparameter stuff ever. And that's kind of funny because I was actually the one that taught weights and biases about the method they use for hyperparameter optimization, which actually tells you this is not quite true. I've used it once and I used it specifically for finding a good set of dropouts for a W D LSTM because there's like five of them. And I told Lucas about how I had like created a random forest that actually tries to, you know, predict how accurate something's going to be and then use that random forest to actually target better sets of hyperparameters. And then yeah, that's what they ended up using for weights and biases, which is really cool. But I kind of like to really. Use a much more human driven approach from like, well, what's the hypothesis I'm trying to test? How can I test that as fast as possible? Like most hyperparameters are independent of most other hyperparameters. So, you know, like you don't have to do a huge grid search, whatever, and you can figure out. So for example, in this case, it's like, okay, well, learning rate of.008 was basically always the best. So let's not try every learning rate for every model for every resize type, etc. That's just use that learning rate. Same thing for resize method. You know, crop was always better for the few things we tried it on. So we don't have to try every combination. And also, like I feel like I learn a lot more about deep learning when I. You know, ask like, well, what do I want to know about this thing? Well, is that thing independent of that other thing? Or is it or are they connected or not? And so in the end, I kind of come away feeling like, okay, well, I now know that. But you know, every model we tried the optimal learning rate is basically the same. Every model we've tried the optimal resize methods, basically the same. And like so I'm. Come away knowing that I don't have to try all these different things every time. And so now. Next time I do another project, I can leverage my knowledge of what I've learned. Rather than do yet another huge hyperparameter sweep. If that makes sense. I see you are the Bayesian optimisation. Yeah, my brain is the thing that's learning exactly. And I find like people with big companies that spend all their time doing these big, you know, hyperparameter optimisations like. I always feel in talking to them that they don't seem to know much about the practice of deep learning. Like they don't seem to know like. What generally works and what generally doesn't work because they never. bother trying to figure out the answers to those questions. But instead they just chuck in a huge. Hyperparameter optimisation thing into. You know, a thousand TPUs. Yeah, that's kind of something I've observed. That's really interesting. I mean, like. Do you does it do you feel like these like. Hyperparamers generalised across different architectures, different models. Oh, totally. Yeah, totally. In fact, yeah, that was a piece of analysis we did. Gosh, I don't know, four or five years ago, along with a fellowship today, I folks in the platform today. I folks were just trying lots of different sets of hyperparameters across this different sets of data sets as possible. And the same sets of hyperparameters were the best or close enough to the best for everything we tried. Oh, that's very. That's very. That's very. Yeah, it is. Yeah. Yeah, it is. With different architectures, like I can someone imagine that data set may be this not that super important, but between transformers and the CNS. I mean, I'm not to the question in this because I don't have any experience to say that this is not correct. I think this is wonderful and it is. It is. It is. It is. It's amazing. So yeah, the fact that. Across 90 different models that we're testing that couldn't be more different. They all had basically the same best learning rate or close enough. You know. The very interesting aspect here is. Doing the learning rate is something that you dump a lot of time into usually when you start working on a project or in a cognitive competition. You would be naturally inclined to, hey, you know, I'm using a different architecture. Let me try to find the experiment with learning rates, but it's nice that you can. Okay. Well, I should mention, this is true of computer vision. But not necessarily for tabular. I suspect like all computer vision problems do look pretty similar, you know, the data for them looks pretty similar. As suspect is also true, like. Specifically of object recognition. So like. Yeah, for. I don't know. I mean, these are things like nobody seems to bother testing like which I find to be crazy, but we should do similar tests for segmentation and, you know, bounding boxes and so forth. But I'm pretty sure we're fine. The same thing. You have the learning rate. So we suggest maybe some different learning rates are good in different places. So the learning rate finder I built before I had done any of his research right. Okay. Like you might have noticed that I hardly ever use it nowadays in the course. We, I don't even know if we've mentioned it yet in this course. Maybe we have the last lesson. I remember. Does anybody remember, do we done the learning rate finally yet in course 22. Yeah. You think we did. Yeah. Can I just add that one of the really you can sit there and play with grammar and it's all the like and skidgy wheels and get nowhere. And that's one of the things I'm really taking away from the course is the fact that you're talking about strategy and which goes back to. Copie and his 2002 paper. He had a term called strategy of analysis and that's something that really stuck with me. And so that sort of cred sends that idea of just mucking around with parameters. Yeah. Exactly. I suppose the magic parameters. These are the defaults and fast AI. Yeah, pretty much, although with learning rate. That's weird with learning rate. The. The defaults a bit lower than the optimal. Just because I didn't want to like push it. You know, I'd rather it always worked pretty well, rather than be pretty much the best, you know. Yeah, yeah. Okay. I'm just going to go and. Disconnect my other computer because it's connected to port 888, which is going to mess things up. We'll be back in one tick. Okay. Actually, now I think about it. I don't quite know why this is connecting on port 889. But part of this is to learn how to debug problems, right? Normally, the Jupyter server uses port 888. And I've only got my SSH connected to forward port 888. So it's currently not working. So the fact that it's using a different port suggests it's already running somewhere. So to find out where it's running, you can use PS, which lists all the processes running on your computer. And generally speaking, I find I get used to some standard set of options that I nearly always want. And then I forget what they mean. So I have no idea what WAU or X means. I just know that there are a set of options that I always use. So that basically lists all your processes, which obviously is a bit too many. So we want to now filter out the ones that contain Jupyter or Notebook. So pipe is how you do that in Linux. So that's going to send the output of this into the input of another program and a program that just prints out a list of matching lines is called grep. So we can grep for Jupyter. OK, there it is. So I'm kind of wondering where that how that's running. I wonder if we've got like multiple sessions of Tmux running. No, we don't. So TboxLS lists all your Tmux sessions. Oh, I've got a stopped version in the background. OK, that's why. So I just have to foreground it. There we go. That was a bit weird. OK, so now that should work. Can I do foreground? No. Fg. I'm going to control z to put it in the background. And where do you control z? Somebody, it actually stops it. Right? You can put it in the background and have it keep running by actually I'll show you. So if I press control z and type jobs, that's stopped. Right? So find out, try to refresh this window. It's going to sit there waiting forever and never going to finish. OK, because it's backgrounds back. It's stopped in the background. If you type bg optionally followed by a job number, which would be number one, and it defaults to the last thing that you put that you put in the background, it will start running it in the background. Even after you stopped. Yeah, so it's now running in the background. So for that type jobs, it's now running. OK. I'd still detach to this console, so if I open up this, you'll see it's still printing out things, right? But I can also do other things. And I don't do this very much because normally if I want something running at the same time, I would just chuck it in another team. I explain. I don't know. It's kind of nice to know this exists. Something else to point out is once I said bg, it had this ampersand after the job. That's because if you run something with an ampersand at the end, it always runs it in the background. So if you want to like fire off six processes to run in parallel, just put an ampersand at the end of each one and it'll run in the background. So for example, there's a script that runs LS six times. And so if I run it, you can see they're all interspersed with each other because it ran all six times at the same time. I see. And let's say like you create a process like this in the background without T mux and you want to kill it and use the thing to. You could type Fg to foreground it and then press control C. Yeah, something like that would be fine or you can, you can kill a single job. So in general, like you probably would want to search for bash job control to learn how to do these things. And as I said, one of the key things to know is that a job number has a percent of the start. So this is actually percent one would be how you go to this. Knowing what the Google is definitely yes. The key is the key thing. Although often you could just put in a few examples. So you could I'm guessing like if I take troll C B G F G jobs, which are the things we just learned about. There we go. It kind of gets us pretty close. Now we know they've got drop control commands. All right. Now. So when I kind of iterate through notebooks, what I tend to do is like once I've got something vaguely working, I generally duplicate it and then I try to get something else vaguely working and once that starts vaguely working, I then rename it to the thing that it is what I want. So then from time to time, then I just clean up the duplicated versions that I didn't end up using and I can tell which they are because I haven't renamed them yet. And so this is kind of how you can make it like you make a car. It looks like you're making copies of it. Yeah. So you can just click file, make a copy. Yep. And then you can click it and click duplicate. And so you like, what do you do after you duplicate it? You try to get it all open up that I'll open up that duplicate and I'll try something else, some different hyper parameter and different method or whatever. So in this case, I started out here in Patty. And kind of just experimented. And show batch and L.R. find and try to get something running. And then, you know, after that, I was like, okay, I've got something working. How do I make it better? And so I created Patty small, but literally it was made a copy, made a copy and it would be called patty copy.ipi and be. And I was like, oh, I wonder about different architectures. So I created this like, okay, well, basically, I want to try different item transforms, different batch transforms and different architectures. So create a train, which takes those three things. And so it creates a set of image loaders with those item transforms and those batch transforms. Use a fixed C to get the same validation set each time. Train it with that architecture. And then return the TTA error. So then, this is kind of like your weights and biases, like, how you keep your different experiments, ideas. Yeah. So, yeah. So now you can see I've kind of gone through and tried a few different sets of item and match transforms for this architecture. And this is like, so I had some just small architectures. So they'll run reasonably quickly. So these ran in about six minutes or so. And this is very handy, right? If you go sell all output toggle, you can quickly get an overview of what you're doing. And so from that, I kind of got a sense of which things seem to work pretty well for this one. And then I replicated that for a different architecture and found those things, which, you know, these are very, very different ones. Transform is based, one's confident based. You know, find the things which work pretty well consistently across very different architectures. And for those, then try those on other ones, swing V2 and swing. And yeah, then find, you know, so then let's toggle the results back on. So I'm kind of looking at two things. The first is what's the error rate at the end of training. The other is what's the TTA error rate. So my squish worked pretty well for both. Crop worked pretty well for both. This is all for cons next. This 640 by 480 to 88 by 224 didn't work so well. I mean, it's not terrible, but it's definitely worse. And 320 by 240 instead. You know, you talk a little bit about what you're looking for in the TTA versus. No, I just want to see like, I mean, the bad thing I care about is TTA because that's what I'm going to end up using. Yeah, that's the main one, but. Like, let's see. In this case. This one's not really any better or worse than our best cons next, but the TTA is way better. So that gets very encouraging, which is interesting. So this is now for the OT, right? Now, V it, we can't do the rectangular ones because V it has a fixed input size. So their final transformation has to be 220, 420, 224. So if you pass an end instead of a tuple, it's going to create square. Final images. And you know, on the other hand, this one looks crappy. Right, so definitely want to use squish for the IT. And then this one looked pretty good. You know, so this was using padding. So like for the IT, I probably wouldn't use crop. Last time I looked, TTA was not really a thing in other modeling frameworks that is given to you. Is that still the case? As far as I know, that's true. Yeah. You know, so there are a lot of people. Well, one group in particular has been copying without credit, everything they can. They might have done it. I won't mention their name, but yeah. So, swin V2. Apparently, Tanish told me is what all the cool kids on Kaggle use nowadays. That's a fixed resolution. And I found that there and for the larger sizes, there was no 224. You had the choice of 192 or 256. 256 got so slow, I couldn't bear it. But interestingly, even going down to 192, swins TTA is actually nearly as good as it is. Nearly as good as the best VIT. So that's, I thought that was pretty encouraging. This one, interestingly, like VIT didn't do nearly as well for the crop. And again, like VIT, it did pretty well on the pad. And then this is swin V1, which does have a 224. And so here, this TTA is okay, but the final results, not great. And so to me, I'm like, no, that's not fantastic. This one's again, you know, it's interesting, the crop, none of them are going well, except for con next. This one's not great either, right? So swin V1, little unimpressive. So basically, that's what I did next. And then I was like, okay, let's pick the ones that look good. And I made a duplicate of patty small. And I just did a search and replace of small with large. So we've now got con next large. And the other things I did differently was I got rid of the fixed random seed. So there's no seed equals 42 here. And so that means we're going to have a different training set each time. And so these are now not comparable, which is fine. And I have a few of one of them's like totally crap, right? But they're not totally comparable. But the point is now once I train each of these, they're training on a different architecture, a different resizing method. And I append to a list. So I start off with a empty list and I append the TTA predictions. And so, and I deleted the cells from the duplicate that weren't very good in patty small. So you'll see there's no crop anymore. Just squish and pad for VIT. And for SWINV2. Probably shouldn't have kept both of the SWINV ones. Actually, they weren't so good. And then what I did in the very last Kaggle entry was I took the two VIT ones because they were the clear best. And I appended them to the list. So they were there twice. So it's just a slightly clunky way of doing a weighted average. If you like. Yes, take them all together. Take the mean of their predictions. Find the argmax across the mean of their predictions to get the predictions and then submit in the same way as before. So that was basically my process. It's like it's very like, yeah, not particularly thoughtful. You know, it's pretty mechanical, which is what I like about it. In fact, you could probably automate this whole thing. So somebody is going to say something. No, I was going to say how critical is like this model stacking in Kaggle. Like, just curious how you think about that. I mean, it's like, I mean, you can kind of, I mean, we should try, right? We should probably submit. In fact, let's, well, we're kind of out of time. How about next time? Let's submit just the VIT, the best VIT. And we'll see how it goes. And that will give us, yeah, that will give us a sense of how much the. Ensembling matters. We kind of know ahead of time, it's not going to matter. Hugely. I mean, you specifically said on Kaggle on Kaggle, it definitely matters because in Kaggle, you want to win. But in real life, my small conf next got 97, well, rounded up. That's 98%. And my ensemble got 98.8%. Now that's in terms of error rate. That's nearly halving the error. So I guess that's actually pretty good. Really important question. How do you keep track of what submissions are tied to which notebook? Oh, I just put a description to remind me, but you know, a better approach would actually be to write the notebook name there. Which is what I normally do, but in this case, I wasn't taking it particularly seriously, I guess. So I was only planning to do these ones and that was it. So it's basically like, okay, do one with a single small model, then do one with an ensemble of small models and then do one with an ensemble of big models. And then it's after I submitted that that I thought, oh, I should probably wait. The VIT is a bit higher. So I ended up with the fourth one. So it's pretty easy for me. They only did for significant submissions. So easy to track. But yeah, I think. Now that I know actually that I'm doing a little bit more, because I actually did want to try one more thing. I think what I'll probably do is I'll go back and I'm going to, you can edit these and go go and put in the notebook name. And each one. And then. And then I wouldn't go back and change those notebooks later, unless there was likes, but I probably never. I would, I would just duplicate them and make changes in the duplicate and rename them to something sensible. And then of course this all ends up back in GitHub. So I always see. Yeah, see what's going on. So this is like, Ebel up, Samuel, without. It's good. It's like, you have a, you'd like every, like quote run is a notebook, like in a certain, like the way to advise. Kind of keep track. Yeah. Yeah. Exactly. But I mean, the only reason I can kind of do this is because I had already done. Like lots of runs of models. To find out which ones I can focus on. Right. So I didn't have to try a hundred architectures. I mean, in the way, it forces you to really look at it closely. Yeah. And I like have this dashboard. Right. Kind of like these, like, this role. My view is that this approach, you will actually become a better deep learning practitioner. And I also believe almost nobody does this approach and I almost feel like there are very few people I come across who are actually good deep learning practitioners, like not many people seem to know. What works and what doesn't. So, yeah. All right. Well, that's it. I think. Thanks for joining again and. Yeah. See you all next time. Bye. Thank you. Thank you. Everybody.
